{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ec7f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all ok\n"
     ]
    }
   ],
   "source": [
    "print(\"all ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0800ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2234609a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyDyg07nEQ9f5sJ74Xjur3LV-C1ZjQcK8go'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "757cf54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ChatGroq(model='qwen/qwen3-32b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "348648c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user is asking for the capital of France. Let me think. I remember that France is a country in Western Europe. The capital is a city that\\'s pretty well-known. I think it\\'s Paris. Wait, isn\\'t Paris also known as the City of Light? Yeah, that rings a bell. But I should double-check to make sure I\\'m not mixing up any information. Sometimes I confuse capitals of different countries. Let me recall other European capitals: Germany\\'s is Berlin, Italy\\'s is Rome, Spain\\'s is Madrid. France\\'s capital must be Paris. I don\\'t think there\\'s any other city that\\'s commonly mistaken for it. Oh, wait, maybe someone might think Lyon or Marseille, but no, those are major cities but not the capital. Definitely, the capital is Paris. I can also mention that Paris is famous for the Eiffel Tower and the Louvre Museum to add some context. Yeah, that should be accurate. No need to complicate it further. The answer is Paris.\\n</think>\\n\\nThe capital of France is **Paris**. \\n\\nKnown as the \"City of Light,\" Paris is renowned for its iconic landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. It serves as the political, economic, and cultural center of France. \\n\\n**Answer:** Paris.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is the capital of france?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa3711c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\miniconda3\\envs\\llmops_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model='all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c755e95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\miniconda3\\envs\\llmops_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings.embed_query(\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2745277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import Docx2txtLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b04e51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "960ca70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path=os.path.join(os.getcwd(),\"data\")\n",
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6850d4e2",
   "metadata": {},
   "source": [
    "@(pdf|txt|docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "242f944d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:20<00:36,  9.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:25<00:21,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [01:06<00:40, 20.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [03:12<00:00, 32.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredFileLoader, DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(file_path,  glob=\"**/*.*\", show_progress=True, loader_cls=UnstructuredFileLoader)\n",
    "data = loader.load()\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1a873fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tapas Kumar Paul\\n\\nData Scientist in AIOPs\\n\\nKolkata, West Bengal\\n\\nMobile No- 8016254143 / 8158805321\\n\\nPan No- CWKPP3505R\\n\\nEmail- tapaskumarpaul84@gmail.com , Linkedin , GitHub\\n\\nSummary\\n\\nAs a Data Scientist with 3+ years of experience in data analytics, machine learning, and statistical modelling. Expertise in building Machine Learning model building, Deep Learning model, data visualization, and Natural Language Processing (NLP) based data-driven decision-making. Proven ability to solve complex problems and improve business outcomes using AI based technology. Seeking to leverage skills in Data science and AI to drive impactful insights in a dynamic environment.\\n\\nTechnical Skills\\n\\nProgramming Languages: Python, R, SQL\\n\\nMachine Learning: Supervised/Unsupervised Learning, Neural Networks, Decision Trees, Random Forest, SVM, K-Means, KNN, Ensemble Learning, Clustering, Linear Regression, Logistic Regression, Time Series forecasting.\\n\\nDeep Learning: ANN, RNN, CNN, LSTM, Encoders-Decoders, Transformers, GAN.\\n\\nNatural Language Processing: Tokenizer, WordEmbedding, TFIDF, Text Classification, NER, Chatbot building using RASA.\\n\\nComputer Vision: Image classification, Object detection, Object tracking, OpenCV, OCR.\\n\\nData Visualization: Matplotlib, Seaborn, Advanced Excel.\\n\\nGenrativeAI: GenAI, Langchain, OpenAI, Retrieval Augmented Generation (RAG), LLM, HuggingFace.\\n\\nDatabases: PostgreSQL\\n\\nCloud Platforms: AWS (S3, EC2, CloudWatch, Load Balancer).\\n\\nVersion Control: Git, GitHub\\n\\nOther Tools: NewRelic, Grafana/Prometheus ( Monitoring tools), ServiceNow, JIRA(Ticketing Tools)\\n\\nPython Libraries: Pandas, Regular Expression (re) ,Sklearn, Tensorflow, Keras, Transformer, Pytorch, nltk, spacy, boto3, cv2, matplotlib, seaborn , YOLO , Flask, langchain etc.\\n\\nProfessional Experience\\n\\nGenpact India Pvt. Ltd.\\n\\nData Scientist in AIOPs, Kolkata, India                                                           Apr, 2021 to Present\\n\\nObjective: To enhance the operational efficiency by implementing in-house framework called GenAIOPs. Development and modify modules like Noise Reduction, Self- Resolution, Chatbot, Self-Heal for better performance.\\n\\nMethodology: Implement Text classification, Named Entity Recognition technics using different ML and DL concepts and use RASA framework for developing Chatbot. Developed Text summarization and QnA solution using GenAI concepts.\\n\\nKey Technologies: Python, SQL, TensorFlow, Keras, Scikit-learn, AWS, transformer, Pandas, Matplotlib, RASA, BERT, nltk, LLM, RAG, GPT-4, llama3.\\n\\nImpact: Reduced manual intervention for resolving Incident by 80% and reduced MTTR and significantly increasing operational efficiency.\\n\\n\\n\\nProject 3: Implement Self-Resolution and Noise Reduction module for GE Power internal team.                                                                                                                                     Jun,2023-May,2024\\n\\nProblems: Earlier support team did all the access management and report related issue resolve manually and manually suppressed all the non-actionable tickets and duplicate tickets.\\n\\nMethodology: Using NLP based Text classification and NER technics identify the ticket category like Password Reset, create account, delete account etc and provide the resolution by using python automation script. Using Noise reduction module auto suppressed all the tickets where no need to take any action.\\n\\nKey Technologies: Python, NLP, Machine Learning, BERT, SVM, LSTM, RNN, Random Forest, Sklearn, Pandas, ServiceNow, Postgres.\\n\\nImpact: Using Self-Resolution module reduce the manual effort for resolving Access Management and report related incident and increase the operational efficiency. Using Noise Reduction module reduce the open ticket queue. So operational efficiency has increase by 62%.\\n\\n--------------------------------------------------------------------------------------------------------------------\\n\\nProject 2: Develop conversational Chatbot using Rasa Framework for a Healthcare firm.\\n\\n                                                                                                                                   Aug, 2022- Mar, 2023\\n\\nProblems:  Earlier user connect to the support team through calls, emails or other contacts. Also\\n\\nSometime user asked only FAQ base questions but user need to wait for long time for assistance.\\n\\nMethodology: Developed a conversational AI based chatbot which can assist the user for any product related queries immediately. Chatbot can provide some SOP based resolution to the user like User Access management, Invoice generation, create order, order status etc. Integrate with the ServiceNow for ticket logging.\\n\\nKey Technologies: Python, RASA3, NLP, Pandas, Text classification, Regex, ServiceNow.\\n\\nImpact: Increase the operational efficiency, 24*7 support through Chatbot and User friendly which  increase 25% in customer engagement and increase the business by a significant level\\n\\n--------------------------------------------------------------------------------------------------------------------\\n\\nProject 1: Implementing Self- Resolution module for a Consulting firm’s HR & Finance team.                                                                                                                 Aug,2021- May,2022\\n\\nProblems: Generally HR and Finance team manually resolve the incident tickets so the MTTR is very high and operational efficiency is low.\\n\\nMethodology: Implement NLP based Text Classification technics and Entity Recognition technics for classify the tickets into pre-mentioned 8 classes. Integrate the Text classification service into ServiceNow tool and build some python automation scripts for resolving the issue tickets.\\n\\nKey Technologies: Python, Postgres, Transformer, BERT, NLP, Pandas, NLTK, ServiceNow.\\n\\nImpact: By implementing Self-Resolution module reduced the manual effort by 50% and increase the efficiency with a significant level.\\n\\n------------------------------------------------------------------------------------------------------------------------------------------\\n\\nManikaran Analytics Limited.\\n\\nAnalyst in Operations, New Delhi, India                                                          Nov, 2018- Jan, 2020\\n\\nObjective: For increasing the forecasting accuracy of Wind and Solar power plant generation manually check the weather condition and prepare a power generation forecasting schedule for a power plant and submit to the authorized portal like PTC, State electricity board.\\n\\nMethodology: Check the weather forecasting condition in different tools and prepare a forecasting schedule w.r.t the weather of the power plant area.\\n\\nImpact: Increase the accuracy of model generated schedule by 30% after modifying the schedule manually.\\n\\n\\n\\nCertifications\\n\\nThe Complete Python Developer by Udemy.\\n\\nDP-100, A-Z Machine Learning using Azure Machine Learning by Udemy.\\n\\nThe Complete SQL Bootcamp: Go from Zero to Hero by Udemy.\\n\\nRasa for Beginners by Udemy.\\n\\nEducation Details\\n\\nPost Graduate Diploma in Artificial Intelligence (PGDAI), CDAC Noida, (Feb, 2020-Jan, 2021).\\n\\nB.Tech in Electrical Engineering, Techno India University, West Bengal. (2014-2028).\\n\\nHobbies\\n\\nCooking, Travelling, Listening music\\n\\n-------------------------------------------------------------------------\\n\\nLanguages\\n\\nEnglish\\n\\nBengali\\n\\nHindi'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[5].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9acb6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path=os.path.join(os.getcwd(),\"data\",\"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "144760d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader=PyPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "575120fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "document=pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e5e805e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "427daeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_spliter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53cefa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=text_spliter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77158d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1935"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef963001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa8b00f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[1934].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "124cfa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6725321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='Attention Is All You Need\\n\\nAshish Vaswani∗ Google Brain avaswani@google.com\\n\\nNoam Shazeer∗ Google Brain noam@google.com\\n\\nNiki Parmar∗ Google Research nikip@google.com\\n\\nJakob Uszkoreit∗ Google Research usz@google.com\\n\\nLlion Jones∗ Google Research llion@google.com\\n\\nAidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu\\n\\nŁukasz Kaiser∗ Google Brain lukaszkaiser@google.com\\n\\nIllia Polosukhin∗ ‡ illia.polosukhin@gmail.com\\n\\nAbstract'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='1\\n\\nIntroduction\\n\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [29, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='†Work performed while at Google Brain. ‡Work performed while at Google Research.\\n\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n\\n2 Background'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8].\\n\\n3 Model Architecture'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='3 Model Architecture\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29]. Here, the encoder maps an input sequence of symbol representations (x1,...,xn) to a sequence of continuous representations z = (z1,...,zn). Given z, the decoder then generates an output sequence (y1,...,ym) of symbols one element at a time. At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\\n\\n3.1 Encoder and Decoder Stacks\\n\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n\\n2\\n\\nFigure 1: The Transformer - model architecture.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='2\\n\\nFigure 1: The Transformer - model architecture.\\n\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='3.2 Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\n3.2.1 Scaled Dot-Product Attention'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='3.2.1 Scaled Dot-Product Attention\\n\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n\\n3\\n\\nScaled Dot-Product Attention\\n\\nMulti-Head Attention\\n\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\\n\\nquery with all keys, divide each by values.\\n\\n√'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='query with all keys, divide each by values.\\n\\n√\\n\\ndk, and apply a softmax function to obtain the weights on the\\n\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\\n\\nAttention(Q,K,V ) = softmax(\\n\\nQKT √ dk\\n\\n)V'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='The two most commonly used attention functions are additive attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='theoretical complexity, dot-product attention is much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized matrix multiplication code.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='1√\\n\\ndk\\n\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1√ dk\\n\\n.\\n\\n3.2.2 Multi-Head Attention'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneﬁcial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the ﬁnal values,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the ﬁnal values, as depicted in Figure 2.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\n\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random i=1 qiki, has mean 0 and variance dk.\\n\\nvariables with mean 0 and variance 1. Then their dot product, q · k = (cid:80)dk\\n\\n4\\n\\n(1)\\n\\nMultiHead(Q,K,V ) = Concat(head1,...,headh)W O'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='variables with mean 0 and variance 1. Then their dot product, q · k = (cid:80)dk\\n\\n4\\n\\n(1)\\n\\nMultiHead(Q,K,V ) = Concat(head1,...,headh)W O\\n\\nwhere headi = Attention(QW Q\\n\\ni ,KW K i\\n\\n,V W V\\n\\ni )\\n\\nWhere the projections are parameter matrices W Q and W O ∈ Rhdv×dmodel.\\n\\ni ∈ Rdmodel×dk, W K\\n\\ni ∈ Rdmodel×dk, W V\\n\\ni ∈ Rdmodel×dv'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content=',V W V\\n\\ni )\\n\\nWhere the projections are parameter matrices W Q and W O ∈ Rhdv×dmodel.\\n\\ni ∈ Rdmodel×dk, W K\\n\\ni ∈ Rdmodel×dk, W V\\n\\ni ∈ Rdmodel×dv\\n\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n\\n3.2.3 Applications of Attention in our Model'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\nIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information ﬂow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\\n\\n3.3 Position-wise Feed-Forward Networks'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='3.3 Position-wise Feed-Forward Networks\\n\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\\n\\nFFN(x) = max(0,xW1 + b1)W2 + b2\\n\\n(2)'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='FFN(x) = max(0,xW1 + b1)W2 + b2\\n\\n(2)\\n\\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048.\\n\\n3.4 Embeddings and Softmax'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax dmodel. linear transformation, similar to [24]. In the embedding layers, we multiply those weights by'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='√\\n\\n3.5 Positional Encoding\\n\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n\\n5'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='5\\n\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\\n\\nLayer Type\\n\\nSelf-Attention Recurrent Convolutional Self-Attention (restricted)\\n\\nComplexity per Layer\\n\\nO(n2 · d) O(n · d2) O(k · n · d2) O(r · n · d)'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='Layer Type\\n\\nSelf-Attention Recurrent Convolutional Self-Attention (restricted)\\n\\nComplexity per Layer\\n\\nO(n2 · d) O(n · d2) O(k · n · d2) O(r · n · d)\\n\\nSequential Maximum Path Length Operations O(1) O(n) O(1) O(1)\\n\\nO(1) O(n) O(logk(n)) O(n/r)\\n\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and ﬁxed [8].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='In this work, we use sine and cosine functions of different frequencies:\\n\\nPE(pos,2i) = sin(pos/100002i/dmodel) PE(pos,2i+1) = cos(pos/100002i/dmodel)'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='PE(pos,2i) = sin(pos/100002i/dmodel) PE(pos,2i+1) = cos(pos/100002i/dmodel)\\n\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of PEpos.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='We also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\\n\\n4 Why Self-Attention'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='4 Why Self-Attention\\n\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1,...,xn) to another sequence of equal length (z1,...,zn), with xi,zi ∈ Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [25] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='6\\n\\nthe input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [15], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='As side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n5 Training\\n\\nThis section describes the training regime for our models.\\n\\n5.1 Training Data and Batching'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='tokens into a 32000 word-piece vocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='5.2 Hardware and Schedule\\n\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\\n\\n5.3 Optimizer'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='5.3 Optimizer\\n\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and (cid:15) = 10−9. We varied the learning rate over the course of training, according to the formula:\\n\\nlrate = d−0.5\\n\\nmodel · min(step_num−0.5,step_num · warmup_steps−1.5)\\n\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\\n\\n5.4 Regularization'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='5.4 Regularization\\n\\nWe employ three types of regularization during training:\\n\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\\n\\n7\\n\\n(3)'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='7\\n\\n(3)\\n\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\n\\nModel\\n\\nByteNet [15] Deep-Att + PosUnk [32] GNMT + RL [31] ConvS2S [8] MoE [26] Deep-Att + PosUnk Ensemble [32] GNMT + RL Ensemble [31] ConvS2S Ensemble [8] Transformer (base model) Transformer (big)\\n\\nBLEU\\n\\nEN-DE EN-FR 23.75\\n\\n24.6 25.16 26.03\\n\\n26.30 26.36 27.3 28.4'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='BLEU\\n\\nEN-DE EN-FR 23.75\\n\\n24.6 25.16 26.03\\n\\n26.30 26.36 27.3 28.4\\n\\n39.2 39.92 40.46 40.56 40.4 41.16 41.29 38.1 41.0\\n\\nTraining Cost (FLOPs)\\n\\nEN-DE\\n\\nEN-FR\\n\\n2.3 · 1019 9.6 · 1018 2.0 · 1019\\n\\n1.8 · 1020 7.7 · 1019\\n\\n1.0 · 1020 1.4 · 1020 1.5 · 1020 1.2 · 1020 8.0 · 1020 1.1 · 1021 1.2 · 1021\\n\\n3.3 · 1018 2.3 · 1019'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='1.8 · 1020 7.7 · 1019\\n\\n1.0 · 1020 1.4 · 1020 1.5 · 1020 1.2 · 1020 8.0 · 1020 1.1 · 1021 1.2 · 1021\\n\\n3.3 · 1018 2.3 · 1019\\n\\nLabel Smoothing During training, we employed label smoothing of value (cid:15)ls = 0.1 [30]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n6 Results\\n\\n6.1 Machine Translation'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [31]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [31].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of ﬂoating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision ﬂoating-point capacity of each GPU 5.\\n\\n6.2 Model Variations'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='6.2 Model Variations\\n\\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n\\n8'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n\\n8\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\\n\\nbase\\n\\n(A)\\n\\n(B)\\n\\n(C)\\n\\n(D)\\n\\nN dmodel\\n\\n6\\n\\n512\\n\\n2 4 8\\n\\n256 1024\\n\\ndff\\n\\n2048'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='base\\n\\n(A)\\n\\n(B)\\n\\n(C)\\n\\n(D)\\n\\nN dmodel\\n\\n6\\n\\n512\\n\\n2 4 8\\n\\n256 1024\\n\\ndff\\n\\n2048\\n\\n1024 4096\\n\\nh\\n\\n8 1 4 16 32\\n\\ndk\\n\\n64 512 128 32 16 16 32\\n\\n32 128\\n\\ndv\\n\\n64 512 128 32 16\\n\\n32 128\\n\\nPdrop\\n\\n0.1\\n\\n0.0 0.2\\n\\n(cid:15)ls\\n\\n0.1\\n\\n0.0 0.2\\n\\nPPL train steps (dev) 100K 4.92 5.29 5.00 4.91 5.01 5.16 5.01 6.11 5.19 4.88 5.75 4.66 5.12 4.75 5.77 4.95 4.67 5.47 4.92 300K 4.33\\n\\nBLEU params ×106 (dev) 25.8 65 24.9 25.5 25.8 25.4 25.1 25.4 23.7 25.3 25.5 24.5 26.0 25.4 26.2 24.6 25.5 25.3 25.7 25.7 26.4'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='BLEU params ×106 (dev) 25.8 65 24.9 25.5 25.8 25.4 25.1 25.4 23.7 25.3 25.5 24.5 26.0 25.4 26.2 24.6 25.5 25.3 25.7 25.7 26.4\\n\\n58 60 36 50 80 28 168 53 90\\n\\n(E) big\\n\\n6\\n\\npositional embedding instead of sinusoids\\n\\n1024\\n\\n4096\\n\\n16\\n\\n0.3\\n\\n213'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [8], and observe nearly'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='helpful in avoiding over-ﬁtting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical results to the base model.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='7 Conclusion\\n\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.\\n\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\\n\\n9\\n\\nReferences\\n\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\n\\narXiv:1607.06450, 2016.\\n\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='arXiv:1607.06450, 2016.\\n\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\n\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\n\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\n\\nreading. arXiv preprint arXiv:1601.06733, 2016.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\n\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\\n\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\n\\npreprint arXiv:1610.02357, 2016.\\n\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\n\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n\\n[9] Alex Graves. Generating sequences with recurrent neural networks.\\n\\narXiv preprint\\n\\narXiv:1308.0850, 2013.\\n\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\\n\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\n\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n\\n9(8):1735–1780, 1997.\\n\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\n\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\n\\non Learning Representations (ICLR), 2016.\\n\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\n\\nIn International Conference on Learning Representations, 2017.\\n\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\n\\narXiv:1703.10722, 2017.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\n\\narXiv:1703.10722, 2017.\\n\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\\n\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\n\\nInformation Processing Systems, (NIPS), 2016.\\n\\n10'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\n\\nInformation Processing Systems, (NIPS), 2016.\\n\\n10\\n\\n[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\n\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\n\\nmodel. In Empirical Methods in Natural Language Processing, 2016.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='model. In Empirical Methods in Natural Language Processing, 2016.\\n\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\n\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\n\\npreprint arXiv:1608.05859, 2016.\\n\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='preprint arXiv:1608.05859, 2016.\\n\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\n\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\\n\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\n\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\attention-is-all-you-need-Paper.pdf'}, page_content='[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\\n\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n\\n11'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='Automating Customer Service using LangChain Building custom open-source GPT Chatbot for organizations\\n\\nKeivalya Pandya 19me439@bvmengineering.ac.in Birla Vishvakarma Mahavidyalaya, Gujarat, India\\n\\nProf. Dr. Mehfuza Holia msholia@bvmengineering.ac.in Birla Vishvakarma Mahavidyalaya, Gujarat, India'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='Abstract— In the digital age, the dynamics of customer service are evolving, driven by technological advancements and the integration of Large Language Models (LLMs). This research paper introduces a groundbreaking approach to automating customer service using LangChain, a custom LLM tailored for organizations. The paper explores the obsolescence of traditional customer support techniques, particularly Frequently Asked Questions (FAQs), and proposes a paradigm shift towards responsive, customer'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='traditional customer support techniques, particularly Frequently Asked Questions (FAQs), and proposes a paradigm shift towards responsive, customer interactions. The heart of this innovation lies in the fusion of open-source methodologies, web scraping, fine-tuning, and the into customer service seamless platforms. This framework, presented as \"Sahaay,\" demonstrates the ability to scale across industries and organizations, offering real-time support and query resolution. Key elements of this'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content=\"demonstrates the ability to scale across industries and organizations, offering real-time support and query resolution. Key elements of this research encompass data collection via web scraping, the role of embeddings, the utilization of Google's Flan T5 XXL, Base and Small language models for knowledge retrieval, and the integration of the chatbot into customer service platforms. The results section provides insights into their performance and use cases, here particularly within an educational\"),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='into customer service platforms. The results section provides insights into their performance and use cases, here particularly within an educational institution. This research heralds a new era in customer service, where technology is harnessed to create efficient, personalized, and responsive interactions. Sahaay, powered by LangChain, redefines the customer-company relationship, elevating customer retention, value extraction, and brand image. As organizations embrace LLMs, customer service'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='the customer-company relationship, elevating customer retention, value extraction, and brand image. As organizations embrace LLMs, customer service becomes a dynamic and customer- centric ecosystem.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='context-aware,\\n\\npersonalized\\n\\nand\\n\\nintegration of LangChain\\n\\nopen-source\\n\\nstate-of-the-art\\n\\nKeywords— Customer Service Automation, Large Language\\n\\nModels, LangChain, Web Scraping, Context-Aware Interactions\\n\\nI. INTRODUCTION'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='Models, LangChain, Web Scraping, Context-Aware Interactions\\n\\nI. INTRODUCTION\\n\\n“Customer is king” is the ancient mantra reflecting the significance of customers in every business. In the digital age, where the rhythms of modern life are guided by the pulse of technology, the realm of customer service stands as the frontline of engagement between businesses and their clientele. It is the place where queries are answered, problems are resolved, and trust is forged.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='customer service automation, it becomes abundantly clear that the traditional methods once hailed as revolutionary, are gradually becoming obsolete.\\n\\nThis paper is an invitation to envision a future where customer service is not a cost center but a wellspring of customer satisfaction and loyalty. We propose an open-source framework that can be scaled to any industry or organization to fulfill the consumer needs for support and query resolution within seconds.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='For demonstration purposes, we use the information presented by Birla Vishvakarma Mahavidyalaya (BVM) Engineering website https://bvmengineering.ac.in/ as the context for our chatbot, from where it can retrieve all the information in real-time and answer to any queries that are raised by the users. Here, users can be anyone ranging from prospective students, current students who intend to get information from the Notice Board, researchers who wish to search for their potential research guide,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='students, current students who intend to get information from the Notice Board, researchers who wish to search for their potential research guide, and so on. The applications are endless.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='College\\n\\non\\n\\ntheir\\n\\nII. LITERATURE SURVEY'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content=\"S. Kim (2023) et al addresses the challenge of deploying resource-intensive large neural models, such as Transformers, for information retrieval (IR) while maintaining efficiency. Experimental results on MSMARCO benchmarks demonstrate the effectiveness of this approach, achieving successful distillation of both dual-encoder and cross-encoder teacher models into smaller, 1/10th size asymmetric students while retaining 95-97% of the teacher's performance [2]. L. Bonifacio et al (2022) highlights\"),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content=\"models into smaller, 1/10th size asymmetric students while retaining 95-97% of the teacher's performance [2]. L. Bonifacio et al (2022) highlights the recent transformation in the Information Retrieval (IR) field, propelled by the emergence of large pretrained transformer models. The MS MARCO dataset played a pivotal role in this revolution, enabling zero-shot transfer learning across various tasks [3].\"),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='This paper proposed a novel open-source approach to building LLM Chatbots using custom knowledge from the content in the website. It is unique in several ways:\\n\\nThis research paper brings the future of customer service, where automation, personalization, and responsiveness converge to redefine the customer-company relationship. At the heart of this transformation lies the integration of LLMs, exemplified by LangChain [1].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='1. We propose an open-source framework which is robust with the type of dataset available on the webpage or the web of links.\\n\\n2. This implementation aims to compliment the use of FAQs with a more interactive and user-friendly interface.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='2. This implementation aims to compliment the use of FAQs with a more interactive and user-friendly interface.\\n\\nIn the annals of customer service history, FAQs and traditional support mechanisms have long held sway. These venerable tools have dutifully served as repositories of information, attempting to address the queries and concerns of customers. However, as we stand at the cusp of a new era in'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='3. We then do a comparative study of various models, their performance on the provided data relative to the expected response from the LLM.\\n\\nSubmitted to the 3rd International Conference on “Women in Science & Technology: Creating Sustainable Career” 28 -30 December, 2023\\n\\nIII. METHODOLOGY\\n\\nA. Evaluating the Performance of LLMs\\n\\nThis section covers the data collection, details about the selected model, fine-tuning, and integration with the Gradio APIs for web deployment.\\n\\nA. Data Collection'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='A. Data Collection\\n\\nIt is relatively difficult to evaluate LangChain agents, especially when trained on large chunks of context datasets for information retrieval. Hence, the current solution for the lack of metrics is to rely on human knowledge to get a sense of how the chain/agent is performing.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='To gather the necessary data for our project, we employed BeautifulSoup web scraping techniques to retrieve publicly accessible information from an organization’s homepage. We observed this page is often linked with all the relevant the user/visitor. This approach information required for allowed us to collect a wide array of data, including customer service FAQs, product manuals, support forums, chat logs, associated institutions, and so on. This data further serves as the context for our LLM.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='B. Embeddings\\n\\nEmbeddings play a pivotal role in the development of any LLM powered. They are vector representations of words or phrases in a continuous mathematical space that capture semantic and contextual information, allowing the model to understand the meaning and relationships between words, which is essential for providing meaningful responses to user queries.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='We have used HuggingFace Instuct Embeddings – “hkunlp/instructor-large” a text embedding model fine-tuned for specific tasks and domains, such as classification, retrieval, clustering, and text evaluation [4]. What sets Instructor apart is its ability to generate tailored text embeddings without requiring additional fine-tuning. These embeddings are then stored using FAISS (Facebook AI Similarity Search) library that allows developers to quickly search for embeddings of multimedia documents'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='are then stored using FAISS (Facebook AI Similarity Search) library that allows developers to quickly search for embeddings of multimedia documents that are similar to each other [5].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='C. Language Model\\n\\nWe have chosen Google’s Flan T5 XXL as the most appropriate language model after comparing with other Flan T5 distributions to retrieve knowledge from the vectorspace and chat_history (or memory) [6]. The model retains the context of previous messages and uses that as a reference to predict answers for the upcoming questions. This helps users to have an interactive conversation with the chatbot, instead of a monotonous and robotic one.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='D. Integration with Customer Service Platforms\\n\\nA simple chat window can be activated at the corner of any website which would enable users to interact with the chatbot and ask any relevant questions or doubts regarding the organization. However, for the demonstration purpose of this paper, we are using Gradio API framework [7].\\n\\nIV. RESULTS\\n\\nFig. 1. Model Architecture – Sahaay'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='IV. RESULTS\\n\\nFig. 1. Model Architecture – Sahaay\\n\\nIn this section, we mention the metrics of comparison, provide comparative analysis, and use cases in association with an educational institution.\\n\\nIt is evident from TABLE-I, II and III that the XXL model outperforms other competitive LLMs such as BASE and SMALL.\\n\\nSubmitted to the 3rd International Conference on “Women in Science & Technology: Creating Sustainable Career” 28 -30 December, 2023\\n\\nSr. No.\\n\\n1.\\n\\n2.\\n\\n3.\\n\\n4.\\n\\n5.\\n\\n6.\\n\\nSr. No.\\n\\n1.\\n\\n2.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='Sr. No.\\n\\n1.\\n\\n2.\\n\\n3.\\n\\n4.\\n\\n5.\\n\\n6.\\n\\nSr. No.\\n\\n1.\\n\\n2.\\n\\n3.\\n\\n4.\\n\\n5.\\n\\n6.\\n\\nSr. No.\\n\\n1.\\n\\n2.\\n\\n3.\\n\\n4.\\n\\n5.\\n\\n6.\\n\\nTABLE I.\\n\\nGoogle’s Flan-T5-XXL Performance\\n\\nc. Follow-up question\\n\\nQuery/Prompt\\n\\nWhat is BVM?\\n\\nWhere is it?a\\n\\nWhat is IEEE BVM?\\n\\nWhat is TRS BVM?\\n\\nName the annual newsletter?\\n\\nTell me something about ICWSTCSC\\n\\nMetrics\\n\\nAnswer'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='Where is it?a\\n\\nWhat is IEEE BVM?\\n\\nWhat is TRS BVM?\\n\\nName the annual newsletter?\\n\\nTell me something about ICWSTCSC\\n\\nMetrics\\n\\nAnswer\\n\\nBirla Mahavidyalaya Vallabh Vidyanagar, Gujarat India Institute of Electrical and Electronics Engineers Student Branch of BVM BVM Student associated with Society India Vishvakarma Magazine and Newsletter The International Conference for Women in Science and Creating Technology Sustainable Career is the 3rd International Conference happening in hybrid mode.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='Vishvakarma\\n\\nChapter Robotics\\n\\nPerformance\\n\\n✯✯✯✯\\n\\n✯✯✯\\n\\n✯✯✯✯\\n\\n✯✯✯✯\\n\\n✯✯✯✯\\n\\n✯✯✯✯✯\\n\\nB. Applications'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='Customer service establishes a direct connection between the customer and the company. It retains customers and extracts higher value from them. By harnessing the power of Large Language Models as shown in Fig. 2, customer service to new heights, facilitating efficient, can be elevated personalized, and responsive interactions. The LangChain fine- tuned over custom knowledge of the product, service, or organization can effectively address a wide array of customer inquiries and issues. Its'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='fine- tuned over custom knowledge of the product, service, or organization can effectively address a wide array of customer inquiries and issues. Its ability to understand context and history empowers it to provide personalized support to customers. Automated customer service powered by LLMs is available around the clock and is also proficient in multiple languages.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='a. Follow-up question\\n\\nTABLE II.\\n\\nGOOGLE’S FLAN-T5-BASE PERFORMANCE\\n\\nMetrics\\n\\nQuery/Prompt\\n\\nAnswer\\n\\nPerformance\\n\\nWhat is BVM?\\n\\nWhere is it?b\\n\\nBVM is the first engineering college of Gujarat State established way back in 1948. VV Nagar, Gujarat, India\\n\\n✯✯✯✯\\n\\n✯✯✯\\n\\nWhat is IEEE BVM?\\n\\nWhat is TRS BVM? Name the annual newsletter? Tell me something about ICWSTCSC\\n\\nfirst engineering college of Gujarat State established way back in 1948 CVM BVM Robotics Society Of India Vishvakarma Magazine'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='first engineering college of Gujarat State established way back in 1948 CVM BVM Robotics Society Of India Vishvakarma Magazine\\n\\nBVM Conference\\n\\n3rd\\n\\nInternartional\\n\\n✯\\n\\n✯✯\\n\\n✯✯✯✯\\n\\n✯✯✯✯\\n\\nb. Follow-up question\\n\\nTABLE III.\\n\\nGOOGLE’S FLAN-T5-SMALL PERFORMANCE\\n\\nMetrics\\n\\nQuery/Prompt\\n\\nAnswer\\n\\nPerformance\\n\\nWhat is BVM?\\n\\nWhere is it?c'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='✯✯✯✯\\n\\nb. Follow-up question\\n\\nTABLE III.\\n\\nGOOGLE’S FLAN-T5-SMALL PERFORMANCE\\n\\nMetrics\\n\\nQuery/Prompt\\n\\nAnswer\\n\\nPerformance\\n\\nWhat is BVM?\\n\\nWhere is it?c\\n\\nBVM is the first Autonomous Engineering institute of Gujarat to obtain academic autonomy for all its UG & PG programs, Grant University from Commission (UGC). BVM Engineering College\\n\\n✯✯✯\\n\\n✯\\n\\nFig. 2. User interface – Gradio framework\\n\\nWhat is IEEE BVM? What is TRS BVM? Name the annual newsletter?\\n\\nTell me something about ICWSTCSC'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content=\"✯✯✯\\n\\n✯\\n\\nFig. 2. User interface – Gradio framework\\n\\nWhat is IEEE BVM? What is TRS BVM? Name the annual newsletter?\\n\\nTell me something about ICWSTCSC\\n\\nengineering college of Gujarat State BVM\\n\\nCampus Publications\\n\\nICWSTCSC 2023 PMSSS after students' admission (AY: 2023-24) at BVM Internartional Conference.\\n\\nreporting\\n\\n3rd\\n\\n✯\\n\\n✯\\n\\n✯\\n\\n✯✯✯\\n\\nV. CONCLUSION\"),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='reporting\\n\\n3rd\\n\\n✯\\n\\n✯\\n\\n✯\\n\\n✯✯✯\\n\\nV. CONCLUSION\\n\\nIn the ever-evolving landscape of customer service, the introduction of Sahaay’s innovative approach presented in this paper, using LangChain as a prime example, ushered in a new era of automation. Automating customer service using Sahaay’s open-source Large Language architecture leveraging LangChain revolutionizes the customer-company relationship to provide efficient, and CX.\\n\\nIt enables companies'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='It enables companies\\n\\nSubmitted to the 3rd International Conference on “Women in Science & Technology: Creating Sustainable Career” 28 -30 December, 2023\\n\\npersonalized, and responsive support, ultimately leading to customer retention, increased customer value, and a more positive brand image. As organizations continue to leverage the capabilities of LLMs, the landscape of customer service is evolving into a more dynamic and customer-centric ecosystem.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content=\"REFERENCES [1] Asbjørn Følstad and Marita Skjuve. 2019. Chatbots for customer service: user experience and motivation. In Proceedings of the 1st International Conference on Conversational User Interfaces (CUI '19). Association for Computing Machinery, New York, NY, USA, Article 1, 1–9. https://doi.org/10.1145/3342775.3342784\"),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='This paper demonstrates a comparison between various model performances and evaluates them on the basis of the quality of compared response generated. We have GOOGLE/FLAN-T5-XXL with GOOGLE/FLAN-T5-BASE, and GOOGLE/FLAN-T5-SMALL and observed that the XXL model outperforms the other LLMs in the provided task. Each model is posed with the same questions.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='In the future, Sahaay can access PDFs, Videos, Audio, and other files to extract relevant information about, for example, student activities, research work and innovation carried out by BVM. This multimodal capability has the potential to change forever the way we interact with websites and retrieve information in much less time.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='[2] Kim, S., Rawat, A. S., Zaheer, M., Jayasumana, S., Sadhanala, V., Jitkrittum, W., Menon, A. K., Fergus, R., & Kumar, S. (2023). EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval. ArXiv. /abs/2301.12005'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='[3] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. for Information Retrieval. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \\'22). Association for Computing Machinery, New York, NY, USA, 2387–2392. https://doi.org/10.1145/3477495.3531863 [4] Su, Hongjin, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. \"One Embedder, Any'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='Hongjin, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. \"One Embedder, Any Task: Instruction-Finetuned Text Embeddings.\" ArXiv, (2022). /abs/2212.09741. Johnson, Jeff, Matthijs Douze, and Hervé Jégou. \"Billion-scale Similarity Search with GPUs.\" ArXiv, (2017). Accessed September 28, 2023. /abs/1702.08734.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='InPars: Unsupervised Dataset Generation\\n\\n[5]\\n\\nACKNOWLEDGMENT\\n\\nThe authors would like to express their deepest appreciation to the research facility provided at TRS BVM Laboratory for encouraging multi-disciplinary collaborative research within the campus. We’d also like to thank Birla Vishvakarma Mahavidyalaya (Engineering College) for allowing us to experiment with the innovation on their website.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='[6] Chung, Hyung W., Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li et al. \"Scaling Instruction-Finetuned Language Models.\" ArXiv, (2022). Accessed September 28, 2023. /abs/2210.11416\\n\\n[7] Abid, A., Abdalla, A., Abid, A., Khan, D., Alfozan, A., & Zou, J. (2019). Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild. ArXiv. /abs/1906.02569'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\customer_chain.pdf'}, page_content='Submitted to the 3rd International Conference on “Women in Science & Technology: Creating Sustainable Career” 28 -30 December, 2023'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='6 1 0 2\\n\\nt c O 6 2\\n\\n]\\n\\nG L . s c [\\n\\n1 v 9 2 2 8 0 . 0 1 6 1 : v i X r a\\n\\nWord Embeddings and Their Use In Sentence Classiﬁcation Tasks\\n\\nAmit Mandelbaum\\n\\nAdi Shalev\\n\\nHebrew University of Jerusalm\\n\\namit.mandelbaum@mail.huji.ac.il\\n\\nbitan.adi@gmail.com\\n\\nOctober 27, 2016\\n\\nAbstract'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='This paper has two parts. In the ﬁrst part we discuss word embeddings. We discuss the need for them, some of the methods to create them, and some of their interesting properties. We also compare them to image embeddings and see how word embedding and image embedding can be combined to perform different tasks. In the second part we implement a convolutional neural network trained on top of pre-trained word vectors. The network is used for several sentence-level classiﬁcation tasks, and achieves'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='neural network trained on top of pre-trained word vectors. The network is used for several sentence-level classiﬁcation tasks, and achieves state-of-art (or comparable) results, demonstrating the great power of pre-trainted word embeddings over random ones.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='I.\\n\\nIntroduction\\n\\nresearch area in NLP 1.\\n\\nThere are some deﬁnitions for what Word'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='research area in NLP 1.\\n\\nThere are some deﬁnitions for what Word\\n\\nEmbeddings are, but in the most general notion, word embeddings are the numer- ical representation of words, usually in a shape of a vector in (cid:60)d. Being more speciﬁc, word embeddings are unsupervisedly learned word representation vectors whose relative similari- ties correlate with semantic similarity. In com- putational linguistics they are often referred as distributional semantic model or distributed repre- sentations.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='The theoretical foundations of word embed- dings can be traced back to the early 1950’s and in particular in the works of Zellig Harris, John Firth, and Ludwig Wittgenstein. The ear- liest attempts at using feature representations to quantify (semantic) similarity used hand- crafted features. A good example is the work on semantic differentials [Osgood, 1964]. The early 1990’s saw the rise of automatically gen- erated contextual features, and the rise of Deep Learning methods for Natural'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Osgood, 1964]. The early 1990’s saw the rise of automatically gen- erated contextual features, and the rise of Deep Learning methods for Natural Language Pro- cessing (NLP) in the early 2010’s helped to in- crease their popularity, to the point that, these days, word embeddings are the most popular'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='This work will be divided into two parts. In the ﬁrst part we will discuss the need for word embeddings, some of the methods to create them, and some interesting features of those embeddings. We also compare them to image embeddings (usually referred as image features) and see how word embedding and image embedding can be combined to perform different tasks.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='In the second part of this paper we will present our implementation of Convolutional Neural Networks for Sentence Classiﬁcation [Kim ,2014]. This work which became very popular is a very good demonstration of the power of pre-trained word embeddings. Us- ing a relatively simple model, the authors were able to achieve state-of-art (or comparable) re- sults, for several sentence-level classiﬁcation tasks. In this part we will present the model, discuss the results and compare them to those of the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='sults, for several sentence-level classiﬁcation tasks. In this part we will present the model, discuss the results and compare them to those of the original article. We will also extend and test the model on some datasets that were not used in the original article. Finally, we will'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='1In 2015 the dominating subject at EMNLP (\"Empiri- cal Methods in NLP\") conference was word embeddings, source: http://sebastianruder.com/word-embeddings-1/\\n\\n1\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016\\n\\npropose some extensions for the model which might be a good proposition for future work.\\n\\nII. Word Embeddings\\n\\ni. Motivation'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='propose some extensions for the model which might be a good proposition for future work.\\n\\nII. Word Embeddings\\n\\ni. Motivation\\n\\nIt is obvious that every mathematical system or algorithm needs some sort of numeric input to work with. However, while images and audio naturally come in the form of rich, high- dimensional vectors (i.e. pixel intensity for images and power spectral density coefﬁcients for audio data), words are treated as discrete atomic symbols.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='The naive way of converting words to vectors might assign each word a one-hot vector in (cid:60)|V| where |V| being vocabulary size. This vector will be all zeros except one unique index for each word. Representing words in this way leads to substantial data sparsity and usually means that we may need more data in order to successfully train statistical models.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Models in both categories share, in at least some way, the assumption that words that ap- pear in the same contexts share semantic mean- ing.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='One of the most inﬂuential early works in count-based methods is the LSI/LSA Indexing/Analysis) Semantic (Latent [Deerwester et al., 1990] method. This method is based on the Firth’s hypothesis from 1957 [Firth, 1957] that the meaning of a word is deﬁned \"by the company it keeps\". This hypothesis leads to a very simple albeit a very high-dimensional word embedding. Formally, each word can be represented as a vector in (cid:60)N where N is the unique number of words in a given dictionary (in'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='word embedding. Formally, each word can be represented as a vector in (cid:60)N where N is the unique number of words in a given dictionary (in practice N=100,000). Then, by taking a very large corpus (e.g. Wikipedia), let Count5(w1,w2) be the number of times w1 and w2 occur within a distance 5 of each other in the corpus. Then the word embedding for a word w is a vector of dimension N, with one coordinate for each dictionary word. The coordinate corresponding to word w2 is Count5(w,w2).'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='The problem with the resulting embedding is that it uses extremely high-dimensional vec- In the LSA article, is was empirically tors. discovered that these embeddings can be re- duced to vectors R300 by doing a rank-300 SVD on the NxN original embeddings matrix.\\n\\nFigure 1: Density of different data sources.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Figure 1: Density of different data sources.\\n\\nWhat mentioned above raise the need for continuous, vector space representations of words that contain data that can be leveraged by models. To be more speciﬁc we want seman- tically similar words to be mapped to nearby points, thus making the representation carry useful information about the word actual mean- ing.\\n\\nii. Word Embeddings Methods\\n\\nWord embeddings models can be divided into main categories:'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='ii. Word Embeddings Methods\\n\\nWord embeddings models can be divided into main categories:\\n\\nThis method was later reﬁned with reweight- ing heuristics, such as taking the loga- rithm, or Pointwise Mutual Information (PMI) [Kenneth et al., 1990] on the count, which is a very popular method.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='The second family of methods, sometimes also referred as neural probabilistic language mod- els, had theoretical and some practical appear- ance as early as 1986 [Hinton, 1986], but ﬁrst to show the utility of pre-trained word em- beddings were arguably Collobert and Weston in 2008 [Collobert and Weston, 2008]. Unlike count-based models, predictive models try to predict a word from its neighbors in terms of learned small, dense embedding vectors.\\n\\nCount-based methods • Predictive methods'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Count-based methods • Predictive methods\\n\\nTwo of the most popular methods which appeared recently are the Glove (Global Vectors for Word Representation) method\\n\\n2\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='2\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016\\n\\n[Pennington et. al., 2014], which is an unsuper- vised learning method, although not predictive in the common sense, and Word2Vec, a family of energy based predictive models, presented by [Mikolov et. al., 2013]. As Word2Vec is the embedding method used in our work it shall be brieﬂy discussed here.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='is also using a simpliﬁed version of NCE [Gutmann and HyvÃd’rinen, 2012] called Neg- ative sampling where the objective function is deﬁned as follows:\\n\\nlogσ(v(cid:48)T wO\\n\\nvwI) +\\n\\nk ∑ i=1\\n\\nEwi∼Pn(w)[σ(−v(cid:48)T wi\\n\\nvwI)]\\n\\niii. Word2Vec'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='iii. Word2Vec\\n\\nWord2vec is a particularly computationally- efﬁcient predictive model for learning word embeddings from raw text. It comes in two ﬂavors, the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model. Algo- rithmically, these models are similar, except that CBOW predicts target words (e.g. ’mat’) from source context words (’the cat sits on the’), while the skip-gram does the inverse and predicts source context-words from the In the skip-gram model (see ﬁg- target words.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='(1) where v(cid:48) w and vw are the \"input\" and \"output\" vector representations of w, σ is the sigmoid function but can also be seen as the network parameters function, and Pn is some noise prob- ability used to sample random words. In the article they recommend k to be between 5 to 20, while the context of predicted words should be 5 or 10. This above objective is later put in the Skip-Gram objective (equtaion 2) to produce optimal word embeddings.\\n\\n1 T\\n\\nT ∑ t=1\\n\\n∑ −c≤j≤c,j(cid:54)=0'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='1 T\\n\\nT ∑ t=1\\n\\n∑ −c≤j≤c,j(cid:54)=0\\n\\nlogp(wt+j|wt)\\n\\n(2)\\n\\nThis objective enables the model to differentiate data from noise by means of logistic regression, thus learning high-quality vector representa- tions.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='The CBOW does exactly the same but the di- rection is inverted. In other words the CBOW trains a binary logistic classiﬁer where, given a window of context words, gives a higher proba- bility to \"correct\" if the next word is correct and a higher probability to \"incorrect\" if the next word is a random sampled one. Notice that CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for smaller datasets. However, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have larger datasets.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Figure 2: The Skip-gram model architecture\\n\\nFinally the vector we used in our work had a dimension of 300. The Network was trained on the Google News dataset which contains 30 billion training words, with negative sampling as mentioned above. These embeddings can be found online2.\\n\\nure 2) a neural network is trained over a large corpus in where the training objective is to learn word vector representations that are good at predicting the nearby words. The method'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='A lot of follow-up work was done on the Word2Vec method. One interesting work was\\n\\n2code.google.com/p/word2vec\\n\\n3\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016\\n\\nFigure 3: Left: Word2Vec t-SNE [Maaten and Hinton, 2008] visualization of our implementation, using text8 dataset and a window size of 5. Only 400 words are visualized. Right: Zooming in of the rectangle in the left ﬁgure.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='done by [Goldberg and Levy, 2014] where ex- periments and theory were used to suggest that these newer methods are related to the older PMI based models, but with new hyperparam- eters and/or term reweightings. In this project appendix you can ﬁnd a simpliﬁed version of Word2Vec we implemented in TensorFlow architecture using the text8 dataset3 and the Skip-Gram model. See ﬁgure 3 for visualized results.\\n\\niv. Word Embeddings Properties'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='iv. Word Embeddings Properties\\n\\nSimilarity: The simplest property of embed- dings obtained by all the methods described above is that similar words tend to have sim- ilar vectors. More formally, the similarity be- tween two words (as rated by humans on a [-1,1] scale) correlates with the cosine similar- ity between those words’ vectors. The fact that'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='as naturally, similar words tend to appear in similar context. This, however creates the problem that antonyms (e.g. cold and hot etc.) also appear with the same context while they are, by deﬁnition, have opposite mean- ing. In [Mikolov et. al., 2013] the score of the (accept,reject) pair is 0.73, and the score of (long,short) is 0.71.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='The problem of antonyms was tackled di- rectly by [Schwartz et al., 2015]. In this arti- cle, the authors introduce a symmetric pattern based approach to word representation which is particularly suitable for capturing word sim- ilarity. Symmetric patterns are a special type of patterns that contain exactly two wildcards and that tend to be instantiated by wildcard pairs such that each member of the pair can take the X or the Y position. For example, the symmetry of the pattern \"X or Y\" is'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='by wildcard pairs such that each member of the pair can take the X or the Y position. For example, the symmetry of the pattern \"X or Y\" is exempli- ﬁed by the semantically plausible expressions \"cats or dogs\" and \"dogs or cats\". Speciﬁcally it was found that two patterns are particularly indicative of antonymy - \"from X to Y\" and \"either X or Y\".'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Figure 4: What words have embeddings closest to a\\n\\ngiven word? From [Collobert et al., 2011]\\n\\nUsing their model the authors were able to achieve a ρ score of 0.56 on the simlex999 dataset [Hill et al., 2016], improving state-of- the-art word2vec skip-gram model results by as much as 5.5-16.7%. Furthermore, the au- thors demonstrated the adaptability of their model to antonym judgment speciﬁcations.\\n\\nwords embedding are related to their context- words stand behind the similarity property'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='words embedding are related to their context- words stand behind the similarity property\\n\\n3http://mattmahoney.net/dc/textdata\\n\\nLinear analogy relationships: A more interesting property of recent embeddings [Mikolov et. al., 2013] is that they can solve\\n\\n4\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='4\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016\\n\\nanalogy relationships via linear algebra. This is despite the fact that those embeddings are being produced via nonlinear methods. For example, vqueen is the most similar answer to the vking − vmen + vwomen equation. It turns out, though, that much more sophisticated relation- ships are also encoded in this way as we can see in ﬁgure 5 below.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='that reduces the effect of the overﬁtting coming from the PMI approximation, thus achieving much better results than higher dimensional vectors.\\n\\nv. Word Embeddings Extensions\\n\\nIn this last subsection we will review two inter- esting works that extend the word embedding concept to phrases and sentences using differ- ent approaches.\\n\\nFigure 5: Relationship pairs in a word embedding. From\\n\\n[Mikolov et. al., 2013]'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='An interesting theoretical work on non-linear embeddings (especially PMI) was done by In their article they sug- [Arora et al., 2015]. gest that the creation of a textual corpus is driven by the random walk of a discourse vec- tor ct ∈ (cid:60)d, which is a unit vector whose direc- tion in space represents what is being talked about. Each word has a (time-invariant) la- tent vector vw ∈ (cid:60)d that captures its correla- tions with the discourse vector. Using a word production model they'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='has a (time-invariant) la- tent vector vw ∈ (cid:60)d that captures its correla- tions with the discourse vector. Using a word production model they predict that words oc- curring at successive time steps will also tend to have vectors that are close together, thus explaining why similar words have similar vec- tors.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='In [Mitchell and Lapata, 2008] the authors address the problem that vector-based mod- els are typically directed at representing words in isolation and methods for constructing rep- resentations for phrases or sentences have re- ceived little attention in the literature. The authors suggests the use of two composition operations, multiplication and addition (and their combination). This way the authors are able to combine word embeddings into phrase or sentences embeddings while taking into ac-'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='addition (and their combination). This way the authors are able to combine word embeddings into phrase or sentences embeddings while taking into ac- count important properties like word order and semantic relationship between words (i.e. semantic composition types).'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='In MIL (Multi Instance Transfer Learning) [Kotzias et al., 2014] the authors propose a neu- ral network model which learns embedding at increasing level of hierarchy, starting from word embeddings, going to sentences and end- ing with entire document embeddings. The au- thors then use transfer learning by pulling the sentence or word embedding that were trained as part of the document embeddings and use them for sentence or word review classiﬁcation or similarity tasks (See ﬁgure 6 below).'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Using the above model the authors introduce the \"RELATIONS = DIRECTIONS\" notion for linear analogies. The authors claim that for each relation R, some direction µR can be found which satisﬁes some equation. This leads to the ﬁnding that given enough examples of a relationship R, it is possible to compute µR us- ing SVD and then given a pair of words with a realtion R and a word c, ﬁnd the best analogy with word d by ﬁnding the pair c and d such that vc −vd has highest possible projection over'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='with a realtion R and a word c, ﬁnd the best analogy with word d by ﬁnding the pair c and d such that vc −vd has highest possible projection over µR. In this way, thay also explain that low di- mension of the vectors has a \"purifying\" effect'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='III. Word Embeddings vs. Image Embeddings\\n\\ni.\\n\\nImage Embeddings\\n\\nImage embeddings, or image features, were wildly used for most image processing and classiﬁcation tasks until the early 2010’s. The features ranged from simple histograms or edge maps to the more sophisticated and very popular SIFT [Lowe, 1999] and HOG\\n\\n5\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016\\n\\nFigure 6: Deep multi-instance approach for [Kotzias et al., 2014]\\n\\ntransfer\\n\\nlearning taken from\\n\\nreview data,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='transfer\\n\\nlearning taken from\\n\\nreview data,\\n\\n[Dalal and Triggs, 2005]. recent years have seen the rise of Deep Learning for image classiﬁcation, especially since 2012 when the AlexNet [Krizhevsky et al., 2012] ar- ticle was published. As those Convolutional Neural Networks (CNN) operated directly on the images, it was suggested that these net- works learn the best image features for the spe- ciﬁc task that they are trained for, thus obviat- ing the need for speciﬁc hand-crafted features.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='However,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='The authors also suggest using the pre-trained one before last layer as a feature map, or Image Embeddings input for simpler SVM classiﬁers. Another popular work was done a bit earlier in [Yangqing et al., 2014] where they also used a pre-trained CNN features as a base for visual recognition tasks. This work was followed by several works with one of them being considered the philosophical fa- ther of the algorithm we implement later. In [Razavian et al., 2014] the authors used the one before'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='of them being considered the philosophical fa- ther of the algorithm we implement later. In [Razavian et al., 2014] the authors used the one before last layer of a network similar to AlexNet that was pre-trained on ImageNet [Russakovsky et al., 2015] as image embed- dings. The authors were able to acheive state- of-art results on several recognition tasks, us- ing simple classiﬁers like SVM. The result was surprising due to the fact that the CNN model was originally optimized for the task of'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='tasks, us- ing simple classiﬁers like SVM. The result was surprising due to the fact that the CNN model was originally optimized for the task of object classiﬁcation in ILSVRC 2013 dataset. Never- theless, it showed itself to be a strong competi- tor to the more sophisticated and highly tuned state-of-the-art methods.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='These works and others suggested that given a large enough database of images, a CNN can learn an image embedding which captures the \"essence\" of the picture and can be used later as an input to different tasks, similar to what is done with word embeddings.\\n\\nii. Similarities and Differences\\n\\nFigure 7: The CNN architecture of AlexNet'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='In recent years though, an extensive research was done on the nature and usage of the ker- nels and features learned by CNN’s. Exten- sive study of CNN feature layers was done in [Zeiler and Fergus, 2014] where they empir- ically conﬁrmed that each convolutional layer of the CNN learns a set of ﬁlters. Their ex- periments also conﬁrm that ﬁlters complexity and expressive power is rising from layer to layer (i.e. as the network goes deeper) starting from simple edge detectors to complex objects'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='and expressive power is rising from layer to layer (i.e. as the network goes deeper) starting from simple edge detectors to complex objects detectors like eyes, ﬂowers, faces and more.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='As we saw earlier Word embedding and Im- age embeddings are similar in the sense that while they are being learned as part of a spe- ciﬁc task, they can be successfully used later for a variety of other tasks. Also, in both cases, similar images or words will usually have sim- ilar embeddings. However Word embeddings and image embeddings differ in some aspects. The ﬁrst difference is that while word embed- dings depends mostly on the words surround- ing the given word, image embeddings usually'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='some aspects. The ﬁrst difference is that while word embed- dings depends mostly on the words surround- ing the given word, image embeddings usually rely on the speciﬁc image itself. This might ex- plain the fact that linear analogies does not ap- pear naturally in images. An interesting work was done in [Reed et al., 2015] where a neural network is trained to make visual analogies'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='6\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016\\n\\nand learn to make them based on appearance, rotation, 3D pose, and various object attributes. Another difference is that while word em- beddings are usually low-ranked, image em- beddings might have same or even higher di- mension then the original image. Those em- beddings are still useful as they contain a lot of information that is extracted from the image and can be used easily.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Weston, et al., 2010] where a representation of images and representation of annotations where both mapped to a joint feature space by learning a mapping which optimizes top-of- the-list ranking measures for images and an- notations. This method, however, learns linear mappings from image features to the embed- ding space, and the available labels were only those provided in the image training set. It could thus not generalize to new classes.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Lastly, we notice that word embeddings are trained on a speciﬁc corpus where the ﬁnal embedding results come as the form of word- vectors. This limits the embedding to be valid only for words that were found in the original corpus while other words will need to be ini- tialized as random vectors (as also done in our work). In images on the other hand, the em- beddings come as a pre-trained model where features or embeddings can be pulled for any sort of image by feeding the model with the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='hand, the em- beddings come as a pre-trained model where features or embeddings can be pulled for any sort of image by feeding the model with the image, making image embeddings models a bit more robust (although they might subjected to other constraints like size and image type).'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='In 2014 DeVise (A Deep Visual-Semantic Embedding Model) model was shown by [Frome et al., 2013]. This work which con- tinued earlier work [Socher et al., 2013], com- bined image embedding and word embedding trained separately into joint similarity metric (see ﬁgure 8). This enabled them to give perfor- mance comparable to a state-of-the-art softmax based model on a ﬂat object classiﬁcation met- ric, while simultaneously making more seman- tically reasonable errors. Their model was also able to'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='based model on a ﬂat object classiﬁcation met- ric, while simultaneously making more seman- tically reasonable errors. Their model was also able to make correct predictions across thou- sands of previously unseen classes by lever- aging semantic knowledge elicited only from un-annotated text.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='iii.\\n\\nJoint Word-Image Embeddings\\n\\nTo conclude this part we will review some of the recent work done in the exciting area of joint word-image embeddings. The ﬁrst immediate usage of joint word-image embed- dings is image annotations or image label- ing. An early notable work was done by'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Another line of works which combines im- age and words embeddings is the image cap- tioning area. In this area the embeddings are usually not combined into a joint space but rather used together to create captions for im- ages. In [Karpathy and Fei Fei, 2015] an image'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Figure 8: : (a) Left: a visual object categorization network with a softmax output layer; Right: a skip-gram language model; Center: the joint model, which is initialized with parameters pre-trained at the lower layers of the other two models. (b) t-SNE visualization [19] of a subset of the ILSVRC 2012 1K label embeddings learned using skip-gram. Taken from [Frome et al., 2013]\\n\\n7\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='7\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016\\n\\nFigure 9: Image captiones generated with Deep Visual-Semantic model Taken from [Karpathy and Fei Fei, 2015]'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Figure 9: Image captiones generated with Deep Visual-Semantic model Taken from [Karpathy and Fei Fei, 2015]\\n\\nfeatures pulled from a pre-trained CNN are fed into a Recurrent Neural Network (RNN) which uses word embeddings in order to gen- erate a captioning for the image, based on the image features and previous words (see ﬁg- ure 9). This sort of combination appears in most image captioning works or video action recognition tasks.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='done in Theano framework by the authors4 and another simpliﬁed version of the model was done in TensorFlow5. In our work we used small parts of the mentioned codes, how- ever most of the code had to be re-written and expanded in order to perform a true implmen- tation of the article’s model.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Finally, a slightly more sophisticated method combining RNN’s and Fisher Vectors can be found in [Lev et al., 2015] where the authors were able to achieve state-of-art results on both image captioning and video action recognition tasks, using transfer learning on the embed- dings learned for the image captioning tasks.\\n\\nIV. CNN for Sentence Classification Model'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='IV. CNN for Sentence Classification Model\\n\\nIn this section and the following we are going to represent our implementation of The Convo- lutional Neural Networks for Sentence Classiﬁ- cation model [Kim ,2014] and our results. This model gained much popularity since it was ﬁrst introduced in late 2014, mainly because it provides a very strong demonstration for the power of pre-trained word embeddings.\\n\\ni. Model details'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='The model architecture, shown in ﬁgure 10, is a slight variant of the CNN architecture of [Collobert et al., 2011]. Formally, let xi ∈ (cid:60)k be the k-dimensional word vector correspond- ing to the i-th word in the sentence. Let n be the length (in number of words) of the longest sentence in the dataset, and let lh be the width of the widest ﬁlter in the network. Then, the input to the network is a k × (n + lh − 1) ma- trix, which is a concatenation of the word em- beddings vectors of each'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='in the network. Then, the input to the network is a k × (n + lh − 1) ma- trix, which is a concatenation of the word em- beddings vectors of each sentences, padded by lh − 1 zero vectors in the beginning and some more zero vectors in the end so there are n + lh − 1 vectors eventually.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='The input of the network is convolved with ﬁlters of different widths (i.e. number of words in the window) and different sizes (i.e. number of features). For example, a feature ci is gen- erated from a window of words xi:i+h−1 by a ﬁlter with width h is:\\n\\nThe model and results were examined in de- tail in [Zhang and Wallace, 2015] where they test many types of conﬁgurations for the model, including different sizes and number of ﬁlters, different activation units and different word embbeddings.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='ci = f(wxi:i+h−1 + b)\\n\\nwhere w are the ﬁlter weights, b is a bias term\\n\\n4https://github.com/yoonkim/CNN_sentence 5https://github.com/dennybritz/cnn-text-\\n\\nA partial implementation of the model was\\n\\nclassiﬁcation-tf\\n\\n8\\n\\n(3)\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016\\n\\nFigure 10: Model architecture with two channels for an example sentence. Taken from [Kim ,2014]'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Figure 10: Model architecture with two channels for an example sentence. Taken from [Kim ,2014]\\n\\nand f is a non-linear function like ReLU. This process is done for all ﬁlters and for all words to create a number of feature maps for each ﬁlter. Next, those features maps are then max- pooled (so we can deal with different sentence sizes) and ﬁnally connected to a soft-max clas- siﬁcation layer.\\n\\nYou can see the dataset statistics summary in table 1 below.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='You can see the dataset statistics summary in table 1 below.\\n\\nMR: Movie reviews with one sentence Classiﬁcation involves reviews positive/negative\\n\\nreview.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='MR: Movie reviews with one sentence Classiﬁcation involves reviews positive/negative\\n\\nreview.\\n\\nFor regularization we employ dropout [Hinton et al., 2014] on the penultimate layer. This entails randomly (under some probabil- ity) setting values in the weight vector to 0. In the original article they also employed constraint on l2 norms of this layer, however [Zhang and Wallace, 2015] found that it had negligible contribution to results and therefore was not used here.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='SST-1: Stanford Sentiment Treebank-an extension of MR but with train/dev/test splits provided and ﬁne-grained la- bels (very positive, positive, neutral, negative, very negative), re-labeled by [Socher et al., 2013] 7'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Training of the network is done by minimiz- ing the corss-entropy loss between predicted labels (soft-max layer) and correct ones. The parameters to be estimated include the weight vector(s), of the ﬁlter(s), the bias term in the ac- tivation function, the weight vector of the soft- max function and (optionally) the word embed- dings. Optimization is performed using SGD [Rumelhart et al., 1988] and back-propagation, with a small mini-batch size speciﬁed later.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='SST-2: Same as SST-1 but with neutral reviews removed and binary labels.\\n\\nSubj: Subjectivity dataset where the task is to classify a sentence as being subjective or objective [Pang and Lee, 2004].\\n\\nTREC: TREC question dataset-task involves classifying a question into 6 ques- tion types (whether the question is about location, numeric information, person, 6https://www.cs.cornell.edu/people/pabo/movie-'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='TREC: TREC question dataset-task involves classifying a question into 6 ques- tion types (whether the question is about location, numeric information, person, 6https://www.cs.cornell.edu/people/pabo/movie-\\n\\nV. Datasets\\n\\nreview-data/'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='V. Datasets\\n\\nreview-data/\\n\\n7http://nlp.stanford.edu/sentiment/ Data is actually provided at the phrase-level and hence we train the model on both phrases and sentences but only score on sentences at test time, as in [Socher et al., 2013] Thus the training set is an order of magnitude larger than listed in table 1.\\n\\n9\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016\\n\\netc.) [Li and Roth, 2002]8.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='9\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016\\n\\netc.) [Li and Roth, 2002]8.\\n\\nIrony: [Wallace et al., 2014] This contains 16,006 sentences from reddit labeled as ironic (or not). The dataset is imbalanced (relatively few sentences are ironic). Thus before training, we under-sampled negative instances to make classes sizes equal. This dataset was not used in the original article but was tested in [Zhang and Wallace, 2015].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='c Data 2 MR 5 SST-1 2 SST-2 Subj 2 TREC 6 2 Irony 51 Opi 10 Tweet 2 Polite\\n\\nl 20 18 19 23 10 75 38 39 53\\n\\nN 10662 11855 9613 10000 5952 1074 7086 25552 4353\\n\\n|V| 18765 17836 16185 21323 9592 6138 7310 33438 10135\\n\\n|Vpre| Test 16488 CV 16262 14838 17913 CV 500 9125 CV 5718 CV 6538 5964 17023 CV 7951\\n\\n2210 1821'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='|Vpre| Test 16488 CV 16262 14838 17913 CV 500 9125 CV 5718 CV 6538 5964 17023 CV 7951\\n\\n2210 1821\\n\\nOpi: Opinions dataset, which comprises sentences extracted from user reviews on a given topic, e.g. \"sound quality of ipod nano\". There are 51 such topics and each topic contains approximately 100 sentences. The test is to classify which opinion belongs to which topic [Ganesan et al., 2010]. This dataset was not used in the original article but was tested in [Zhang and Wallace, 2015].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Table 1: Summary statistics for the datasets after tok- enization. c: Number of target classes. l: Av- erage sentence length. N: Dataset size. |V|: Vocabulary size. |Vpre|: Number of words present in the set of pre-trained word vectors. Test: Test set size (CV means there was no stan- dard train/test split and thus 10-fold CV was used).\\n\\nmentioned below. Below is a list of parame- ters and speciﬁcations that were used for all experiments:'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='mentioned below. Below is a list of parame- ters and speciﬁcations that were used for all experiments:\\n\\nTweet: Tweets from 10 different authors. Classiﬁcation involves classifying which Tweet belongs to which author9. This dataset was not used in the original article.\\n\\nPolite: Sentnces taken Wikipedia editors’ logs which have 25 ranges of politeness [Danescu-Niculescu-Mizil et al., 2013]. We narrowed it to 2 binary classes (po- lite/inpolite). This dataset was not used in the original article.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Word embeddings: We used the pre- trained Word2Vec [Mikolov et. al., 2013] mentioned earlier. Each word embedding is in (cid:60)300. For words that are not found in Word2Vec we randomly initialized them with a uniform distribution in the range of [-0.5,0.5].\\n\\nFilters: We used ﬁlters with window sizes of [3,4,5] with 100 features each. For activation we used ReLU.\\n\\nVI. Experimental Setup\\n\\nDropout rate: 0.5.\\n\\ni. Hyperparameters and Training\\n\\nMini-Batch size: 50.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='VI. Experimental Setup\\n\\nDropout rate: 0.5.\\n\\ni. Hyperparameters and Training\\n\\nMini-Batch size: 50.\\n\\nIn our implementation of the model we experi- mented with a lof of different conﬁgurations. Eventually, since results differences were mi- nor, we decided to use the same architecture and parameters mentioned in the original ar- ticle for all experiments, with some changes'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Optimizer: While AdaDelte optimizer [Zeiler, 2012] was used in the original article. We decided to use the more recent ADAM optimizer [Kingma and Ba, 2014], as it seemed to converge much faster (i.e. needed less epochs on training) and in some cases improved the results.\\n\\n8http://cogcomp.cs.illinois.edu/Data/QA/QC/ 9http://www.cs.huji.ac.il/ nogazas/pages/projects.html,\\n\\nThanks to Noga Zaslavsky\\n\\nLearning rate: 0.001. We lower it to\\n\\n10'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Thanks to Noga Zaslavsky\\n\\nLearning rate: 0.001. We lower it to\\n\\n10\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016\\n\\nModel CNN-rand CNN-static CNN-non-static\\n\\nMR Orig Ours Orig Ours Orig Ours Orig Ours Orig Ours 97.6 76.1 98.2 81.0 98.6 81.5\\n\\nSST-1\\n\\nSST-2\\n\\nSubj\\n\\nTREC\\n\\n41 48.1 47.3\\n\\n45.0 45.5 48.0\\n\\n76.4 80.3 80.5\\n\\n80.2 85.4 85.8\\n\\n82.7 86.8 87.2\\n\\n89.6 93.0 93.4\\n\\n91.1 92.5 93\\n\\n91.2 92.8 93.6\\n\\nTable 2: Results on datasets that were tested in [Kim ,2014] (Orig above) .'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='82.7 86.8 87.2\\n\\n89.6 93.0 93.4\\n\\n91.1 92.5 93\\n\\n91.2 92.8 93.6\\n\\nTable 2: Results on datasets that were tested in [Kim ,2014] (Orig above) .\\n\\n0.0005 after 8 epchs and to 0.00005 after 16 epochs. Notice that the original article didn’t mention the learning rate.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Number of epochs: This was also not mentioned in the original article but can be found in the authors code10. We used 25 epochs for the static version (see Model Variations below). For non static we used either 4 (MR, SST1, SST2, Subj), 10 (Polite), 16 (Twitter, Opi), or 25 (TREC). For the random version we used 25 except for Tweet where we used 10, and MR and SST-1 where we used 4.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='l2-loss: We added l2-loss with λ = 0.15 on the weights and biases of the ﬁnal layer. Altough this was not done in the original artcle, we found it to slightly improve the results.\\n\\nCNN-static: model with pre-trained vec- tors from word2vec. All words-including the unknown ones that are randomly initialized-are kept static and only the other parameters of the model are learned.\\n\\nCNN-non-static: Same as above but the pretrained vectors are ﬁne-tuned for each task.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='CNN-non-static: Same as above but the pretrained vectors are ﬁne-tuned for each task.\\n\\nThe authors also used a multi-channel model where one channel is static and the other is not. However, experiments showed that on most datasets, this did not improve the results. As implementing this would have required a lot more coding, we decided to drop it.\\n\\nVII. Results and discussion\\n\\nAs mentioned earlier, we decided not to use l2 constrains on the norms due to their negligi- ble contribution.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='VII. Results and discussion\\n\\nAs mentioned earlier, we decided not to use l2 constrains on the norms due to their negligi- ble contribution.\\n\\nii. Model Variations'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='ii. Model Variations\\n\\nIn this section we will compare the results we got in our implementation to the ones achieved in the original article. Full results can be found in the original article, and we do note that most of them are state-of-art results, or com- parable. For datasets that were not present in the original article we shall compare with other achieved results, whether ours or others’.\\n\\nWe experiment with several variants of the model like in the original article.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='We experiment with several variants of the model like in the original article.\\n\\nCNN-rand: Our baseline model where all words are randomly initialized and then modiﬁed during training.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='CNN-rand: Our baseline model where all words are randomly initialized and then modiﬁed during training.\\n\\n10We note here that in the original article they used early stopping with a dev set. However, the early stopping parameters are not mentioned and experiments demanded a lot of coding which is behind the scope of this project. We do assume that the 25 number used in the code might be close enough to the actual number used in the article.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='In table 2 above we can see a comparison between our results and the ones in the origi- nal article [Kim ,2014]. We can see that overall, our results are comparable (and sometimes bet- ter) to the ones in the original article. We also see that like in the original article, our base- line model with all randomly initialized words (CNN-rand) does not perform well on its own (on most cases). These results suggest that the pre-trained vectors are good, ’universal’ feature extractors and can be'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='not perform well on its own (on most cases). These results suggest that the pre-trained vectors are good, ’universal’ feature extractors and can be utilized across datasets. Finetuning the pre-trained vectors'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='11\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016\\n\\nModel Random Static Non-Static ConvSent SVM+TF-IDF\\n\\nOpi 64.8 65.3 66.4 64.9 -\\n\\nIrony 60.2 60.5 62.1 67.112 -\\n\\nTweet Polite 89.1 83.8 89.2 - 92.5\\n\\n66.2 66.2 65.7 - -\\n\\nTable 3: Resutls for datasets that were not used is\\n\\nin the original article. [Zhang and Wallace, 2015].\\n\\nConvesent\\n\\nwere trained on news. This makes the static version a bad choice as it keep the embeddings random during training.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Convesent\\n\\nwere trained on news. This makes the static version a bad choice as it keep the embeddings random during training.\\n\\nOn this dataset we also applied a simple SVM classiﬁer on the TF-IDF features of each tweet. This simple classiﬁer produced much better results, as TF-IDF features are sensitive to uniqe words in a tweet (like hashtags), that usually indicates which is the author, thus mak- ing classiﬁcation easier.\\n\\nfor each task gives still further improvements (CNN-non-static).'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='The differences in some of our results can be related to the different optimizer we used, and the fact that we did not use early stopping. We do note that our results (at least on the non-static version) were achieved with much less training than the original article11. We also note that on the TREC dataset we were able to achieve a new state-of-art results, improving the current one (95%) by 3.6%. Both these beneﬁts can be related to the use of ADAM [Kingma and Ba, 2014] optimizer.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='On table 3 we can see our results for datasets that were not used in the original article. We also compare them to other results where ap- plicable.\\n\\nOn the Opi and Irony dataset we note that the general line of improved results with pre- trained vectors is maintained. On the Opi dataset we were also able to achieve a new state-of-art result. We were also able to achieve comparable results on the Irony dataset. No- tice that the other reported result is AUC and not accuracy.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='The other two results are interesting. On the Tweet dataset we notice that random vectors actually perform a lot better than pre-trained static ones. The reason is that on this dataset, almost half of the vocabulary was not found in the Word2Vec embeddings. This makes sense, as tweets usually contain a lot of marks (for example :-) ) and hashtags which would natu- rally will not be available in embeddings that'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='11That, if we take the 25 epochs in the code we men- tioned earlier as an indication to the nubmer of epochs training used in the original article\\n\\nOn the Poilte dataset we notice that results does not matter on the choice of model. The results themselves are also not very good. This results needs further inspection but they might suggests that this model is not ﬁtted for this task or that politeness is a complicated task for automatic classiﬁcation.\\n\\nVIII. Conclusions and Future Directions'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='VIII. Conclusions and Future Directions\\n\\nIn this work we reviewed word embeddings. We saw their origins, discussed the different methods for creating them, and saw some of their interesting properties. We think that word embeddings is a very exciting topic for both research and applications, and expect that a lot research is going to be carried for better understanding of their properties and better creation methods.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='In this work we also compared Image fea- tures and word embeddings and saw how they can be combined to build learning algorithms that can ﬁnally gain a good understanding of pictures and scenes. This area is just in its be- ginning and we expect a lot of work to be car- ried towards creating a hybrid system which gains understanding of both vision and lan- guage, and that combines those understand- ings together to the beneﬁt of both ﬁelds.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Finally, we saw that despite little tuning of hyperparameters, a simple CNN with one layer of convolution, trained on top of Word2Vec embeddings, performs remarkably well on sen- tence classiﬁcation tasks. These results add to the well-established evidence that unsuper- vised pre-training of word vectors is an impor- tant ingredient in deep learning for NLP.\\n\\n12\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='To conclude this work we propose here two lines for future work that we think might be interesting to check. First, in the spirit of [Kotzias et al., 2014], we notice that in our net- work, the one before last layer is actually learn- ing sentence embeddings. It might be interest- ing to train the network on some classiﬁcation task with a relatively large dataset, and then use the learned sentence embeddings in the same fashion word embeddings are used in our work. For example we can train the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='large dataset, and then use the learned sentence embeddings in the same fashion word embeddings are used in our work. For example we can train the net- work on the MR task and then take the learned sentence embeddings and use them as an em- bedding input to some document classiﬁcation task. We can then check if this method achieves improvement over models that try to classify documents using only pre-trained word em- beddings.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='The second line of research is in the spirit of [Zeiler and Fergus, 2014]. ConvNets visualiza- tion helped to gain a lot of insights about image sturctre and how features in increasing level of complexity are combined to create images. It might be interesting to apply those same method of visualization to the ﬁlters used in our, or similar works and see if the ConvNet ﬁlters learn some interesting semantic proper- ties or compositions that can give insights on the structure of language and how'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='see if the ConvNet ﬁlters learn some interesting semantic proper- ties or compositions that can give insights on the structure of language and how computers (or even humans) percept them.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='References\\n\\nKoray Kavukcuoglu, and Pavel Kuksa. \"Natural language processing (almost) from scratch.\" Journal of Machine Learn- ing Research 12, no. Aug (2011): 2493- 2537.\\n\\n[Dalal and Triggs, 2005] Dalal, Navneet, and Bill Triggs. \"Histograms of oriented gra- dients for human detection.\" 2005 IEEE Computer Society Conference on Com- puter Vision and Pattern Recognition (CVPR’05). Vol. 1. IEEE, 2005.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Danescu-Niculescu-Mizil et al., 2013] Cristian, Danescu-Niculescu-Mizil, Jure Moritz Sudhof, Dan Jurafsky, Leskovec, and Christopher Potts. \"A computational approach to politeness with application to social factors.\" arXiv preprint arXiv:1306.6078 (2013).'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Deerwester et al., 1990] Deerwester, Scott, Su- san T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. \"In- dexing by latent semantic analysis.\" Jour- nal of the American society for informa- tion science 41, no. 6 (1990): 391.\\n\\n[Firth, 1957] Firth, J.R. (1957). \"A synopsis of linguistic theory 1930-1955\". Studies in Linguistic Analysis (Oxford: Philological Society): 1-32. Reprinted in F.R. Palmer, ed. (1968). Selected Papers of J.R. Firth 1952-1959. London: Longman.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Arora et al., 2015] Arora, Sanjeev, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. \"Rand-walk: A latent variable model approach to word embeddings.\" arXiv preprint arXiv:1502.03520 (2015).\\n\\n[Collobert and Weston, 2008] Collobert, Ro- nan, and Jason Weston. \"A uniﬁed ar- chitecture for natural language process- ing: Deep neural networks with multitask learning.\" Proceedings of the 25th inter- national conference on Machine learning. ACM. (2008).'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Collobert et al., 2011] Collobert, Ronan, Jason Weston, LÃl’on Bottou, Michael Karlen,\\n\\n[Frome et al., 2013] Frome, Andrea, Greg S. Corrado, Jon Shlens, Samy Bengio, Jeff Dean, and Tomas Mikolov. \"Devise: A deep visual-semantic embedding model.\" In Advances in neural information pro- cessing systems, pp. 2121-2129. 2013.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Kavita, [Ganesan et al., 2010] Ganesan, ChengXiang Zhai, and Jiawei Han. \"Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions.\" Proceedings of the 23rd international conference on computational linguistics. Association for Computational Linguistics, 2010.\\n\\n13\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016\\n\\n[Goldberg and Levy, 2014] Goldberg,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='13\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016\\n\\n[Goldberg and Levy, 2014] Goldberg,\\n\\nYoav, and Omer Levy. \"word2vec Explained: de- riving Mikolov et al.’s negative-sampling word-embedding method.\" arXiv preprint arXiv:1402.3722 (2014).'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Gutmann and HyvÃd’rinen, 2012] Gutmann, Michael U., and Aapo HyvÃd’rinen. \"Noise-contrastive estimation of un- normalized statistical models, with applications to natural image statistics.\" Journal of Machine Learning Research 13.Feb (2012): 307-361.\\n\\n[Hill et al., 2016] Hill, Felix, Roi Reichart, and Anna Korhonen. \"Simlex-999: Evaluating semantic models with (genuine) similar- ity estimation.\" Computational Linguistics (2016).\\n\\n[Hinton, 1986] Hinton, Geoffrey E.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Hinton, 1986] Hinton, Geoffrey E.\\n\\n\"Dis- tributed representations.\" Parallel Dis- tributed Processing: Explorations in the Microstructure of Cognition (1986).\\n\\n[Hinton et al., 2014] Srivastava, Nitish, Geof- frey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. \"Dropout: a simple way to prevent neural networks from overﬁtting.\" Journal of Ma- chine Learning Research 15, no. 1 (2014): 1929-1958.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Karpathy and Fei Fei, 2015] Karpathy, An- drej, and Li Fei-Fei. \"Deep visual-semantic alignments for generating image de- scriptions.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.\\n\\nstochastic optimization.\" arXiv preprint arXiv:1412.6980 (2014).\\n\\n[Kotzias et al., 2014] Kotzias, Dimitrios, Misha Denil, Phil Blunsom, and Nando de Freitas. \"Deep multi-instance transfer learning.\" arXiv preprint arXiv:1411.3128 (2014).\\n\\n[Krizhevsky et al., 2012] Krizhevsky,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Krizhevsky et al., 2012] Krizhevsky,\\n\\nAlex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet classiﬁcation with deep convo- lutional neural networks.\" Advances in neural information processing systems. 2012.\\n\\n[Lev et al., 2015] Lev, Guy, Gil Sadeh, Ben- jamin Klein, and Lior Wolf. \"RNN Fisher Vectors for Action Recognition and Image Annotation.\" arXiv preprint arXiv:1512.03958 (2015).'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Li and Roth, 2002] Li, Xin, and Dan Roth. \"Learning question classiﬁers.\" Proceed- ings of the 19th international conference on Computational linguistics-Volume 1. Association for Computational Linguis- tics, 2002.\\n\\n[Lowe, 1999] Lowe, David G. \"Object recogni- tion from local scale-invariant features.\" Computer vision, 1999. The proceedings of the seventh IEEE international confer- ence on. Vol. 2. Ieee, 1999.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Maaten and Hinton, 2008] Maaten, Laurens van der, and Geoffrey Hinton. \"Visualiz- ing data using t-SNE.\" Journal of Machine Learning Research 9.Nov (2008): 2579- 2605.\\n\\n[Kenneth et al., 1990] Church, Kenneth Ward, and Patrick Hanks. \"Word association norms, mutual information, and lexi- cography.\" Computational linguistics 16.1 (1990): 22-29.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Mikolov et. al., 2013] Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. \"Efﬁcient estimation of word represen- tations in vector space.\" arXiv preprint arXiv:1301.3781 (2013).\\n\\n[Kim ,2014] Kim, Yoon. \"Convolutional neu- ral networks for sentence classiﬁcation.\" arXiv preprint arXiv:1408.5882 (2014).\\n\\n[Kingma and Ba, 2014] Kingma,\\n\\nDiederik, and Jimmy Ba. \"Adam: A method for'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Kingma and Ba, 2014] Kingma,\\n\\nDiederik, and Jimmy Ba. \"Adam: A method for\\n\\n[Mitchell et al., 2008] Mitchell, Tom M., Svet- lana V. Shinkareva, Andrew Carlson, Kai- Min Chang, Vicente L. Malave, Robert A. Mason, and Marcel Adam Just. \"Predict- ing human brain activity associated with\\n\\n14\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016\\n\\nthe meanings of nouns.\" science 320, no. 5880 (2008): 1191-1195.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='14\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016\\n\\nthe meanings of nouns.\" science 320, no. 5880 (2008): 1191-1195.\\n\\nWilliams. \"Learning representations by back-propagating errors.\" Cognitive mod- eling 5.3 (1988): 1.\\n\\n[Mitchell and Lapata, 2008] Mitchell, Jeff, and Mirella Lapata. \"Vector-based Models of Semantic Composition.\" ACL. 2008.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Mitchell and Lapata, 2008] Mitchell, Jeff, and Mirella Lapata. \"Vector-based Models of Semantic Composition.\" ACL. 2008.\\n\\n[Osgood, 1964] Osgood, Charles E. \"Semantic differential technique in the comparative study of cultures.\" American Anthropolo- gist 66.3 (1964): 171-200.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Russakovsky et al., 2015] Russakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang et al. \"Imagenet large scale visual recog- nition challenge.\" International Journal of Computer Vision 115, no. 3 (2015): 211- 252.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Pang and Lee, 2004] Pang, B., Lee, L. A sen- timental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd annual meeting on Association for Computational Linguistics (p. 271). As- sociation for Computational Linguistics. 2004.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Pang and Lee, 2005] Pang, Bo, and Lillian Lee. \"Seeing stars: Exploiting class re- lationships for sentiment categorization with respect to rating scales.\" Proceedings of the 43rd annual meeting on association for computational linguistics. Association for Computational Linguistics, 2005.\\n\\n[Pennington et. al., 2014] Pennington, Jeffrey, Richard Socher, and Christopher D. Man- ning. \"Glove: Global Vectors for Word Representation.\" EMNLP. Vol. 14. (2014).'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Salton et al., 1975] Salton, Gerard, Anita Wong, and Chung-Shu Yang. \"A vector space model for automatic indexing.\" Communications of the ACM 18.11 (1975): 613-620.\\n\\n[Schwartz et al., 2015] Schwartz, Roy, Roi Re- ichart, and Ari Rappoport. \"Symmetric pattern based word embeddings for im- proved word similarity prediction.\" Proc. of CoNLL. 2015.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Socher et al., 2013] Socher, Richard, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. \"Recursive deep models for semantic compositionality over a sentiment treebank.\" In Proceedings of the conference on empirical methods in natural language processing (EMNLP), vol. 1631, p. 1642. 2013.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Razavian et al., 2014] Sharif Razavian, Ali, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. \"CNN features off- the-shelf: an astounding baseline for recognition.\" In Proceedings of the IEEE Conference on Computer Vision and Pat- tern Recognition Workshops, pp. 806-813. 2014.\\n\\n[Reed et al., 2015] Reed, Scott E., Yi Zhang, Yuting Zhang, and Honglak Lee. \"Deep visual analogy-making.\" In Advances in Neural Information Processing Systems, pp. 1252-1260. 2015.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Socher et al., 2013] Socher, Richard, Milind Ganjoo, Christopher D. Manning, and Andrew Ng. \"Zero-shot learning through cross-modal transfer.\" In Advances in neu- ral information processing systems, pp. 935-943. 2013.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Wallace et al., 2014] Byron C Wallace, Laura Kertz Do Kook Choe, and Eugene Char- niak. Humans require context to infer ironic intent (so computers probably do, too). In Proceedings of the Annual Meet- ing of the Association for Computational Linguistics (ACL), 2014, pages 512-516.\\n\\n[Rumelhart et al., 1988] Rumelhart, David E., and Ronald J.\\n\\nGeoffrey E. Hinton,\\n\\n[Weston, et al., 2010] Weston,\\n\\nJason, Samy Bengio, and Nicolas Usunier. \"Large scale\\n\\n15'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='Geoffrey E. Hinton,\\n\\n[Weston, et al., 2010] Weston,\\n\\nJason, Samy Bengio, and Nicolas Usunier. \"Large scale\\n\\n15\\n\\nWord Embeddings for Sentence Classiﬁcation Tasks • July 2016\\n\\nimage annotation: learning to rank with joint word-image embeddings.\" Machine learning 81.1 (2010): 21-35.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='image annotation: learning to rank with joint word-image embeddings.\" Machine learning 81.1 (2010): 21-35.\\n\\n[Yangqing et al., 2014] Jia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. \"Caffe: Convolutional architecture for fast feature embedding.\" In Proceedings of the 22nd ACM international conference on Multi- media, pp. 675-678. ACM, 2014.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Zhang and Wallace, 2015] Zhang, Ye, and By- ron Wallace. \"A Sensitivity Analysis of (and Practitioners’ Guide to) Convolu- tional Neural Networks for Sentence Clas- siﬁcation.\"arXiv preprint arXiv:1510.03820 (2015).\\n\\n[Zeiler, 2012] Zeiler, Matthew D. \"ADADELTA: an adaptive learning rate method.\" arXiv preprint arXiv:1212.5701 (2012).'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\Embedding.pdf'}, page_content='[Zeiler, 2012] Zeiler, Matthew D. \"ADADELTA: an adaptive learning rate method.\" arXiv preprint arXiv:1212.5701 (2012).\\n\\n[Zeiler and Fergus, 2014] Zeiler, Matthew D., and Rob Fergus. \"Visualizing and under- standing convolutional networks.\" Euro- pean Conference on Computer Vision. Springer International Publishing, 2014.\\n\\n16'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='4 2 0 2\\n\\nt c O 7 1\\n\\n] L C . s c [\\n\\n0 1 v 5 3 4 6 0 . 7 0 3 2 : v i X r a\\n\\nA Comprehensive Overview of Large Language Models\\n\\nHumza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g, Muhammad Usmanf,g, Naveed Akhtarh,j, Nick Barnesi, Ajmal Mianj'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='aThe University of Sydney, Sydney, Australia bUniversity of Engineering and Technology (UET), Lahore, Pakistan cThe Chinese University of Hong Kong (CUHK), HKSAR, China dUniversity of Technology Sydney (UTS), Sydney, Australia eCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia fKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia gSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia hThe'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='and Minerals (KFUPM), Dhahran, Saudi Arabia gSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia hThe University of Melbourne (UoM), Melbourne, Australia iAustralian National University (ANU), Canberra, Australia jThe University of Western Australia (UWA), Perth, Australia'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Abstract'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to provide not only a systematic survey but also a quick,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='the advanced topics at the frontier of research in LLMs. This review article is intended to provide not only a systematic survey but also a quick, comprehensive reference for the researchers and practitioners to draw insights from extensive, informative summaries of the existing works to advance the LLM research.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Keywords: Large Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking\\n\\n1. Introduction'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Language plays a fundamental role in facilitating commu- nication and self-expression for humans and their interaction with machines. The need for generalized models stems from the growing demand for machines to handle complex language tasks, information re- trieval, conversational interactions, etc. Recently, significant breakthroughs have been witnessed in language models, pri- marily attributed to transformers [1], increased computational capabilities, and the availability of large-scale'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='witnessed in language models, pri- marily attributed to transformers [1], increased computational capabilities, and the availability of large-scale training data. These developments have brought about a revolutionary trans- formation by enabling the creation of LLMs that can approxi- mate human-level performance on various tasks [2, 3]. Large'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='including translation, summarization,\\n\\n∗Equal contribution Email addresses: humza_naveed@yahoo.com (Humza Naveed),\\n\\naukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi Qiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib), saeed.anwar@kfupm.edu.sa (Saeed Anwar), muhammad.usman@kfupm.edu.sa (Muhammad Usman), naveed.akhtar1@unimelb.edu.au (Naveed Akhtar), nick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au (Ajmal Mian)'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Figure 1: The trend of papers released over the years containing keywords \"Large Language Model\", \"Large Language Model + Fine-Tuning\", and \"Large Language Model + Alignment\".\\n\\nPreprint submitted to Elsevier\\n\\nOctober 18, 2024\\n\\n2019\\n\\nT5 (Oct)\\n\\nGPT-3 (May)\\n\\nWebGPT (Dec)\\n\\nOPT-IML\\n\\nTK-Instruct (May)\\n\\nmT0 (Dec)\\n\\nWizard-LM\\n\\nVicuna\\n\\nAlpaca (Mar)\\n\\nHuaTuo (Apr)\\n\\nKoala (May)Wizard-Coder (Jun)\\n\\nGoat\\n\\nPanGu-α (Apr)\\n\\nCPM-2 (Jun)\\n\\nGPT-NeoX-20B (Apr)\\n\\nCodeGen (Mar)\\n\\nGalactica (Nov)\\n\\nGLM (Oct)\\n\\nOPT\\n\\nUL2 (May)'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Koala (May)Wizard-Coder (Jun)\\n\\nGoat\\n\\nPanGu-α (Apr)\\n\\nCPM-2 (Jun)\\n\\nGPT-NeoX-20B (Apr)\\n\\nCodeGen (Mar)\\n\\nGalactica (Nov)\\n\\nGLM (Oct)\\n\\nOPT\\n\\nUL2 (May)\\n\\nLLaMA (Feb)\\n\\nLLaMA 2 (Jul)\\n\\nMPT (Jun)\\n\\nCodeT5+\\n\\nCode Llama (Aug)\\n\\nStarCoder\\n\\nXuan Yuan 2.0 (May)\\n\\n2020\\n\\n2021202220232024\\n\\nmT5 (Oct)\\n\\nHyperCLOVA (Sep)\\n\\nERNIE 3.0\\n\\nCodex (Jul)\\n\\nJurassic-1 (Aug)\\n\\nYuan 1.0 (Oct)\\n\\nGopher (Dec)\\n\\nERNIE 3.0 Titan\\n\\nGLaM\\n\\nLaMDA\\n\\nT0 (Oct)\\n\\nChatGPT (Nov)\\n\\nSparrow (Sep)\\n\\nFLAN-U-PaLM (Oct)\\n\\nBard (Oct)\\n\\nMT-NLG (Jan)\\n\\nAlphaCode (Feb)'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Gopher (Dec)\\n\\nERNIE 3.0 Titan\\n\\nGLaM\\n\\nLaMDA\\n\\nT0 (Oct)\\n\\nChatGPT (Nov)\\n\\nSparrow (Sep)\\n\\nFLAN-U-PaLM (Oct)\\n\\nBard (Oct)\\n\\nMT-NLG (Jan)\\n\\nAlphaCode (Feb)\\n\\nChinchilla (Mar)\\n\\nPaLM (Apr)\\n\\nU-PALM (Oct)\\n\\nBLOOM (Nov)\\n\\nAlexaTM (Aug)\\n\\nPaLM2 (May)\\n\\nGPT-4\\n\\nPanGu-Σ (Mar)\\n\\nBloombergGPT\\n\\nClaude\\n\\nGemini (Dec)\\n\\nDeepSeek (Jan)\\n\\nLLaMA 3\\n\\nGrok-1 (Mar)\\n\\nSnowflake Arctic (Apr)DeepSeek-V2 (May)\\n\\nMixtral 8x22B\\n\\nNemotron (Feb)\\n\\nGPT-4o (May)\\n\\nOpenAI o1 (Sep)\\n\\nGemini-1.5 (Feb)\\n\\nGrok-1.5 (Apr)'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='OpenAI o1 (Sep)\\n\\nGemini-1.5 (Feb)\\n\\nGrok-1.5 (Apr)\\n\\nFigure 2: Chronological display of LLM releases: blue cards represent ‘pre-trained’ models, while orange cards correspond to ‘instruction-tuned’ models. Models on the upper half signify open-source availability, whereas those on the bottom are closed-source. The chart illustrates the increasing trend towards instruction-tuned and open-source models, highlighting the evolving landscape and trends in natural language processing research.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Language Models (LLMs) have emerged as cutting-edge arti- ficial intelligence systems that can process and generate text with coherent communication [4] and generalize to multiple tasks [5, 6]. The historical progress in natural language processing (NLP) evolved from statistical to neural language modeling and then from pre-trained language models (PLMs) to LLMs. While conventional language modeling (LM) trains task-specific mod- els in supervised settings, PLMs are trained in a self-supervised'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='(PLMs) to LLMs. While conventional language modeling (LM) trains task-specific mod- els in supervised settings, PLMs are trained in a self-supervised setting on a large corpus of text [7, 8, 9] with the aim of learning a generic representation that is shareable among various NLP tasks. After fine-tuning for downstream tasks, PLMs surpass the performance gains of traditional language modeling (LM). The larger PLMs bring more performance gains, which has led to the transitioning of PLMs to LLMs'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='gains of traditional language modeling (LM). The larger PLMs bring more performance gains, which has led to the transitioning of PLMs to LLMs by significantly increas- ing model parameters (tens to hundreds of billions) [10] and training dataset (many GBs and TBs) [10, 11]. Following this development, numerous LLMs have been proposed in the lit- erature [10, 11, 12, 6, 13, 14, 15]. An increasing trend in the number of released LLMs and names of a few significant LLMs proposed over the years are'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[10, 11, 12, 6, 13, 14, 15]. An increasing trend in the number of released LLMs and names of a few significant LLMs proposed over the years are shown in Fig 1 and Fig 2, respec- tively. The early work on LLMs, such as T5 [10] and mT5 [11] em- ployed transfer learning until GPT-3 [6] showed LLMs are zero-shot transferable to downstream tasks without fine-tuning. LLMs accurately respond to task queries when prompted with task descriptions and examples. However, pre-trained LLMs fail to follow'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='fine-tuning. LLMs accurately respond to task queries when prompted with task descriptions and examples. However, pre-trained LLMs fail to follow user intent and perform worse in zero-shot set- tings than in few-shot. Fine-tuning them with task instruc- tions data [16, 17, 18, 19] and aligning with human prefer- ences [20, 21] enhances generalization to unseen tasks, im- proving zero-shot performance significantly and reducing mis- aligned behavior. In addition to better generalization and'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='to unseen tasks, im- proving zero-shot performance significantly and reducing mis- aligned behavior. In addition to better generalization and domain adaptation, LLMs appear to have emergent abilities, such as reasoning, planning, decision-making, in-context learning, answering in zero-shot settings, etc. These abilities are known to be ac- quired by them due to their gigantic scale even when the pre- trained LLMs are not trained specifically to possess these at- tributes [22, 23, 24]. Such'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='by them due to their gigantic scale even when the pre- trained LLMs are not trained specifically to possess these at- tributes [22, 23, 24]. Such abilities have led LLMs to be widely adopted in diverse settings, including multi-modal, robotics,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='tool manipulation, question answering, autonomous agents, etc. Various improvements have also been suggested in these areas either by task-specific training [25, 26, 27, 28, 29, 30, 31] or better prompting [32]. The LLMs abilities to solve diverse tasks with human-level performance come at the cost of slow training and inference, extensive hardware requirements, and higher running costs. Such requirements have limited their adoption and opened up opportunities to devise better architectures'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='requirements, and higher running costs. Such requirements have limited their adoption and opened up opportunities to devise better architectures [15, 33, 34, 35] and training strategies [36, 37, 21, 38, 39, 40, 41]. Param- eter efficient tuning [38, 41, 40], pruning [42, 43], quantiza- tion [44, 45], knowledge distillation, and context length inter- polation [46, 47, 48, 49] among others are some of the methods widely studied for efficient LLM utilization. Due to the success of LLMs on a wide'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='polation [46, 47, 48, 49] among others are some of the methods widely studied for efficient LLM utilization. Due to the success of LLMs on a wide variety of tasks, the research literature has recently experienced a large influx of LLM-related contributions. Researchers have organized the LLMs literature in surveys [50, 51, 52, 53], and topic-specific surveys in [54, 55, 56, 57, 58]. In contrast to these surveys, our contribution focuses on providing a comprehensive yet concise overview of the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='surveys in [54, 55, 56, 57, 58]. In contrast to these surveys, our contribution focuses on providing a comprehensive yet concise overview of the general direction of LLM research. This arti- cle summarizes architectural and training details of pre-trained LLMs and delves deeper into the details of concepts like fine- tuning, multi-modal LLMs, augmented LLMs, datasets, eval- uation, applications, challenges, and others to provide a self- contained comprehensive overview. Our key contributions'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='LLMs, datasets, eval- uation, applications, challenges, and others to provide a self- contained comprehensive overview. Our key contributions are summarized as follows.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='We present a survey on the developments in LLM research, providing a concise, comprehensive overview of the direc- tion.\\n\\nWe present extensive summaries of pre-trained models that include fine-grained details of architecture and training de- tails.\\n\\nWe summarize major findings of the popular contributions and provide a detailed discussion on the key design and development aspects of LLMs to help practitioners effec- tively leverage this technology.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='In this self-contained article, we cover a range of con- cepts to present the general direction of LLMs compre- hensively, including background, pre-training, fine-tuning,\\n\\n2\\n\\nFigure 3: A broader overview of LLMs, dividing LLMs into seven branches: 1. Pre-Training 2. Fine-Tuning 3. Efficient 4. Inference 5. Evaluation 6. Applications 7. Challenges\\n\\nmulti-modal LLMs, augmented LLMs, LLMs-powered agents, datasets, evaluation, etc.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='We loosely follow the existing terminology to ensure a stan- dardized outlook of this research direction. For instance, fol- lowing [50], our survey discusses pre-trained LLMs with 10B parameters or more. We refer the readers interested in smaller pre-trained models to [51, 52, 53]. The organization of this paper is as follows. Section 2 discusses the background of LLMs. Section 3 focuses on LLMs overview, architectures, training pipelines and strategies, fine-tuning, and'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='utilization in different domains. Section 4 highlights the config- uration and parameters that play a crucial role in the function- ing of these models. Summary and discussions are presented in section 3.8. The LLM training and evaluation, datasets, and benchmarks are discussed in section 5, followed by challenges and future directions, and conclusion in sections 7 and 8, re- spectively.\\n\\n3\\n\\n2. Background'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='3\\n\\n2. Background\\n\\nWe provide the relevant background to understand the fun- damentals related to LLMs in this section. We briefly discuss necessary components in LLMs and refer the readers interested in details to the original works.\\n\\n2.1. Tokenization'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='2.1. Tokenization\\n\\nTokenization [59] is an essential pre-processing step in LLM training that parses the text into non-decomposing units called tokens. Tokens can be characters, subwords [60], sym- bols [61], or words, depending on the tokenization process. Some of the commonly used tokenization schemes in LLMs include wordpiece [62], byte pair encoding (BPE) [61], and un- igramLM [60]. Readers are encouraged to refer to [63] for a detailed survey.\\n\\n2.2. Encoding Positions'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='The transformer processes input sequences in parallel and independently of each other. Moreover, the attention mod- ule in the transformer does not capture positional information. As a result, positional encodings were introduced in trans- former [64], where a positional embedding vector is added to the token embedding. Variants of positional embedding include absolute, relative, or learned positional encodings. Within rel- ative encoding, Alibi and RoPE are two widely used positional'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='embedding include absolute, relative, or learned positional encodings. Within rel- ative encoding, Alibi and RoPE are two widely used positional embeddings in LLMs. Alibi [65]: It subtracts a scalar bias from the attention score that increases with the distance between token positions. This favors using recent tokens for attention. RoPE [66]: It rotates query and key representations at an an- gle proportional to the token absolute position in the input sequence, resulting in a relative'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='It rotates query and key representations at an an- gle proportional to the token absolute position in the input sequence, resulting in a relative positional encoding scheme which decays with the distance between the tokens.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='2.3. Attention in LLMs'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Attention assigns weights to input tokens based on impor- tance so that the model gives more emphasis to relevant tokens. Attention in transformers [64] calculates query, key, and value mappings for input sequences, where the attention score is obtained by multiplying the query and key, and later used to weight values. We discuss different attention strategies used in LLMs below. Self-Attention [64]: Calculates attention using queries, keys, and values from the same block (encoder or decoder).'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='strategies used in LLMs below. Self-Attention [64]: Calculates attention using queries, keys, and values from the same block (encoder or decoder). Cross Attention: It is used in encoder-decoder architectures, where encoder outputs are the queries, and key-value pairs come from the decoder. Sparse Attention [67]: Self-attention has O(n2) time complex- ity which becomes infeasible for large sequences. To speed up the computation, sparse attention [67] iteratively calculates attention in sliding'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='ity which becomes infeasible for large sequences. To speed up the computation, sparse attention [67] iteratively calculates attention in sliding windows for speed gains. Flash Attention [68]: Memory access is the major bottleneck in calculating attention using GPUs. To speed up, flash attention employs input tiling to minimize the memory reads and writes between the GPU high bandwidth memory (HBM) and the on-chip SRAM.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='4\\n\\n2.4. Activation Functions\\n\\nThe activation functions serve a crucial role in the curve- fitting abilities of neural networks [69]. We discuss activation functions used in LLMs in this section. ReLU [70]: The Rectified linear unit (ReLU) is defined as:\\n\\nReLU(x) = max(0, x)'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='ReLU(x) = max(0, x)\\n\\nGeLU [71]: The Gaussian Error Linear Unit (GeLU) is the combination of ReLU, dropout [72] and zoneout [73]. GLU variants [74]: The Gated Linear Unit [75] is a neural network layer that is an element-wise product (⊗) of a linear transformation and a sigmoid transformed (σ) linear projection of the input given as:\\n\\nGLU(x,W,V,b,c) = (xW + b) ⊗ σ(xV + c),\\n\\nwhere X is the input of layer and l, W,b,V and c are learned parameters. Other GLU variants [74] used in LLMs are:'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='where X is the input of layer and l, W,b,V and c are learned parameters. Other GLU variants [74] used in LLMs are:\\n\\nReGLU(x,W,V,b,c) = max(0, xW + b)⊗, GEGLU(x,W,V,b,c) = GELU(xW + b) ⊗ (xV + c), SwiGLU(x,W,V,b,c,β) = Swishβ(xW + b) ⊗ (xV + c).\\n\\n2.5. Layer Normalization'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='2.5. Layer Normalization\\n\\nLayer normalization leads to faster convergence and is an in- tegrated component of transformers [64]. In addition to Layer- Norm [76] and RMSNorm [77], LLMs use pre-layer normal- ization [78], applying it before multi-head attention (MHA). Pre-norm is shown to provide training stability in LLMs. An- other normalization variant, DeepNorm [79] fixes the issue with larger gradients in pre-norm.\\n\\n2.6. Distributed LLM Training'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='2.6. Distributed LLM Training\\n\\nThis section describes distributed LLM training approaches'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='briefly. More details are available in [13, 37, 80, 81]. Data Parallelism: Data parallelism replicates the model on multiple devices where data in a batch gets divided across de- vices. At the end of each training iteration weights are synchro- nized across all devices. Tensor Parallelism: Tensor parallelism shards a tensor compu- tation across devices. It is also known as horizontal parallelism or intra-layer model parallelism. Pipeline Parallelism: Pipeline parallelism shards model layers'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='devices. It is also known as horizontal parallelism or intra-layer model parallelism. Pipeline Parallelism: Pipeline parallelism shards model layers across different devices. This is also known as vertical paral- lelism. Model Parallelism: A combination of tensor and pipeline par- allelism is known as model parallelism. 3D Parallelism: A combination of data, tensor, and model par- allelism is known as 3D parallelism. Optimizer Parallelism: Optimizer parallelism also known as zero redundancy'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='of data, tensor, and model par- allelism is known as 3D parallelism. Optimizer Parallelism: Optimizer parallelism also known as zero redundancy optimizer [37] implements optimizer state partitioning, gradient partitioning, and parameter partitioning across devices to reduce memory consumption while keeping the communication costs as low as possible.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='(1)\\n\\n(2)\\n\\n2.7. Libraries'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Some commonly used libraries for LLMs training are: Transformers [82]: The library provides access to various pre- trained transformer models with APIs to train, fine-tune, infer, and develop custom models. DeepSpeed [36]: A library for scalable distributed training and inference of deep learning models. Megatron-LM [80]: It provides GPU-optimized techniques for large-scale training of LLMs. JAX [83]: A Python library for high-performance numerical It can differenti- computing and scaleable'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='techniques for large-scale training of LLMs. JAX [83]: A Python library for high-performance numerical It can differenti- computing and scaleable machine learning. ate native Python and NumPy functions and execute them on GPUs. Colossal-AI [84]: A collection of components to write dis- tributed deep learning models. BMTrain [81]: A library to write efficient stand-alone LLMs training code. FastMoE [85]: Provides API to build mixture-of-experts (MoE) model in PyTorch. MindSpore [86]: A deep'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='write efficient stand-alone LLMs training code. FastMoE [85]: Provides API to build mixture-of-experts (MoE) model in PyTorch. MindSpore [86]: A deep learning training and inference frame- work extendable to mobile, edge, and cloud computing. PyTorch [87]: A framework developed by Facebook AI Re- search lab (FAIR) to build deep learning models. The main features of PyTorch include a dynamic computation graph and a pythonic coding style. Tensorflow [88]: A deep learning framework written by'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='The main features of PyTorch include a dynamic computation graph and a pythonic coding style. Tensorflow [88]: A deep learning framework written by Google. The key features of TensorFlow are graph-based com- putation, eager execution, scalability, etc. MXNet [89]: Apache MXNet is a deep learning framework with support to write programs in multiple languages, includ- ing, Python, C++, Scala, R, etc. It also provides support for dynamic and static computation graphs.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='2.8. Data PreProcessing\\n\\nThis section briefly summarizes data preprocessing tech-'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='niques used in LLMs training. Quality Filtering: For better results, training data quality is essential. Some approaches to filtering data are: 1) classifier- based and 2) heuristics-based. Classifier-based approaches train a classifier on high-quality data and predict the quality of text for filtering, whereas heuristics-based employ some rules for filtering like language, metrics, statistics, and keywords. Data Deduplication: Duplicated data can affect model per- formance and increase data'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='for filtering like language, metrics, statistics, and keywords. Data Deduplication: Duplicated data can affect model per- formance and increase data memorization; therefore, to train LLMs, data deduplication is one of the preprocessing steps. like sentences, This can be performed at multiple levels, documents, and datasets. Privacy Reduction: Most of the training data for LLMs is collected through web sources. This data contains private information; therefore, many LLMs employ heuristics-based'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='of the training data for LLMs is collected through web sources. This data contains private information; therefore, many LLMs employ heuristics-based methods to filter information such as names, addresses, and phone numbers to avoid learning personal information.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='2.9. Architectures\\n\\nHere we discuss the variants of the transformer architectures used in LLMs. The difference arises due to the application of\\n\\n5\\n\\nFigure 4: An example of attention patterns in language models, image is taken from [93].\\n\\nFigure 5: An example of language model training objectives, image from [93].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='the attention and the connection of transformer blocks. An il- lustration of attention patterns of these architectures is shown in Figure 4. Encoder Decoder: This architecture processes inputs through the encoder and passes the intermediate representation to the decoder to generate the output. Here, the encoder sees the complete sequence utilizing self-attention whereas the decoder processes the sequence one after the other with implementing cross-attention. Causal Decoder: A type of'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='utilizing self-attention whereas the decoder processes the sequence one after the other with implementing cross-attention. Causal Decoder: A type of architecture that does not have an encoder and processes and generates output using a decoder, where the predicted token depends only on the previous time steps. Prefix Decoder: It is also known as a non-causal decoder, where the attention calculation is not strictly dependent on the past information and the attention is bidirectional. An example'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='non-causal decoder, where the attention calculation is not strictly dependent on the past information and the attention is bidirectional. An example of a non-causal attention mask is shown in Figure 4. Mixture-of-Experts: It is a variant of transformer architecture with parallel independent experts and a router to route tokens to experts. These experts are feed-forward layers after the at- tention block [90]. Mixture-of-Experts (MoE) is an efficient sparse architecture that offers comparable'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='experts are feed-forward layers after the at- tention block [90]. Mixture-of-Experts (MoE) is an efficient sparse architecture that offers comparable performance to dense models and allows increasing the model size without increas- ing the computational cost by activating only a few experts at a time [91, 92].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='2.10. Pre-Training Objectives\\n\\nThis section describes LLMs pre-training objectives. For\\n\\nmore details see the paper [93]. Full Language Modeling: An autoregressive language model- ing objective where the model is asked to predict future tokens given the previous tokens, an example is shown in Figure 5. Prefix Language Modeling: A non-causal training objective, where a prefix is chosen randomly and only remaining target tokens are used to calculate the loss. An example is shown in Figure 5.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Figure 6: A basic flow diagram depicting various stages of LLMs from pre-training to prompting/utilization. Prompting LLMs to generate responses is possible at different training stages like pre-training, instruction-tuning, or alignment tuning. “RL” stands for reinforcement learning, “RM” represents reward-modeling, and “RLHF” represents reinforcement learning with human feedback.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Masked Language Modeling: In this training objective, tokens or spans (a sequence of tokens) are masked randomly and the model is asked to predict masked tokens given the past and future context. An example is shown in Figure 5. Unified Language Modeling: Unified language modeling [94] is a combination of causal, non-causal, and masked language training objectives. Here in masked language modeling, the attention is not bidirectional but unidirectional, attending either left-to-right or'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='training objectives. Here in masked language modeling, the attention is not bidirectional but unidirectional, attending either left-to-right or right-to-left context.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='2.12. LLMs Adaptation Stages\\n\\nThis section discusses the fundamentals of LLMs adaptation stages, from pre-training to fine-tuning for downstream tasks and utilization. An example of different training stages and in- ference in LLMs is shown in Figure 6. In this paper, we refer to alignment-tuning as aligning with human preferences, while occasionally the literature uses the term alignment for different purposes.\\n\\n2.12.1. Pre-Training\\n\\n2.11. LLMs Scaling Laws'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Scaling laws study the optimal combination of model param- eters, dataset size, and computational resources that predict the improvement in the model performance. It has been shown that the loss scales according to the power-law with model size, dataset size, and compute resources [95]. This study suggests larger models are more important than big data for better perfor- mance. Another variant of scaling law [96] suggests the model size and the number of training tokens should be scaled'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='big data for better perfor- mance. Another variant of scaling law [96] suggests the model size and the number of training tokens should be scaled equally.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='is trained in a self- supervised manner on a large corpus to predict the next to- kens given the input. The design choices of LLMs vary from encoder-decoder to decoder-only architectures with different building blocks and loss functions in sections 2.5, 2.4, 2.10.\\n\\nIn the very first stage,\\n\\nthe model\\n\\n2.12.2. Fine-Tuning\\n\\nThere are different styles to fine-tune an LLM. This section'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='In the very first stage,\\n\\nthe model\\n\\n2.12.2. Fine-Tuning\\n\\nThere are different styles to fine-tune an LLM. This section\\n\\nbriefly discusses fine-tuning approaches. Transfer Learning: The pre-trained LLMs perform well for various tasks [6, 15]. However, to improve the performance for\\n\\n6'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='a downstream task, pre-trained models are fine-tuned with the task-specific data [10, 11], known as transfer learning. Instruction-tuning: To enable a model to respond to user queries effectively, the pre-trained model is fine-tuned on in- struction formatted data i.e., instruction and an input-output pair. Instructions generally comprise multi-task data in plain natural language, guiding the model to respond according to the prompt and the input. This type of fine-tuning improves zero- shot'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='data in plain natural language, guiding the model to respond according to the prompt and the input. This type of fine-tuning improves zero- shot generalization and downstream task performance. Details on formatting instruction data and its various styles are avail- able in [16, 50, 97]. Alignment-tuning: LLMs are prone to generating false, biased, and harmful text. To make them helpful, honest, and harmless, models are aligned using human feedback. Alignment involves asking LLMs to generate'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='and harmful text. To make them helpful, honest, and harmless, models are aligned using human feedback. Alignment involves asking LLMs to generate unexpected responses and then updat- ing their parameters to avoid such responses [20, 21, 98]. It ensures LLMs operate according to human intentions and values. A model is defined to be an “aligned” model if the model fulfills three criteria of helpful, honest, and harmless or “HHH” [99]. Researchers employ reinforcement learning with human feed-'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='model if the model fulfills three criteria of helpful, honest, and harmless or “HHH” [99]. Researchers employ reinforcement learning with human feed- back (RLHF) [100] for model alignment. In RLHF, a fine-tuned model on demonstrations is further trained with reward model- ing (RM) and reinforcement learning (RL), shown in Figure 6. Below we briefly discuss RM and RL pipelines in RLHF. Reward modeling: trains a model to rank generated responses according to human preferences using a'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='we briefly discuss RM and RL pipelines in RLHF. Reward modeling: trains a model to rank generated responses according to human preferences using a classification objec- tive. To train the classifier humans annotate LLMs generated responses based on the HHH criteria. Reinforcement learning: in combination with the reward model is used for alignment in the next stage. The previously trained reward model ranks LLM-generated responses into preferred vs. non-preferred, which is used to align the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='in the next stage. The previously trained reward model ranks LLM-generated responses into preferred vs. non-preferred, which is used to align the model with proxi- mal policy optimization (PPO). This process repeats iteratively until convergence.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='2.12.3. Prompting/Utilization'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Prompting is a method to query trained LLMs for generating responses, as illustrated in Figure 6. LLMs can be prompted in various prompt setups, where they can be adapted to the instruc- tions without fine-tuning and in other cases with fine-tuning on data containing different prompt styles [16, 101, 102]. A good guide on prompt engineering is available at [32]. Below, we will discuss various widely used prompt setups. Zero-Shot Prompting: LLMs are zero-shot learners and ca- pable of answering'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='at [32]. Below, we will discuss various widely used prompt setups. Zero-Shot Prompting: LLMs are zero-shot learners and ca- pable of answering queries never seen before. This style of prompting requires LLMs to answer user questions without see- ing any examples in the prompt. In-context Learning: Also known as few-shot learning, here, multiple input-output demonstration pairs are shown to the model to generate the desired response. This adaptation style is also called few-shot learning. A'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='input-output demonstration pairs are shown to the model to generate the desired response. This adaptation style is also called few-shot learning. A discussion on formatting in- context learning (ICL) templates is available in [54, 50, 18, 16]. Reasoning in LLMs: LLMs are zero-shot reasoners and can task be provoked to generate answers to logical problems, planning, critical thinking, etc. with reasoning. Generating reasons is possible only by using different prompting styles,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='7'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='whereas to improve LLMs further on reasoning tasks many methods [16, 97] train them on reasoning datasets. We discuss various prompting techniques for reasoning below. Chain-of-Thought (CoT): A special case of prompting where demonstrations contain reasoning information aggregated with inputs and outputs so that the model generates outcomes with step-by-step reasoning. More details on CoT prompts are avail- able in [55, 103, 101]. Self-Consistency: Improves CoT performance by generat- ing'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='step-by-step reasoning. More details on CoT prompts are avail- able in [55, 103, 101]. Self-Consistency: Improves CoT performance by generat- ing multiple responses and selecting the most frequent an- swer [104]. Tree-of-Thought (ToT): Explores multiple reasoning paths with possibilities to look ahead and backtrack for problem- solving [105]. Single-Turn Instructions: In this prompting setup, LLMs are queried only once with all the relevant information in the prompt. LLMs generate responses by'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Instructions: In this prompting setup, LLMs are queried only once with all the relevant information in the prompt. LLMs generate responses by understanding the con- text either in a zero-shot or few-shot setting. Multi-Turn Instructions: Solving a complex task requires mul- tiple interactions with LLMs, where feedback and responses from the other tools are given as input to the LLM for the next rounds. This style of using LLMs in the loop is common in autonomous agents.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='3. Large Language Models\\n\\nThis section reviews LLMs, briefly describing their architec- tures, training objectives, pipelines, datasets, and fine-tuning details.\\n\\n3.1. Pre-Trained LLMs'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='3.1. Pre-Trained LLMs\\n\\nHere, we provide summaries of various well-known pre- trained LLMs with significant discoveries, changing the course of research and development in NLP. These LLMs have consid- erably improved the performance in NLU and NLG domains, and are widely fine-tuned for downstream tasks. Moreover, We also identify key findings and insights of pre-trained LLMs in Table 1 and 2 that improve their performance.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='3.1.1. General Purpose T5 [10]: An encoder-decoder model employing a unified text- to-text training for all NLP problems is shown in Figure 7. T5 places layer normalization outside the residual path in a conven- tional transformer model [64]. It uses masked language mod- eling as a pre-training objective where spans (consecutive to- kens) are replaced with a single mask instead of separate masks for each token. This type of masking speeds up the training as it produces shorter sequences. After'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='with a single mask instead of separate masks for each token. This type of masking speeds up the training as it produces shorter sequences. After pre-training, the model is fine-tuned using adapter layers [106] for downstream tasks.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='GPT-3 [6]: The GPT-3 architecture is the same as the GPT- 2 [5] but with dense and sparse attention in transformer layers similar to the Sparse Transformer [67]. It shows that large mod- els can train on larger batch sizes with a lower learning rate to decide the batch size during training, GPT-3 uses the gradient noise scale as in [107]. Overall, GPT-3 increases model param- eters to 175B showing that the performance of large language'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Figure 7: Unified text-to-text training example, source image from [10].\\n\\nFigure 8: The image is the article of [108], showing an example of PanGu-α architecture.\\n\\nmodels improves with the scale and is competitive with the fine- tuned models.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='mT5 [11]: A multilingual T5 model [10] trained on the mC4 dataset with 101 languages. The dataset is extracted from the public common crawl scrape. The model uses a larger vocab- ulary size of 250,000 to cover multiple languages. To avoid over-fitting or under-fitting for a language, mT5 employs a data sampling procedure to select samples from all languages. The paper suggests using a small amount of pre-training datasets, including all languages when fine-tuning for a task using En- glish'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='all languages. The paper suggests using a small amount of pre-training datasets, including all languages when fine-tuning for a task using En- glish language data. This allows the model to generate correct non-English outputs.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='PanGu-α [108]: An autoregressive model that has a query layer at the end of standard transformer layers, example shown in Figure 8, to predict the next token. Its structure is similar to the transformer layer but with an additional embedding for the next position in the attention mechanism, given in Eq. 3.\\n\\na = pnWq\\n\\nhWk\\n\\nhTHT\\n\\nL'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='CPM-2 [12]: Cost-efficient Pre-trained language Models (CPM-2) pre-trains bilingual (English and Chinese) 11B and 198B mixture-of-experts (MoE) models on the WuDaoCor- pus [109] dataset. The tokenization process removes “_” white space tokens in the sentencepiece tokenizer. The models are trained with knowledge inheritance, starting with only the Chi- nese language in the first stage and then adding English and Chinese data. This trained model gets duplicated multiple times to initialize the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Chi- nese language in the first stage and then adding English and Chinese data. This trained model gets duplicated multiple times to initialize the 198B MoE model. Moreover, to use the model for downstream tasks, CPM-2 experimented with both com-'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='(3)\\n\\n8\\n\\nplete fine-tuning and prompt fine-tuning as in [40] where only prompt-related parameters are updated by inserting prompts at various positions, front, middle, and back. CPM-2 also pro- poses the INFMOE, a memory-efficient framework with a strat- egy to dynamically offload parameters to the CPU for inference at a 100B scale. It overlaps data movement with inference com- putation for lower inference time.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='ERNIE 3.0 [110]: ERNIE 3.0 takes inspiration from multi- task learning to build a modular architecture using Transformer- XL [111] as the backbone. The universal representation mod- ule is shared by all the tasks, which serve as the basic block for task-specific representation modules, which are all trained jointly for natural language understanding, natural language generation, and knowledge extraction. This LLM is primar- ily focused on the Chinese language. It claims to train on the largest'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='natural language generation, and knowledge extraction. This LLM is primar- ily focused on the Chinese language. It claims to train on the largest Chinese text corpora for LLM training, and achieved state-of-the-art in 54 Chinese NLP tasks.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Jurassic-1 [112]: A pair of auto-regressive language mod- els, including a 7B-parameter J1-Large model and a 178B- parameter J1-Jumbo model. The training vocabulary of Jurassic-1 comprise word pieces, complete words, and multi- word expressions without any word boundaries, where possible out-of-vocabulary instances are interpreted as Unicode bytes. Compared to the GPT-3 counterparts, the Jurassic-1 models apply a more balanced depth-to-width self-attention architec- ture [113] and an improved'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Compared to the GPT-3 counterparts, the Jurassic-1 models apply a more balanced depth-to-width self-attention architec- ture [113] and an improved tokenizer for a faster prediction based on broader resources, achieving a comparable perfor- mance in zero-shot learning tasks and a superior performance in few-shot learning tasks given the ability to feed more examples as a prompt.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='HyperCLOVA [114]: A Korean language model with GPT-3\\n\\narchitecture.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Yuan 1.0 [115]: Trained on a Chinese corpus with 5TB of high-quality text collected from the Internet. A Massive Data Filtering System (MDFS) built on Spark is developed to pro- cess the raw data via coarse and fine filtering techniques. To speed up the training of Yuan 1.0 to save energy expenses and carbon emissions, various factors that improve the performance of distributed training are incorporated in architecture and train- ing: like increasing the hidden state size improves pipeline and'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='the performance of distributed training are incorporated in architecture and train- ing: like increasing the hidden state size improves pipeline and tensor parallelism performance, larger micro batches improve pipeline parallelism performance, and larger global batch size improve data parallelism performance. In practice, the Yuan 1.0 model performs well on text classification, Winograd Schema, natural language inference, and reading comprehension tasks.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Gopher [116]: The Gopher family of models ranges from 44M to 280B parameters in size to study the effect of scale on the LLMs performance. The 280B model beats GPT-3 [6], Jurrasic-1 [112], MT-NLG [117], and others on 81% of the evaluated tasks. ERNIE 3.0 TITAN [35]: ERNIE 3.0 Titan extends ERNIE 3.0 by training a larger model with 26x the number of parameters of the latter. This bigger model outperformed other state-of-the- art models in 68 NLP tasks. LLMs produce text with incorrect facts. In'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='parameters of the latter. This bigger model outperformed other state-of-the- art models in 68 NLP tasks. LLMs produce text with incorrect facts. In order to have control of the generated text with fac- tual consistency, ERNIE 3.0 Titan adds another task, Credible and Controllable Generations, to its multi-task learning setup.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='It introduces additional self-supervised adversarial and control- lable language modeling losses to the pre-training step, which enables ERNIE 3.0 Titan to beat other LLMs in their manually selected Factual QA task set evaluations.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='GPT-NeoX-20B [118]: An auto-regressive model that largely follows GPT-3 with a few deviations in architecture design, trained on the Pile dataset without any data deduplication. GPT- NeoX has parallel attention and feed-forward layers in a trans- former block, given in Eq. 4, that increases throughput by 15%. It uses rotary positional embedding [66], applying it to only 25% of embedding vector dimension as in [119]. This reduces the computation without performance degradation. As opposed to'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[66], applying it to only 25% of embedding vector dimension as in [119]. This reduces the computation without performance degradation. As opposed to GPT-3, which uses dense and sparse layers, GPT-NeoX-20B uses only dense layers. The hyperparameter tuning at this scale is difficult; therefore, the model chooses hyperparameters from the method [6] and interpolates values between 13B and 175B models for the 20B model. The model training is distributed among GPUs using both tensor and pipeline'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='and interpolates values between 13B and 175B models for the 20B model. The model training is distributed among GPUs using both tensor and pipeline parallelism.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='x + Attn(LN1(x)) + FF(LN2(x))'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='OPT [14]: It is a clone of GPT-3, developed to open-source a model that replicates GPT-3 performance. Training of OPT employs dynamic loss scaling [120] and restarts from an earlier checkpoint with a lower learning rate whenever loss divergence is observed. Overall, the performance of OPT-175B models is comparable to the GPT3-175B model. BLOOM [13]: A causal decoder model trained on the ROOTS corpus to open-source an LLM. The architecture of BLOOM is shown in Figure 9, with differences like'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[13]: A causal decoder model trained on the ROOTS corpus to open-source an LLM. The architecture of BLOOM is shown in Figure 9, with differences like ALiBi positional em- bedding, an additional normalization layer after the embedding layer as suggested by the bitsandbytes1 library. These changes stabilize training with improved downstream performance. GLaM [91]: Generalist Language Model (GLaM) represents a family of language models using a sparsely activated decoder- only mixture-of-experts'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='GLaM [91]: Generalist Language Model (GLaM) represents a family of language models using a sparsely activated decoder- only mixture-of-experts (MoE) structure [121, 90]. To gain more model capacity while reducing computation, the experts are sparsely activated where only the best two experts are used to process each input token. The largest GLaM model, GLaM (64B/64E), is about 7× larger than GPT-3 [6], while only part of the parameters are activated per input token. The largest GLaM (64B/64E)'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='model, GLaM (64B/64E), is about 7× larger than GPT-3 [6], while only part of the parameters are activated per input token. The largest GLaM (64B/64E) model achieves better overall results as compared to GPT-3 while consuming only one-third of GPT-3’s training energy.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='MT-NLG [117]: A 530B causal decoder based on the GPT- 2 architecture that has roughly 3× GPT-3 model parameters. MT-NLG is trained on filtered high-quality data collected from various public datasets and blends various types of datasets in a single batch, which beats GPT-3 on several evaluations. Chinchilla [96]: A causal decoder trained on the same dataset as the Gopher [116] but with a little different data sampling distribution (sampled from MassiveText). The model architec- ture is similar'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='dataset as the Gopher [116] but with a little different data sampling distribution (sampled from MassiveText). The model architec- ture is similar to the one used for Gopher, with the exception of AdamW optimizer instead of Adam. Chinchilla identifies the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='1https://github.com/TimDettmers/bitsandbytes\\n\\n(4)\\n\\n9\\n\\nFigure 9: The BLOOM architecture example sourced from [13].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='relationship that model size should be doubled for every dou- bling of training tokens. Over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 bil- lion tokens are trained to get the estimates for compute-optimal training under a given budget. The authors train a 70B model with the same compute budget as Gopher (280B) but with 4 times more data. It outperforms Gopher [116], GPT-3 [6], and others on various downstream tasks, after fine-tuning.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='AlexaTM [122]: An encoder-decoder model, where encoder weights and decoder embeddings are initialized with a pre- trained encoder to speed up training. The encoder stays frozen for the initial 100k steps and is later unfrozen for end-to-end training. The model is trained on a combination of denoising and causal language modeling (CLM) objectives, concatenat- ing a [CLM] token at the beginning for mode switching. Dur- ing training, the CLM task is applied for 20% of the time, which improves the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='concatenat- ing a [CLM] token at the beginning for mode switching. Dur- ing training, the CLM task is applied for 20% of the time, which improves the in-context learning performance.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='PaLM [15]: A causal decoder with parallel attention and feed-forward layers similar to Eq. 4, speeding up training by a factor of 15. Additional changes to the conventional trans- former model include SwiGLU activation, RoPE embeddings, multi-query attention that saves computation cost during decod- ing, and shared input-output embeddings. During training, loss spiking was observed, and to fix it, model training was restarted from a 100-step earlier checkpoint by skipping 200-500 batches around'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='loss spiking was observed, and to fix it, model training was restarted from a 100-step earlier checkpoint by skipping 200-500 batches around the spike. Moreover, the model was found to memo- rize around 2.4% of the training data at the 540B model scale, whereas this number was lower for smaller models.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='PaLM-2 [123]: A smaller multi-lingual variant of PaLM, trained for larger iterations on a better quality dataset. PaLM- 2 shows significant improvements over PaLM, while reducing training and inference costs due to its smaller size. To lessen toxicity and memorization, it appends special tokens with a fraction of pre-training data, which shows a reduction in gener- ating harmful responses.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='U-PaLM [124]: This method trains PaLM for 0.1% addi- tional compute with the UL2 (also named as UL2Restore) ob- jective [125], using the same dataset it outperforms the baseline significantly on various NLP tasks, including zero-shot, few- shot, commonsense reasoning, CoT, etc. Training with UL2R involves converting a causal decoder PaLM to a non-causal de- coder PaLM and employing 50% sequential denoising, 25% regular denoising, and 25% extreme denoising loss functions.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='UL2 [125]: An encoder-decoder architecture trained using a mixture of denoisers (MoD) objective. Denoisers include 1) R-Denoiser: a regular span masking, 2) S-Denoiser: which cor- rupts consecutive tokens of a large sequence and 3) X-Denoiser: which corrupts a large number of tokens randomly. During pre- training, UL2 includes a denoiser token from R,S, X to rep- resent a denoising setup. It helps improve fine-tuning perfor- mance for downstream tasks that bind the task to one of the up- stream'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='R,S, X to rep- resent a denoising setup. It helps improve fine-tuning perfor- mance for downstream tasks that bind the task to one of the up- stream training modes. This MoD style of training outperforms the T5 model on many benchmarks. GLM-130B [33]: GLM-130B is a bilingual (English and Chi- nese) model trained using an auto-regressive mask infilling pre- training objective similar to the GLM [126]. This training style makes the model bidirectional as compared to GPT-3, which is'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='mask infilling pre- training objective similar to the GLM [126]. This training style makes the model bidirectional as compared to GPT-3, which is unidirectional. As opposed to GLM, the training of GLM-130B includes a small amount of multi-task instruction pre-training data (5% of the total data) along with self-supervised mask in- filling. To stabilize the training, it applies embedding layer gra- dient shrink.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='LLaMA [127, 21]: A set of decoder-only language models varying from 7B to 70B parameters. LLaMA models series is the most famous among the community for parameter efficiency and instruction tuning. LLaMA-1 [127]: Implements efficient causal attention [128] by not storing and computing masked attention weights and key/query scores. Another optimization is reducing the number of activations recomputed in the backward pass, as in [129].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='LLaMA-2 [21]: This work is more focused on fine-tuning a safer and better LLaMA-2-Chat model for dialogue generation. The pre-trained model has 40% more training data with a larger context length and grouped-query attention.\\n\\nLLaMA-3/3.1 [130]: A collection of models trained on a seven times larger dataset as compared to LLaMA-2 with dou- ble the context length, outperforming its previous variants and other models.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='PanGu-Σ [92]: An autoregressive model with parameters copied from PanGu-α and extended to a trillion scale with Ran- dom Routed Experts (RRE), the architectural diagram is shown in Figure 10. RRE is similar to the MoE architecture, with distinctions at the second level, where tokens are randomly routed to experts in a domain instead of using a learnable gat- ing method. The model has bottom layers densely activated and shared across all domains, whereas top layers are sparsely ac- tivated'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='a learnable gat- ing method. The model has bottom layers densely activated and shared across all domains, whereas top layers are sparsely ac- tivated according to the domain. This training style allows for extracting task-specific models and reduces catastrophic forget- ting effects in the case of continual learning. Mixtral8x22b [131]: A mixture-of-experts (MoE) model with eight distinct experts routes each token to two experts at each layer and combines the outputs additively. Snowflake'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='(MoE) model with eight distinct experts routes each token to two experts at each layer and combines the outputs additively. Snowflake Arctic [132]: Arctic LLM is a hybrid of dense and mixture-of-experts (MoE) architecture. The MoE (128×3.66B MLP experts) is parallel to the dense transformer (10B) with only two experts activated. The model has many experts, com- pared to other MoE LLMs [131, 133], to increase the model capacity and provide an opportunity to choose among many ex- perts for a'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='many experts, com- pared to other MoE LLMs [131, 133], to increase the model capacity and provide an opportunity to choose among many ex- perts for a diverse configuration. The model has 480B param- eters, and only 17B are active during a forward pass, reducing'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='10\\n\\nthe computation significantly. Grok [133, 134]: Grok is a family of LLMs including Grok-1 and Grok-1.5, released by XAI.\\n\\nGrok-1 [133]: Grok-1 is a 314B parameters language MoE model (eight experts), where two experts are activated per to- ken. Grok-1.5 [134]: Grok-1.5 is a multi-modal LLM with a larger context length and improved performance.\\n\\nGemini [135, 136]: Gemini replaces Bard (based on PaLM) with multi-modal capabilities and significant language model- ing performance improvements.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Gemini [135, 136]: Gemini replaces Bard (based on PaLM) with multi-modal capabilities and significant language model- ing performance improvements.\\n\\nGemini-1 [135]: The first-ever auto-regressive model to\\n\\nachieve human-level capabilities on the MMLU benchmark.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Gemini-1.5 [136]: A multi-modal LLM with MoE architec- ture builds on the findings of Gemini-1. The model has a 2M context window and can reason over information up to 10M tokens. Such large context windows were never achieved pre- viously and shown to have a huge impact on performance gain. Nemotron-4 340B [137]: A decoder-only model that has been aligned on 98% synthetic data and only 2% manually annotated data. Utilizing synthetic data at a large proportion improves the model performance'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='been aligned on 98% synthetic data and only 2% manually annotated data. Utilizing synthetic data at a large proportion improves the model performance significantly. The paper suggested intro- ducing alignment data with a smaller subset of previously seen data during the late stage of the model pre-training, enabling the smooth transition from the pre-trained stage to the final train- ing stage. To train better instruction-following models, weaker models are trained into stronger models'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='the pre-trained stage to the final train- ing stage. To train better instruction-following models, weaker models are trained into stronger models iteratively. The syn- thetic data generated by the weaker instruction-tuned model is used to train a base model which is later supervised fine-tuned outperforming the weaker model.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='DeepSeek [138]: DeepSeek studies the LLMs scaling laws in detail to determine the optimal non-embedding model size and training data. The experiments were performed for 8 bud- gets ranging from 1e17 to 3e20 training FLOPs. Each compute budget was tested against ten different models/data scales. The batch size and learning rates were also fitted for the given com- pute budget finding that the batch size should increase with the increased compute budget while decreasing the learning rate.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='for the given com- pute budget finding that the batch size should increase with the increased compute budget while decreasing the learning rate. Following are the equations for the optimal batch-size (B), learning rate (η), model size (M), and data (D):'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Bopt = 0.2920.C0.3271 ηopt = 0.3118.C−0.1250 Mopt = Mbase.Ca Dopt = Dbase.Cb Mbase = 0.1715, Dbase = 5.8316,a = 0.5243,b = 0.4757'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='DeepSeek-v2 [139]: An MoE model that introduces multi- head latent attention (MLA) to reduce inference costs, by com- pressing Key-Value (KV) cache into a latent vector. MLA achieves better performance than multi-head attention (MHA), and other efficient attention mechanisms such as grouped query attention (GQA), multi-query attention (MQA), etc. Because of MLA, DeepSeek-v2 achieves 5.76 times faster inference throughput as compared to DeepSeek [138].\\n\\n(5)\\n\\n3.1.2. Coding'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='CodeGen [140]: CodeGen has a similar architecture to PaLM [15], i.e., parallel attention, MLP layers, and RoPE em- beddings. The model is trained on both natural language and programming language data sequentially (trained on the first dataset, then the second, and so on) on the following datasets 1) PILE, 2) BIGQUERY, and 3) BIGPYTHON. CodeGen pro- posed a multi-step approach to synthesizing code. The purpose is to simplify the generation of long sequences where the previ- ous prompt and'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='pro- posed a multi-step approach to synthesizing code. The purpose is to simplify the generation of long sequences where the previ- ous prompt and generated code are given as input with the next prompt to generate the next code sequence. CodeGen open- source a Multi-Turn Programming Benchmark (MTPB) to eval- uate multi-step program synthesis. Codex [141]: This LLM is trained on a subset of public Python Github repositories to generate code from docstrings. Com- puter programming is an iterative'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[141]: This LLM is trained on a subset of public Python Github repositories to generate code from docstrings. Com- puter programming is an iterative process where the programs are often debugged and updated before fulfilling the require- ments. Similarly, Codex generates 100 versions of a program by repetitive sampling for a given description, which produces a working solution for 77.5% of the problems passing unit tests. Its powerful version powers Github Copilot2.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='AlphaCode [142]: A set of large language models, ranging from 300M to 41B parameters, designed for competition-level code generation tasks. It uses the multi-query attention [143] to reduce memory and cache costs. Since competitive program- ming problems highly require deep reasoning and an under- standing of complex natural language algorithms, the Alpha- Code models are pre-trained on filtered GitHub code in popular languages and then fine-tuned on a new competitive program- ming dataset'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='the Alpha- Code models are pre-trained on filtered GitHub code in popular languages and then fine-tuned on a new competitive program- ming dataset named CodeContests. The CodeContests dataset mainly contains problems, solutions, and test cases collected from the Codeforces platform3. The pre-training employs stan- dard language modeling objectives, while GOLD [144] with tempering [145] serves as the training objective for the fine- tuning on CodeContests data. To evaluate the performance of'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='while GOLD [144] with tempering [145] serves as the training objective for the fine- tuning on CodeContests data. To evaluate the performance of AlphaCode, simulated programming competitions are hosted on the Codeforces platform: overall, AlphaCode ranks at the top 54.3% among over 5000 competitors, where its Codeforces rating is within the top 28% of recently participated users.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='CodeT5+ [34]: CodeT5+ is based on CodeT5 [146], with shallow encoder and deep decoder, trained in multiple stages initially unimodal data (code) and later bimodal data (text-code pairs). Each training stage has different training objectives and activates different model blocks encoder, decoder, or both ac- cording to the task. The unimodal pre-training includes span denoising and CLM objectives, whereas bimodal pre-training objectives contain contrastive learning, matching, and CLM for'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='pre-training includes span denoising and CLM objectives, whereas bimodal pre-training objectives contain contrastive learning, matching, and CLM for text-code pairs. CodeT5+ adds special tokens with the text to enable task modes, for example, [CLS] for contrastive loss, [Match] for text-code matching, etc. StarCoder [147]: A decoder-only model with the SantaCoder architecture, employing Flash attention to scale up the context length to 8k. The StarCoder trains an encoder to filter names,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='2https://github.com/features/copilot 3https://codeforces.com/\\n\\n11\\n\\nemails, and other personal data from the training data. Its fine- tuned variant outperforms PaLM, LLaMA, and LAMDA on HumanEval and MBPP benchmarks.\\n\\n3.1.3. Scientific Knowledge'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Galactica [148]: A large curated corpus of human scientific knowledge with 48 million papers, textbooks, lecture notes, millions of compounds and proteins, scientific websites, en- cyclopedias, and more are trained using the metaseq library3, which is built on PyTorch and fairscale [149]. The model wraps reasoning datasets with the < work > token to provide step-by- step reasoning context to the model, which has been shown to improve the performance on reasoning tasks.\\n\\n3.1.4. Dialog'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='LaMDA [150]: A decoder-only model pre-trained on pub- lic dialog data, public dialog utterances, and public web doc- uments, where more than 90% of the pre-training data is in English. LaMDA is trained with the objective of producing re- sponses that exhibit high levels of quality, safety, and grounded- ness. To achieve this, discriminative and generative fine-tuning techniques are incorporated to enhance the model’s safety and quality aspects. As a result, the LaMDA models can be utilized as a'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='fine-tuning techniques are incorporated to enhance the model’s safety and quality aspects. As a result, the LaMDA models can be utilized as a general language model performing various tasks.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='3.1.5. Finance'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='BloombergGPT [151]: A non-causal decoder model trained using both financial (“FINPILE” from the Bloomberg archive) and general-purpose datasets. The model’s architecture is sim- ilar to the BLOOM [13] and OPT [14]. It allocates 50B param- eters to different blocks of the model using the approach [113]. For effective training, BloombergGPT packs documents to- gether with < |endoftext| > to use the maximum sequence length, uses warmup batch size starting from 1024 to 2048, and manually reduces'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='documents to- gether with < |endoftext| > to use the maximum sequence length, uses warmup batch size starting from 1024 to 2048, and manually reduces the learning rate multiple times during the training.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Xuan Yuan 2.0 [152]: A Chinese financial chat model with BLOOM’s [13] architecture trained on a combination of general purpose, financial, general purpose instructions, and financial institutions datasets. Xuan Yuan 2.0 combined the pre-training and fine-tuning stages to avoid catastrophic forgetting.\\n\\n3.2. Fine-Tuned LLMs'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Pre-trained LLMs have excellent generalization abilities to unseen tasks. However, because they are generally trained with the objective of next token prediction, LLMs have limited ca- pacity to follow user intent and are prone to generate unethical, toxic or inaccurate responses [20]. For their effective utiliza- tion, LLMs are fine-tuned to follow instructions [16, 17, 97] and generate safe responses [20], which also results in increasing zero-shot, few-shot, and cross-task generalization'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='instructions [16, 17, 97] and generate safe responses [20], which also results in increasing zero-shot, few-shot, and cross-task generalization [97, 16, 18], with minimal compute increment, e.g., 0.2% of the total pre- training for PaLM 540B [16]. We review various fine-tuned LLMs and strategies for effective fine-tuning in this section.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Models\\n\\nT5\\n\\nGPT-3\\n\\nmT5\\n\\nPanGu-α\\n\\nCPM-2\\n\\nERNIE 3.0\\n\\nJurassic-1\\n\\nHyperCLOVA\\n\\nYuan 1.0\\n\\nGopher\\n\\nERNIE 3.0 Titan\\n\\nGPT-NeoX-20B\\n\\nTable 1: Noteworthy findings and insights of pre-trained Large Language Models.\\n\\nFindings & Insights\\n\\nEncoder and decoder with shared parameters perform equivalently when parameters are not shared\\n\\nFine-tuning model layers (adapter layers) work better than the conventional way of training on only classification layers'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Fine-tuning model layers (adapter layers) work better than the conventional way of training on only classification layers\\n\\nFew-shot performance of LLMs is better than the zero-shot, suggesting that LLMs are meta- learners\\n\\nLarge multi-lingual models perform equivalently to single language models on downstream tasks. However, smaller multi-lingual models perform worse\\n\\nLLMs have good few shot capabilities'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='LLMs have good few shot capabilities\\n\\nPrompt fine-tuning requires updating very few parameters while achieving performance compara- ble to full model fine-tuning\\n\\nPrompt fine-tuning takes more time to converge as compared to full model fine-tuning • Inserting prompt tokens in-between sentences can allow the model to understand relations between sentences and long sequences'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='In an analysis, CPM-2 finds that prompts work as a provider (additional context) and aggregator (aggregate information with the input text) for the model\\n\\nA modular LLM architecture with a universal representation module and task-specific representa- tion module helps in the finetuning phase\\n\\nOptimizing the parameters of a task-specific representation network during the fine-tuning phase is an efficient way to take advantage of the powerful pre-trained model'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='The performance of LLM is highly related to the network size • To improve runtime performance, more operations can be performed in parallel (width) rather than sequential (depth)\\n\\nTo efficiently represent and fit more text in the same context length, the model uses a larger vo- cabulary to train a SentencePiece tokenizer without restricting it to word boundaries. This further benefits in few-shot learning tasks'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='By employing prompt-based tuning, the performances of models can be improved, often surpassing those of state-of-the-art models when the backward gradients of inputs are accessible\\n\\nThe model architecture that excels in pre-training and fine-tuning cases may exhibit contrasting behavior in zero-shot and few-shot learning\\n\\nRelative encodings enable the model to evaluate for longer sequences than training.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Relative encodings enable the model to evaluate for longer sequences than training.\\n\\nAdditional self-supervised adversarial loss to distinguish between real and generated text improves the model performance as compared to ERNIE 3.0\\n\\nParallel attention + FF layers speed-up training 15% with the same performance as with cascaded layers'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Parallel attention + FF layers speed-up training 15% with the same performance as with cascaded layers\\n\\nInitializing feed-forward output layers before residuals with scheme in [153] avoids activations from growing with increasing depth and width • Training on Pile outperforms GPT-3 on five-shot\\n\\nTable Continued on Next Page\\n\\n12\\n\\nModels\\n\\nOPT\\n\\nGalactica\\n\\nGLaM\\n\\nLaMDA\\n\\nChinchilla\\n\\nPaLM\\n\\nAlexaTM\\n\\nFindings & Insights'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Table Continued on Next Page\\n\\n12\\n\\nModels\\n\\nOPT\\n\\nGalactica\\n\\nGLaM\\n\\nLaMDA\\n\\nChinchilla\\n\\nPaLM\\n\\nAlexaTM\\n\\nFindings & Insights\\n\\nRestart training from an earlier checkpoint with a lower learning rate if loss diverges • Model is prone to generate repetitive text and stuck in a loop\\n\\nGalactica’s performance has continued to improve across validation set, in-domain, and out-of- domain benchmarks, even with multiple repetitions of the corpus, which is superior to existing research on LLMs'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='A working memory token approach can achieve strong performance over existing methods on mathematical MMLU and MATH benchmarks. It sets a new state-of-the-art on several downstream tasks such as PubMedQA (77.6%) and MedMCQA dev (52.9%)\\n\\nThe model capacity can be maintained at reduced computation by replacing the feed-forward layer in each transformer layer with a mixture-of-experts (MoE)'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='The model trained on filtered data shows consistently better performances on both NLG and NLU tasks, where the effect of filtering is more significant on the former tasks\\n\\nFiltered pretraining corpora play a crucial role in the generation capability of LLMs, especially for the downstream tasks\\n\\nThe scaling of GLaM MoE models can be achieved by increasing the size or number of experts in the MoE layer. Given a fixed budget of computation, more experts contribute to a better perfor- mance'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='The model can be fine-tuned to learn to call different external information resources and tools\\n\\nFor higher effectiveness and efficiency, a transformer model can be asymmetrically constructed with a shallower encoder and a deeper decoder\\n\\nTo achieve better performances, it is necessary to employ strategies such as massively scaling upsampling, followed by the filtering and clustering of samples into a compact set'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='To achieve better performances, it is necessary to employ strategies such as massively scaling upsampling, followed by the filtering and clustering of samples into a compact set\\n\\nThe utilization of novel sampling-efficient transformer architectures designed to facilitate large- scale sampling is crucial\\n\\nSimplifying problem descriptions can effectively improve the model’s performance'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Simplifying problem descriptions can effectively improve the model’s performance\\n\\nThe model size and the number of training tokens should be scaled proportionately: for each dou- bling of the model size, the number of training tokens should be doubled as well\\n\\nEnglish-centric models produce better translations when translating to English as compared to non- English\\n\\nGeneralized models can have equivalent performance for language translation to specialized small models'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Generalized models can have equivalent performance for language translation to specialized small models\\n\\nLarger models have a higher percentage of training data memorization • Performance has not yet saturated even at 540B scale, which means larger models are likely to perform better\\n\\nEncoder-decoder architecture is more suitable to train LLMs given bidirectional attention to the context than decoder-only'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Encoder-decoder architecture is more suitable to train LLMs given bidirectional attention to the context than decoder-only\\n\\nCausal Language Modeling (CLM) task can be added to benefit the model with efficient in-context learning\\n\\nPlacing layer norm at the beginning of each transformer layer improves the training stability\\n\\nTable Continued on Next Page\\n\\n13\\n\\nModels\\n\\nU-PaLM\\n\\nUL2\\n\\nGLM-130B\\n\\nCodeGen\\n\\nLLaMA\\n\\nPanGu-Σ\\n\\nBloombergGPT\\n\\nXuanYuan 2.0\\n\\nCodeT5+\\n\\nStarCoder\\n\\nLLaMA-2\\n\\nPaLM-2\\n\\nLLaMA-3/3.1'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='13\\n\\nModels\\n\\nU-PaLM\\n\\nUL2\\n\\nGLM-130B\\n\\nCodeGen\\n\\nLLaMA\\n\\nPanGu-Σ\\n\\nBloombergGPT\\n\\nXuanYuan 2.0\\n\\nCodeT5+\\n\\nStarCoder\\n\\nLLaMA-2\\n\\nPaLM-2\\n\\nLLaMA-3/3.1\\n\\nNemotron-40B\\n\\nDeepSeek\\n\\nDeepSeek-v2\\n\\nFindings & Insights\\n\\nTraining with a mixture of denoisers outperforms PaLM when trained further for a few more FLOPs\\n\\nTraining with a mixture of denoisers improves the infilling ability and open-ended text generation diversity'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Training with a mixture of denoisers improves the infilling ability and open-ended text generation diversity\\n\\nMode switching training enables better performance on downstream tasks • CoT prompting outperforms standard prompting for UL2\\n\\nPre-training data with a small proportion of multi-task instruction data improves the overall model performance\\n\\nMulti-step prompting for code synthesis leads to a better user intent understanding and code gen- eration'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Multi-step prompting for code synthesis leads to a better user intent understanding and code gen- eration\\n\\nA constant performance improvement is observed when scaling the model • Smaller models can achieve good performances with more training data and computing time\\n\\nSparse models provide the benefits of large models at a lower computation cost • Randomly Routed Experts reduces catastrophic forgetting effects which in turn is essential for continual learning'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Randomly Routed Experts allow extracting a domain-specific sub-model in deployment which is cost-efficient while maintaining a performance similar to the original\\n\\nPre-training with general-purpose and task-specific data improves task performance without hurt- ing other model capabilities\\n\\nCombining pre-training and fine-tuning stages in single training avoids catastrophic forgetting'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Combining pre-training and fine-tuning stages in single training avoids catastrophic forgetting\\n\\nCausal LM is crucial for a model’s generation capability in encoder-decoder architectures • Multiple training objectives like span corruption, Causal LM, matching, etc complement each other for better performance\\n\\nHHH prompt by Anthropic allows the model to follow instructions without fine-tuning'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='HHH prompt by Anthropic allows the model to follow instructions without fine-tuning\\n\\nModel trained on unfiltered data is more toxic but may perform better on downstream tasks after fine-tuning\\n\\nModel trained on unfiltered data requires fewer samples for safety alignment\\n\\nData quality is important to train better models • Model and data size should be scaled with 1:1 proportions • Smaller models trained for larger iterations outperform larger models'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Increasing batch size gradually stabilizes the training without loss spikes • High-quality data at the final stages of training improves the model performance • Increasing model context length windows step-wise allows it to better adapt to various sequence lengths\\n\\nModel aligned iteratively on synthetic data with data generated from the previously aligned model achieves competitive performance\\n\\nBatch size should increase with the increase in compute budget while decreasing the learning rate'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Batch size should increase with the increase in compute budget while decreasing the learning rate\\n\\nMult-head latent attention (MLA) performs better than multi-head attention (MHA) while requiring a significantly smaller KV cache, therefore achieving faster data generation\\n\\n14\\n\\nModels\\n\\nT0\\n\\nWebGPT\\n\\nTk-INSTRUCT\\n\\nmT0 and BLOOMZ\\n\\nOPT-IML\\n\\nSparrow\\n\\nFlan\\n\\nWizardCoder\\n\\nLLaMA-2-Chat\\n\\nLIMA\\n\\nTable 2: Key insights and findings from the study of instruction-tuned Large Language Models.\\n\\nFindings & Insights'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='WizardCoder\\n\\nLLaMA-2-Chat\\n\\nLIMA\\n\\nTable 2: Key insights and findings from the study of instruction-tuned Large Language Models.\\n\\nFindings & Insights\\n\\nMulti-task prompting enables zero-shot generalization and outperforms baselines • Even a single prompt per dataset task is enough to improve performance\\n\\nTo aid the model in effectively filtering and utilizing relevant information, human labelers play a crucial role in answering questions regarding the usefulness of the retrieved documents'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Interacting a fine-tuned language model with a text-based web-browsing environment can improve end-to-end retrieval and synthesis via imitation learning and reinforcement learning\\n\\nGenerating answers with references can make labelers easily judge the factual accuracy of answers'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Generating answers with references can make labelers easily judge the factual accuracy of answers\\n\\nInstruction tuning leads to a stronger generalization of unseen tasks • More tasks improve generalization whereas only increasing task instances does not help • Supervised trained models are better than generalized models • Models pre-trained with instructions and examples perform well for different types of inputs'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Instruction tuning enables zero-shot generalization to tasks never seen before • Multi-lingual training leads to even better zero-shot generalization for both English and non- English\\n\\nTraining on machine-translated prompts improves performance for held-out tasks with non-English prompts\\n\\nEnglish only fine-tuning on multilingual pre-trained language model is enough to generalize to other pre-trained language tasks'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='English only fine-tuning on multilingual pre-trained language model is enough to generalize to other pre-trained language tasks\\n\\nCreating a batch with multiple task examples is important for better performance • Only example proportional sampling is not enough, training datasets should also be proportional for better generalization/performance\\n\\nFully held-out and partially supervised tasks performance improves by scaling tasks or categories whereas fully supervised tasks have no effect'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Fully held-out and partially supervised tasks performance improves by scaling tasks or categories whereas fully supervised tasks have no effect\\n\\nIncluding small amounts i.e. 5% of pretraining data during fine-tuning is effective • Only 1% reasoning data improves the performance, adding more deteriorates performance • Adding dialogue data makes the performance worse'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Labelers’ judgment and well-defined alignment rules help the model generate better responses • Good dialogue goals can be broken down into detailed natural language rules for the agent and the raters\\n\\nThe combination of reinforcement learning (RL) with reranking yields optimal performance in terms of preference win rates and resilience against adversarial probing'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Finetuning with CoT improves performance on held-out tasks • Fine-tuning along with CoT data improves reasoning abilities • CoT tuning improves zero-shot reasoning • Performance improves with more tasks • Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models • Improving the model’s performance with instruction tuning is compute-efficient • Multitask prompting enables zero-shot generalization abilities in LLM'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Fine-tuning with re-written instruction-tuning data into a complex set improves performance\\n\\nModel learns to write safe responses with fine-tuning on safe demonstrations, while additional RLHF step further improves model safety and make it less prone to jailbreak attacks\\n\\nLess high quality data is enough for fine-tuned model generalization\\n\\n15\\n\\nFigure 10: This example illustrates the PanGu-(cid:80) architecture, as depicted in the image sourced from [92].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='15\\n\\nFigure 10: This example illustrates the PanGu-(cid:80) architecture, as depicted in the image sourced from [92].\\n\\n3.2.1. Instruction-Tuning with Manually Created Datasets'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Numerous hand-crafted instruction-tuning datasets with different design choices are proposed in the literature to instruction-tune LLMs. The performance of fine-tuned LLMs depends on multiple factors, such as dataset, instruction diver- sity, prompting templates, model size, and training objectives. Keeping this in view, diverse fine-tuned models have emerged in the literature using manually created datasets. The models T0 [17] and mT0 (multi-lingual) [154] employ templates to convert existing'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='have emerged in the literature using manually created datasets. The models T0 [17] and mT0 (multi-lingual) [154] employ templates to convert existing datasets into prompt datasets. They have shown improvements in generalization to zero-shot and held-out tasks. Tk-Instruct [18] fine-tuned the T5 model with in-context instructions to study generalization on unseen tasks when given in-context instructions during test time. The model outperformed Instruct-GPT, despite being smaller in size, i.e.,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='on unseen tasks when given in-context instructions during test time. The model outperformed Instruct-GPT, despite being smaller in size, i.e., 11B parameters as compared to 175B of GPT-3. Increasing Tasks and Prompt Setups: Zero-shot and few-shot performance improves significantly by expanding task collec- tion and prompt styles. OPT-IML [97] and Flan [16] curated larger 2k and 1.8k task datasets, respectively. While increasing task size alone is not enough, OPT-IML and Flan add more prompting'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Flan [16] curated larger 2k and 1.8k task datasets, respectively. While increasing task size alone is not enough, OPT-IML and Flan add more prompting setups in their datasets, zero-shot, few-shot, and CoT. In continuation, CoT Collection [101] fine-tunes Flan-T5 further on 1.88M CoT samples. Another method [102] uses symbolic tasks with tasks in T0, Flan, etc.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='3.2.2. Instruction-Tuning with LLMs Generated Datasets'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Generating an instruction-tuning dataset requires carefully writing instructions and input-output pairs, which are often written by humans, smaller in size, and less diverse. To over- come this, self-instruct [19] proposed an approach to prompt available LLMs to generate instruction-tuning datasets. Self- instruct outperformed models trained on manually created dataset SUPER-NATURALINSTRUCTIONS (a dataset with 1600+ tasks) [18] by 33%. It starts with a seed of 175 tasks, 1 instruction, and 1'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='manually created dataset SUPER-NATURALINSTRUCTIONS (a dataset with 1600+ tasks) [18] by 33%. It starts with a seed of 175 tasks, 1 instruction, and 1 sample per task and iteratively generates new instructions (52k) and instances (82k input-output pairs) using'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='16\\n\\nFigure 11: An example image shows an instance of the Flan training paradigm, taken from [16].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='GPT-3 [6]. Contrary to this, Dynosaur [155] uses the meta-data of datasets on Huggingface to prompt LLMs to generate multi- ple task instruction-tuning datasets. LLaMA Tuned: Various models in the literature instruction- tune LLaMA [156] with GPT-3 [6] or GPT-4 [157] gener- ated datasets. Among these, Alpaca [158], Vicuna [159], and LLaMA-GPT-4 [160] are a few general-purpose fine-tuned models, where Alpaca is trained on 52k samples from text- davinci-003, Vicuna on 70k samples from'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[160] are a few general-purpose fine-tuned models, where Alpaca is trained on 52k samples from text- davinci-003, Vicuna on 70k samples from ShareGPT.com, and LLaMA-GPT-4 by re-creating Alpaca instructions from GPT- 4. Goat [161] fine-tunes LLaMA for arithmetic tasks (1 million samples) by generating data from ChatGPT and outperforms GPT-4, PaLM, BLOOM, OPT, etc., attributing its success to the LLaMA’s consistent tokenization of numbers. HuaTuo [162] is a medical knowledge model, fine-tuned'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='BLOOM, OPT, etc., attributing its success to the LLaMA’s consistent tokenization of numbers. HuaTuo [162] is a medical knowledge model, fine-tuned with a generated QA dataset of 8k instructions. Complex Instructions: Evol-Instruct [163, 164] prompts LLMs to convert given instructions into a more complex set. The in- structions are iteratively evolved with re-writing instructions in complex wording and creating new instructions. With this style of automated instruction generation, WizardLM [163]'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='with re-writing instructions in complex wording and creating new instructions. With this style of automated instruction generation, WizardLM [163] (fine- tuned LLaMA on 250k instructions), outperforms Vicuna and Alpaca, and WizardCoder [164] (fine-tuned StarCoder) beats Claude-Plus, Bard, and others.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='3.2.3. Aligning with Human Preferences'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Incorporating human preferences into LLMs presents a significant advantage in mitigating undesirable behaviors and ensuring accurate outputs. The initial work on alignment, such as InstructGPT [20] aligns GPT-3 using a 3-step approach, instruction-tuning, reward modeling, and fine-tuning with reinforcement learning (RL). The supervised fine-tuned GPT-3 on demonstrations is queried to generate responses, which human labelers rank according to human values, and a reward model is trained on the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='GPT-3 on demonstrations is queried to generate responses, which human labelers rank according to human values, and a reward model is trained on the ranked data. Lastly, the GPT-3 is trained with proximal policy optimization (PPO) using rewards on the generated data from the reward model. LLaMA 2-Chat [21] improves alignment by dividing reward modeling into help- fulness and safety rewards and using rejection sampling in addition to PPO. The initial four versions of LLaMA 2-Chat are fine-tuned'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='into help- fulness and safety rewards and using rejection sampling in addition to PPO. The initial four versions of LLaMA 2-Chat are fine-tuned with rejection sampling and then with PPO on'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='top of rejection sampling. Aligning with Supported Evidence: This style of alignment allows the model to generate responses with proofs and facts, reduces hallucination, and assists humans more effectively, Similar to which increases trust the RLHF training style, a reward model is trained to rank generated responses containing web citations in answers to questions, which is later used to train the model, as in GopherCite [165], WebGPT [166], and Sparrow [167]. The ranking model in Sparrow'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='answers to questions, which is later used to train the model, as in GopherCite [165], WebGPT [166], and Sparrow [167]. The ranking model in Sparrow [167] is divided into two branches, preference reward and rule reward, where human annotators adversarial probe the model to break a rule. These two rewards together rank a response to train with RL. Aligning Directly with SFT: The PPO in the RLHF pipeline is complex, memory-intensive, and unstable, requiring mul- tiple models, reward, value,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='RL. Aligning Directly with SFT: The PPO in the RLHF pipeline is complex, memory-intensive, and unstable, requiring mul- tiple models, reward, value, policy, and reference models. Avoiding this sophisticated alignment pipeline is possible by incorporating minimal changes in the supervised fine-tuning (SFT) pipeline as in [168, 169, 170], with better or compa- rable performance to PPO. Direct preference optimization (DPO) [168] trains a model directly on the human-preferred responses to maximize'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='or compa- rable performance to PPO. Direct preference optimization (DPO) [168] trains a model directly on the human-preferred responses to maximize the likelihood of preferred against unpreferred responses, with per-sample importance weight. Reward ranked fine-tuning RAFT [169] fine-tunes the model on ranked responses by the reward model. Preference ranking optimization (PRO) [171] and RRHF [170] penalize the model to rank responses with human preferences and supervised loss. On the other hand,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='ranking optimization (PRO) [171] and RRHF [170] penalize the model to rank responses with human preferences and supervised loss. On the other hand, chain-of-hindsight (CoH) [172] provides feedback to the model in language rather than reward, to learn good versus bad responses. Aligning with Synthetic Feedback: Aligning LLMs with human feedback is slow and costly. The literature suggests a semi-automated process to align LLMs by prompting LLMs to generate helpful, honest, and ethical responses'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='slow and costly. The literature suggests a semi-automated process to align LLMs by prompting LLMs to generate helpful, honest, and ethical responses to the queries, and fine-tuning using the newly created dataset. Constitutional AI [173] replaces human feedback in RLHF with AI, calling it RL from AI feedback (RLAIF). AlpacaFarm [174] designs prompts to imitate human feedback using LLMs APIs. Oppo- site to constitutional AI, AlpacaFarm injects noise in feedback to replicate human mistakes.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='prompts to imitate human feedback using LLMs APIs. Oppo- site to constitutional AI, AlpacaFarm injects noise in feedback to replicate human mistakes. Self-Align [98] prompts the LLM with ICL examples, instructing the LLM about what the response should contain to be considered useful and ethical. The same LLM is later fine-tuned with the new dataset. Aligning with Prompts: LLMs can be steered with prompts to generate desirable responses without training [175, 176]. The self-correction prompting'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Aligning with Prompts: LLMs can be steered with prompts to generate desirable responses without training [175, 176]. The self-correction prompting in [176] concatenates instructions and CoT with questions, guiding the model to answer its instruction following a strategy to ensure moral safety before the actual answer. This strategy is shown to reduce the harm in generated responses significantly. Red-Teaming/Jailbreaking/Adversarial Attacks: LLMs exhibit harmful behaviors, hallucinations,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='reduce the harm in generated responses significantly. Red-Teaming/Jailbreaking/Adversarial Attacks: LLMs exhibit harmful behaviors, hallucinations, leaking personal in- formation, and other shortcomings through adversarial probing. The models are susceptible to generating harmful responses even though they are aligned for safety [177, 178]. Red- teaming is a common approach to address illicit outputs, where the LLMs are prompted to generate harmful outputs [178, 179].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='in the model’s output.\\n\\n17\\n\\nThe dataset collected through red-teaming is used to fine-tune models for safety. While red-teaming largely relies on human annotators, another work [180] red-team LLMs to find prompts that lead to harmful outputs for other LLMs.\\n\\n3.2.4. Continue Pre-Training'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Although fine-tuning boosts a model’s performance, it leads to catastrophic forgetting of previously learned information. Concatenating fine-tuning data with a few randomly selected pre-training samples in every iteration avoids network forget- ting [181, 152]. This is also effective in adapting LLMs for cases where fine-tuning data is small and the original capac- ity is to be maintained. Prompt-based continued pre-training (PCP) [182] trains the model with text and instructions related to'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='the original capac- ity is to be maintained. Prompt-based continued pre-training (PCP) [182] trains the model with text and instructions related to tasks and then finally instruction-tunes the model for down- stream tasks.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='3.2.5. Sample Efficiency'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='While fine-tuning data is generally many-fold smaller than the pre-training data, it still has to be large enough for accept- able performance [16, 97, 18] and requires proportional com- puting resources. Studying the effects on performance with less data, existing literature [183, 184] finds that models trained on less data can outperform models trained with more data. In [183], 25% of the total downstream data is found enough for state-of-the-art performance. Selecting coreset-based 0.5% of'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='trained with more data. In [183], 25% of the total downstream data is found enough for state-of-the-art performance. Selecting coreset-based 0.5% of the total instruction-tuning data improves the model perfor- mance by 2% in [184], as compared to the complete data tun- ing. Less is more for alignment (LIMA) [185] uses only 1000 carefully created demonstrations to fine-tune the model and has achieved comparable performance to GPT-4.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='3.3. Increasing Context Window'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='LLMs are trained with limited context windows due to ex- pensive attention and high memory requirements. A model trained on limited sequence lengths fails to generalize to unseen lengths at inference time [186, 49]. Alternatively, LLMs with ALiBi [65] positional encodings can perform zero-shot length extrapolation. However, ALiBi has less expressive power [66] and inferior performance on multiple benchmarks [46], and many LLMs use RoPE positional embedding that is unable to perform zero-shot'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='power [66] and inferior performance on multiple benchmarks [46], and many LLMs use RoPE positional embedding that is unable to perform zero-shot extrapolation. A larger context length has benefits such as a better understanding of longer documents, more samples in in-context learning, execution of bigger rea- soning processes, etc. Expanding context length during fine- tuning is slow, inefficient, and computationally expensive [49]. Therefore, researchers employ various context window extrap-'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='length during fine- tuning is slow, inefficient, and computationally expensive [49]. Therefore, researchers employ various context window extrap- olation techniques discussed below. Position Interpolation: Rather than extrapolating, [49] shows that interpolating position encodings within the pre-trained con- text window are more effective. The work demonstrates that only 1000 steps of fine-tuning are enough to achieve better re- sults on larger windows without reducing performance com- pared to'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='demonstrates that only 1000 steps of fine-tuning are enough to achieve better re- sults on larger windows without reducing performance com- pared to the original context size. Giraffe [46] uses power scal- ing in RoPE, and YaRN [47] proposed NTK-aware interpola- tion.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Efficient Attention Mechanism: Dense global attention is one of the major constraints in training larger context win- dow LLMs. Using efficient attention variants, such as lo- cal, sparse, and dilated attention, reduces the computation cost significantly. LongT5 [48] proposes transient global atten- tion (TGlobal), applying attention to local and global tokens (windowed token averaging). The model replaces attention in T5 [10] with TGlobal attention, pre-trains the model on 4098 sequence'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='and global tokens (windowed token averaging). The model replaces attention in T5 [10] with TGlobal attention, pre-trains the model on 4098 sequence length, fine-tunes on larger window sizes, as large as 16k, and improves task performance on longer inputs. This shows the extrapolation ability of TGlobal attention with only fine-tuning. COLT5 [187] uses two branches, one with lightweight and the other with heavyweight attention and feed- forward layers. All tokens are processed from the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[187] uses two branches, one with lightweight and the other with heavyweight attention and feed- forward layers. All tokens are processed from the lightweight branch, and only important tokens are routed to the heavy- weight branch. LongNet [188] replaces standard attention with dilated attention, expanding sequence length to 1 billion tokens. LongLoRA [189] proposes shift-short attention, used during fine-tuning to reduce dense attention costs. However, the model during inference uses dense'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[189] proposes shift-short attention, used during fine-tuning to reduce dense attention costs. However, the model during inference uses dense attention and achieves similar per- formance as full attention fine-tuning. Extrapolation without Training: LM-Infinite [186] and par- allel context windows (PCW) [190] show length extrapolation is possible using pre-trained LLMs. LM-Infinite suggested Λ- shaped attention applied within the original context window limits. Likewise, PCW chunks larger'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='using pre-trained LLMs. LM-Infinite suggested Λ- shaped attention applied within the original context window limits. Likewise, PCW chunks larger inputs into the pre-trained context lengths and applies the same positional encodings to each chunk.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='3.4. Augmented LLMs'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='LLMs are capable of learning from the examples concate- in- nated with the input, known as context augmentation, context learning (ICL), or few-shot prompting. They show ex- cellent generalization to unseen tasks with few-shot prompt- ing, enabling LLMs to answer queries beyond the capacity ac- quired during training [6, 55]. These emergent abilities allow for adapting the model without fine-tuning—a costly process. Aside from this, hallucination, producing inaccurate, unsafe, or factually'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='allow for adapting the model without fine-tuning—a costly process. Aside from this, hallucination, producing inaccurate, unsafe, or factually incorrect responses, is common for LLMs, which is avoided by augmenting contextual data. While the user can pro- vide in-context samples in the query [54, 32], here we specifi- cally refer to the methods that access external storage program- matically, calling them augmented LLMs. The literature suggests various external memory designs to aug- ment LLMs,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='access external storage program- matically, calling them augmented LLMs. The literature suggests various external memory designs to aug- ment LLMs, long-term [191, 192, 193, 194], short-term [195], symbolic [196], and non-symbolic [197, 198]. The memory can be maintained in different formats such as documents, vec- tors, or databases. A few systems maintain intermediate mem- ory representations to retain information across multiple iter- ations [194, 192], while others extract important'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='systems maintain intermediate mem- ory representations to retain information across multiple iter- ations [194, 192], while others extract important information from the datasets and save it in memory for recall [199]. The memory read and write operations are performed either with or without LLMs cooperation [192, 200, 194, 201], acting as a feedback signal in [195]. We discuss different types of aug- mented LLMs below.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='18\\n\\nFigure 12: A flow diagram of Retrieval Augmented LLMs. The retriever ex- tracts a similar context to the input and forwards it to the LLM either in simple language or encoded through Fusion-in-Decoder (FiD). Depending on the task, retrieval and generation may repeat multiple times.\\n\\n3.4.1. Retrieval Augmented LLMs'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='LLMs may have limited memory and outdated information, leading to inaccurate responses. Retrieving relevant informa- tion from external up-to-date storage enables the LLMs to accurately answer with references and utilize more informa- tion. With retrieval augmentation, smaller models have been shown to perform at par with larger models. For instance, the 11B model can become competitive to 540B PaLM in [25] and 7.5B to 280B Gopher in [193]. Retrieval augmented language modeling (RALM) has two'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='the 11B model can become competitive to 540B PaLM in [25] and 7.5B to 280B Gopher in [193]. Retrieval augmented language modeling (RALM) has two major components, shown in Figure 12, namely: 1) retriever and 2) language model. In RALM, the retriever plays a crucial role in driving LLM response, where incorrect information can steer LLMs to false behavior. This leads to the development of various methods to retrieve accurate information and fuse with the query for better performance. Zero-Shot'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='behavior. This leads to the development of various methods to retrieve accurate information and fuse with the query for better performance. Zero-Shot Retrieval Augmentation: This kind of augmen- tation keeps the original LLM architecture and weights unchanged and uses BM25 [202], nearest neighbors, or frozen pre-trained models like Bert [7] as a retriever. The retrieved information is provided as input to the model for response generation, shown to improve performance over LLMs without In some'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='The retrieved information is provided as input to the model for response generation, shown to improve performance over LLMs without In some scenarios, multiple retrieval retrieval [198, 203]. The output iterations are required to complete the task. generated in the first iteration is forwarded to the retriever to fetch similar documents. Forward-looking active retrieval (FLARE) [197] initially generates the response and corrects the output by retrieving relevant documents if the response'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='active retrieval (FLARE) [197] initially generates the response and corrects the output by retrieving relevant documents if the response contains low-confidence tokens. Similarly, RepoCoder [204] fetches code snippets recursively for code completion. Training with Retrieval Augmentation: To reduce failures in retrieval augmentation generation (RAG), researchers train or fine-tune retrievers and LLMs with a retrieval augmentation pipeline. We discuss the literature below based on their focus on'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='researchers train or fine-tune retrievers and LLMs with a retrieval augmentation pipeline. We discuss the literature below based on their focus on the respective training processes of the pipeline. Training LLM: Retrieval-enhanced transformer (RETRO) [193] shows pre-training smaller LLMs with RAG pipeline outper- forms larger LLMs, such as GPT-3 trained without RAG. RETRO uses a 2-trillion token subset of MassiveText as'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='a database. The retrieval pipeline divides the input query into subsets and retrieves relevant chunks from the database for each subset, encoded together with input intermediate representations for generating tokens. It uses cross-chunked attention to attend to previous chunks auto-regressively. A study on RETRO [205] shows models pre-trained without RAG but fine-tuned using RAG lack the performance gains obtained by pre-training with RAG. Training Retriever: Quality of responses generated by'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='RAG but fine-tuned using RAG lack the performance gains obtained by pre-training with RAG. Training Retriever: Quality of responses generated by LLMs is highly dependent on the in-context examples. There- fore, [206, 207, 208, 209] train retrievers to retrieve accurate few-shot samples while keeping the LLM frozen for gener- ation. Retrieved samples are ranked to build ground-truth data to train retrievers with contrastive learning in [206, 208]. RoBERTa is trained for downstream tasks in [207]'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='are ranked to build ground-truth data to train retrievers with contrastive learning in [206, 208]. RoBERTa is trained for downstream tasks in [207] for ICL samples retrieval. REPLUG [209] trains the retriever with supervised signals from the frozen LLM-generated outputs. Training Retriever and LLM: Further benefits are achieved by training both the retriever and the model in [25, 210, 211]. In this case, the error propagates back to the retriever, updating both the language model and the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='the retriever and the model in [25, 210, 211]. In this case, the error propagates back to the retriever, updating both the language model and the retriever. While masked language modeling (MLM) is a common pre-training objec- tive [25, 211], retrieval pre-trained transformer (RPT) [210] used document chunk prediction as a pre-training objective for long text modeling. Encoded Context Augmentation: Concatenating retrieved documents with the query becomes infeasible as the sequence length and'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='for long text modeling. Encoded Context Augmentation: Concatenating retrieved documents with the query becomes infeasible as the sequence length and sample size grow. Encoding the context and fusing it with the decoder (Fusion-in-Decoder) using cross-attention makes it possible to augment more samples without increasing computation costs significantly [212, 193, 210, 25]. Web Augmented: Locally stored memory, but external to LLM, has limited information. However, a large amount of information'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[212, 193, 210, 25]. Web Augmented: Locally stored memory, but external to LLM, has limited information. However, a large amount of information is available on the internet, which gets updated regularly. Rather than storing information locally, various methods retrieve query-related context through a web search and forward it to LLMs [213, 214, 166].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='3.4.2. Tool Augmented LLMs'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='While RAG relies on the retriever to provide context to the LLM to answer queries, tool augmented LLMs capitalize on the reasoning abilities of LLMs to iteratively plan by dividing tasks into sub-tasks, selecting necessary tools, and taking actions to complete the task [215, 216, 217, 27]. A generic pipeline of tool-augmented LLMs is shown in Figure 13, where different modules in Figure 13 are selected in a loop until the task com- pletion. Zero-Shot Tool Augmentation: LLMs in-context learning'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='13, where different modules in Figure 13 are selected in a loop until the task com- pletion. Zero-Shot Tool Augmentation: LLMs in-context learning and reasoning abilities enable them to interact with tools with- out training. Automatic reasoning and tool-use (ART) [217] builds a task library with demonstrations of reasoning steps and calling external tools. It retrieves similar task examples and provides the context to the LLM for inference. Aside from this, [218] shows tool documentation is'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='tools. It retrieves similar task examples and provides the context to the LLM for inference. Aside from this, [218] shows tool documentation is enough to teach LLMs to use tools without demonstrations. RestGPT [219] integrates LLMs with RESTful APIs by decomposing tasks into planning'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='19\\n\\nFigure 13: A basic flow diagram of tool augmented LLMs. Given an input and a set of available tools, the model generates a plan to complete the task. The tool augmented LLMs utilize different modules iteratively, such as retriever, tool execution, read-write to memory, feedback, etc., depending on the task.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='and API selection steps. The API selector understands the API documentation to select a suitable API for the task and plan the execution. ToolkenGPT [220] uses tools as tokens by concate- nating tool embeddings with other token embeddings. During inference, the LLM generates the tool tokens representing the tool call, stops text generation, and restarts using the tool exe- cution output. Training with Tool Augmentation: LLMs are trained to inter- act with diverse tools, enhancing planning'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='restarts using the tool exe- cution output. Training with Tool Augmentation: LLMs are trained to inter- act with diverse tools, enhancing planning abilities to overcome the limitations of zero-shot tool augmentation [221, 27, 222, 223]. Gorilla [221] instruction-tunes LLaMA with information retrieval from API documentation. It uses the self-instruct [19] data generation pipeline with GPT-4 by providing in-context examples retrieved from API documentation. Tool augmented language model (TALM)'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[19] data generation pipeline with GPT-4 by providing in-context examples retrieved from API documentation. Tool augmented language model (TALM) [27] fine-tunes T5 [10] for tool use with a self-play approach, where it iteratively completes tool manipulation tasks and includes them back in the training set. ToolLLM [223] collects 16k APIs from RapidAPI. It samples APIs from the list to generate an instruction-tuning dataset us- ing ChatGPT in single-tool and multi-tool scenarios. For high-'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='RapidAPI. It samples APIs from the list to generate an instruction-tuning dataset us- ing ChatGPT in single-tool and multi-tool scenarios. For high- quality datasets, ToolLLM suggested a depth-first search-based decision tree (DFSDT) method to generate ground-truths with diverse reasoning and planning. Multimodal Tool Augmentation: The compositional reasoning capacity of LLMs allows them to manipulate tools in multi- modal settings [215, 216, 224]. Following the pipeline shown in Figure 13, the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='reasoning capacity of LLMs allows them to manipulate tools in multi- modal settings [215, 216, 224]. Following the pipeline shown in Figure 13, the LLM outlines a plan, generally executing in a sequence: Plan → Tool selection → Execute → Inspect → Generate, to respond to the user query. Here, the database of tools is rich in modalities, including text, images, etc. Many of the multimodal tool augmentation systems employ multimodal LLMs [31, 225, 224, 216], while others utilize single modality'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='LLMs and generate a plan on using different modality tools to solve multimodal queries [226].\\n\\n3.5. LLMs-Powered Agents'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='AI agents are autonomous entities, capable of planning, decision-making, and performing actions to achieve complex goals. In the early days, AI agents were rule-based, de- signed for narrow tasks, and had limited capabilities, such as Clippy [227] and Deep Blue [228]. In contrast to this, LLMs abilities to respond to dynamic scenarios have made it possible to incorporate them in diverse applications, includ- ing LLMs-powered agents [224, 216], where LLMs behave as the brain of agents. LLMs have'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='possible to incorporate them in diverse applications, includ- ing LLMs-powered agents [224, 216], where LLMs behave as the brain of agents. LLMs have been incorporated in web agents [166, 167], coding agents [229], tool agents [27, 223], embodied agents [26], and conversational agents [195], requir- ing minimal to no fine-tuning\". Below we summarize the re- search in LLMs-based autonomous agents. For a more detailed discussion, please refer to [230, 231]. LLMs Steering Autonomous Agents: LLMs'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='the re- search in LLMs-based autonomous agents. For a more detailed discussion, please refer to [230, 231]. LLMs Steering Autonomous Agents: LLMs are the cognitive controllers of the autonomous agents. They generate plans, rea- son about tasks, incorporate memory to complete tasks, and adapt the outline depending on the feedback from the environ- ment. Depending on the acquired capabilities of LLMs, many methods fine-tune, propose a better prompting approach, or uti- lize different modules to'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='ment. Depending on the acquired capabilities of LLMs, many methods fine-tune, propose a better prompting approach, or uti- lize different modules to enhance agents’ performance. Mod- ules and strategies employed in autonomous agents are briefly discussed below. Planning and Reasoning: Completing a complex task requires human-like logical thinking, planning necessary steps, and reasoning current and future directions. Prompting methods like chain-of-thoughts [103], tree-of-thoughts [105], and'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='planning necessary steps, and reasoning current and future directions. Prompting methods like chain-of-thoughts [103], tree-of-thoughts [105], and self- consistency [104] are central to agents, eliciting LLMs to rea- son its actions and choose among different paths for task com- pletion. When LLMs are prompted with a task description and a sequence of actions, they can accurately generate plan ac- tions without any fine-tuning [232]. Reasoning via planning (RAP) [233] incorporates a re-purposed'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='of actions, they can accurately generate plan ac- tions without any fine-tuning [232]. Reasoning via planning (RAP) [233] incorporates a re-purposed LLM as a world model to reason about future outcomes and explore alternative paths for task completion. Retroformer [234] uses a retrospective LLM to improve main LLM planning and reasoning capabil- ities by providing helpful task cues. Feedback: LLMs in open-loop systems generate plans and as- sume that the agent will complete them successfully.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='ities by providing helpful task cues. Feedback: LLMs in open-loop systems generate plans and as- sume that the agent will complete them successfully. However, the actual scenario is different with failures and variable re- sponses from the environment. To correctly complete tasks, many methods use LLMs in a closed-loop where the action re- sponse is provided as feedback to the LLMs to re-assess and update the plan as required [235, 236, 237, 195]. Another di- rection of research exploits LLMs'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='is provided as feedback to the LLMs to re-assess and update the plan as required [235, 236, 237, 195]. Another di- rection of research exploits LLMs as reward functions to train reinforcement learning (RL) policies instead of humans [238]. Memory: LLMs can learn from the context provided in the prompt. In addition to internal memory, various systems em- ploy external memory to save the response history. Reflex- ion [195] maintains an episodic memory to use previous re- sponses as feedback to'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='em- ploy external memory to save the response history. Reflex- ion [195] maintains an episodic memory to use previous re- sponses as feedback to improve future decision-making. Retro- former [234] improves its responses by employing short-term'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='20'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='and long-term memory, where short-term memory contains re- cent responses and long-term memory keeps summarized failed attempts to add in the prompt as reflection. Multi-Agents Systems: LLMs can play user-defined roles and behave like a specific domain expert. In multi-agent systems, each LLM is assigned a unique role, simulating human behav- ior and collaborating with other agents to complete a complex task [229, 239]. LLMs LLMs are good at instruction-following, however, utilizing them for'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='and collaborating with other agents to complete a complex task [229, 239]. LLMs LLMs are good at instruction-following, however, utilizing them for physically grounded tasks requires adaptation, as they lack real-world knowledge. This could lead to generating illogical responses for a particular physical situation [240, 26]. SayCan [240] make LLMs aware of the available low-level task operations. LLM (Say) builds a high-level plan to complete the task and a learned affordance function (Can)'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='aware of the available low-level task operations. LLM (Say) builds a high-level plan to complete the task and a learned affordance function (Can) explores the possibility of executing the plan in the real world. SayCan uses RL to train the language-conditioned affordance function. PaLM-E enables the LLM to solve grounded tasks by training multi-modal LLM feeding inputs directly from the sensors. Manipulation: In the area of manipulation [236, 241], LLMs enhance a robot’s dexterity and'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='multi-modal LLM feeding inputs directly from the sensors. Manipulation: In the area of manipulation [236, 241], LLMs enhance a robot’s dexterity and adaptability, excelling in tasks like object recognition, grasping, and collaboration. They ana- lyze visual and spatial information to determine the most effec- tive approach to interact with objects. Navigation: LLMs enhance a robot’s ability to navigate com- plex environments with precision and adaptability [242, 243, 244, 245]. They generate'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Navigation: LLMs enhance a robot’s ability to navigate com- plex environments with precision and adaptability [242, 243, 244, 245]. They generate feasible paths and trajectories for robots, accounting for intricate environmental details [246]. This ability is valuable in scenarios requiring precise and dynamically adaptable navigation in environments like ware- houses, transport, healthcare facilities, and residences.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='in Physical Environment:\\n\\n3.6. Efficient LLMs\\n\\nDeploying LLMs in production is expensive. Reducing their running costs while preserving performance is an appealing area of research. This section summarizes the approaches sug- gested to enhance LLMs’ efficiency.\\n\\n3.6.1. Parameter Efficient Fine-Tuning'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Fine-tuning LLMs with tens or hundreds of billions of pa- rameters, such as GPT-3 (175B), BLOOM (176B), MT-NLG (540B), etc., is computationally intensive and time-consuming. To avoid complete model fine-tuning, numerous parameter- efficient fine-tuning (PEFT) techniques [40, 247, 41, 38, 39] try to achieve acceptable model fine-tuning performance at reduced costs. As compared to full fine-tuning [248], PEFT performs better in low-resource setups, achieves comparable perfor- mance on'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='at reduced costs. As compared to full fine-tuning [248], PEFT performs better in low-resource setups, achieves comparable perfor- mance on medium-resource scenarios, and performs worse than full fine-tuning under high-resource availability. An overview of different PEFT approaches is shown in Figure 14. Adapter Tuning: Adds a few trainable parameters within the transformer block. The adapter layer is a sequence of feature downscaling, non-linearity, and upscaling [106]. Variants of adapter'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='within the transformer block. The adapter layer is a sequence of feature downscaling, non-linearity, and upscaling [106]. Variants of adapter tuning inject adapter layers sequentially [106] and in parallel [38], whereas the mixture of adapter (AdaMix) [249]'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Figure 14: Illustration of parameter-efficient fine-tuning paradigms, where x is input and h is hidden state, figure courtesy [38]. Parallel adapter and LoRA fall in the adapter tuning category.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='employs multiple adapter modules in a single layer. AdaMix routes input instances randomly to one of the multiple down- scale and upscale modules. The mixture of adapters is averaged out for inference to avoid additional latency. Low-Rank Adap- tation (LoRA) [250] learns low-rank decomposed matrices to freeze original weights. The learned weights are fused with the original weights for inference, avoiding latency. Prompt Tuning: Prompting is an effective way to adapt a pre-trained LLM for the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='are fused with the original weights for inference, avoiding latency. Prompt Tuning: Prompting is an effective way to adapt a pre-trained LLM for the downstream task. However, manual prompts bring uncertainty in the model’s prediction, where a change in a single word drops the performance [247]. Prompt tuning alleviates this problem by fine-tuning only 0.001%-3% additional parameters [251]. It concatenates trainable prompt parameters with the model embeddings [247, 40, 251]. Task- specific fixed'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='0.001%-3% additional parameters [251]. It concatenates trainable prompt parameters with the model embeddings [247, 40, 251]. Task- specific fixed discrete prompts are concatenated with input em- beddings in [40]. As discrete prompts bring instability, prompts are encoded through a learnable mapping in P-Tuning [247], naming continuous prompts, which are appended with the dis- crete prompts. Only the prompt encoder is trainable in the model. In an extension of P-Tuning, continuous prompts are'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='are appended with the dis- crete prompts. Only the prompt encoder is trainable in the model. In an extension of P-Tuning, continuous prompts are concatenated with each layer of the network in [251]. Progres- sive prompts [252] avoid catastrophic forgetting and transfer previously learned knowledge by sequentially adding trainable prompt embeddings to the previously frozen task embeddings. Prefix Tuning: A set of trainable task-specific prefix vectors are appended to the frozen transformer'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='to the previously frozen task embeddings. Prefix Tuning: A set of trainable task-specific prefix vectors are appended to the frozen transformer layers in prefix tun- ing [41]. The prefix vectors are virtual tokens attended by the context tokens on the right. In addition, adaptive prefix tun- ing [253] applies a gating mechanism to control the information from the prefix and actual tokens. Bias Tuning: Fine-tuning only bias terms in small to medium training data has been found effective in'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='information from the prefix and actual tokens. Bias Tuning: Fine-tuning only bias terms in small to medium training data has been found effective in BitFit [254]. This method achieves full fine-tuning performance for tasks with less training data and comparable performance with more training data.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='FP16 format [44]. Such demanding requirements for deploying LLMs make it harder for smaller organizations to utilize them. Model compression is an effective solution but comes at the cost of degraded performance, especially at large scales greater than 6B. These models exhibit very large magnitude outliers that do not exist in smaller models [255], making it challenging and re- quiring specialized methods for quantizing LLMs [44, 256]. Post-Training Quantization: Minimal or no training is re-'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='making it challenging and re- quiring specialized methods for quantizing LLMs [44, 256]. Post-Training Quantization: Minimal or no training is re- quired in this type of quantization, without significantly com- promising the model performance. LLM-8-bit [255] uses full- precision matrix multiplication for weights associated with out- lier features and 8-bit for remaining features. The lower pre- cision multiplication outputs are converted to FP-16 and con- catenated with others. The quantized'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='and 8-bit for remaining features. The lower pre- cision multiplication outputs are converted to FP-16 and con- catenated with others. The quantized models have homogenous word embeddings, which may degrade their performance. To fix this, token-level knowledge distillation is employed in [45] along with independent quantization scaling factors for each module due to varying weight distribution. Feature distribu- tions are asymmetric and appear in different channels; outlier suppression [257]'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='each module due to varying weight distribution. Feature distribu- tions are asymmetric and appear in different channels; outlier suppression [257] shifts and scales per-channel activation dis- tributions for effective quantization. SmoothQuant [44] quan- tizes activations and weights to INT8 format by smoothing activations and migrating the quantization difficulty toward weights. It multiplies the inverse of the smoothing factor with weights, which introduces a few outliers in the weights but'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='difficulty toward weights. It multiplies the inverse of the smoothing factor with weights, which introduces a few outliers in the weights but is easier to quantify than unsmoothed activations. OPTQ [256] uses the optimal brain compression (OBC) [258] algorithm to quantize the model layer-by-layer and update weights to com- pensate for quantization error. To improve speed and per- formance, OPTQ updates weights in arbitrary order, employs lazy updates, and uses better Cholesky kernels.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='error. To improve speed and per- formance, OPTQ updates weights in arbitrary order, employs lazy updates, and uses better Cholesky kernels. Outlier-aware weight quantization (OWQ) [259] uses the OPTQ algorithm for quantization but assigns higher precision to vulnerable weights, causing outliers and lower precision for others. Quantization-Aware Training: To compensate for perfor- is fine-tuned in mance degradation, a quantized model quantization-aware training (QAT) [260, 261, 262]. Al- pha'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Training: To compensate for perfor- is fine-tuned in mance degradation, a quantized model quantization-aware training (QAT) [260, 261, 262]. Al- pha Tuning quantizes the model using binary coding quan- tization (BCQ) [263] and fine-tunes only quantization scal- This approach improves performance over ing factors.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='3.6.2. Quantization\\n\\nLLMs require extensive computing and memory for infer- ence. Deploying a 175B parameter GPT-3 model needs at least five 80GB A100 GPUs and 350GB of memory to store in\\n\\n21'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='parameter-efficient fine-tuning of the pre-trained model. Sim- ilarly, parameter-efficient and quantization-aware adaptation (PEQA) [264] reduces the precision of fully-connected layers and fine-tunes only quantization scaling parameters. LLM- QAT [262] generates training data from the pre-trained network and trains a quantized student model with knowledge distilla- tion. QLoRA [261] fine-tunes 4-bit quantized pre-trained LLM with LoRA [250] using a 4-bit normal float, which shows better'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='with knowledge distilla- tion. QLoRA [261] fine-tunes 4-bit quantized pre-trained LLM with LoRA [250] using a 4-bit normal float, which shows better performance over a 4-bit integer and float.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='3.6.3. Pruning'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Pruning is an alternative approach to quantization to com- press model size, thereby reducing LLMs deployment costs significantly. Compared to task-agnostic pruning, task-specific pruning is easily achievable with good performance, where a model is fine-tuned on the downstream task and pruned for It is possible to prune LLMs for individual faster inference. tasks, but the cost of pruning and deploying task-specific mod- els is high. To overcome this, many structured and unstructured pruning'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='inference. tasks, but the cost of pruning and deploying task-specific mod- els is high. To overcome this, many structured and unstructured pruning methods for LLMs have been proposed to maintain rea- sonable performance across all tasks while shrinking the model size [265, 42, 266]. Unstructured Pruning: This kind of pruning removes less im- portant weights without maintaining any structure. Existing LLM pruning methods take advantage of the unique charac- teristics of LLMs, uncommon for'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='portant weights without maintaining any structure. Existing LLM pruning methods take advantage of the unique charac- teristics of LLMs, uncommon for smaller models, where a small subset of hidden states are activated with large magni- tude [255]. Pruning by weights and activations (Wanda) [265] prunes weights in every row based on importance, calculated by multiplying the weights with the norm of input. The pruned model does not require fine-tuning, thereby saving computa- tional costs. Outlier'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='by multiplying the weights with the norm of input. The pruned model does not require fine-tuning, thereby saving computa- tional costs. Outlier weighed layerwise sparsity (OWL) [267] extends Wanda with non-uniform layer pruning. It shows that the number of outliers varies for different layers; therefore, the model should have variable pruning ratios for better perfor- mance for every layer. Contrastive pruning (CAP) [43] itera- tively prunes the model by training the sparse model using con-'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='for better perfor- mance for every layer. Contrastive pruning (CAP) [43] itera- tively prunes the model by training the sparse model using con- trastive loss between pre-trained, fine-tuned, and snapshots of previous sparse models to learn task-specific and task-agnostic knowledge. Structured Pruning: Here, the parameters are removed in groups, rows, columns, or matrices, which speeds up the inference because of effective hardware tensor core utiliza- tion [265]. LLM-Pruner [42] employs a'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='rows, columns, or matrices, which speeds up the inference because of effective hardware tensor core utiliza- tion [265]. LLM-Pruner [42] employs a 3-stage structured pruning strategy, identifying the groups of hidden states caus- ing each other to activate during the forward-pass, keeping im- portant groups and removing less important ones, and fine- tuning the pruned model with LoRA. Sparsity-induced mask learning (SIMPLE) [268] prunes the network using learnable masks. Similarly, another'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='fine- tuning the pruned model with LoRA. Sparsity-induced mask learning (SIMPLE) [268] prunes the network using learnable masks. Similarly, another method prunes LLMs by learning masks and removing unimportant rank-1 components of the factorized weight matrix [266].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='3.7. Multimodal LLMs\\n\\nInspired by the success of LLMs in natural language process- ing applications, an increasing number of research works are\\n\\n22'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='now facilitating LLMs to perceive different modalities of infor- mation like image [269, 270, 271], video [272, 273, 274], au- dio [275, 274, 276], etc. Multimodal LLMs (MLLMs) present substantial benefits compared to standard LLMs that process only text. By incorporating information from various modal- ities, MLLMs can achieve a deeper understanding of context, leading to more intelligent responses infused with a variety of expressions. Importantly, MLLMs align closely with human perceptual'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='of context, leading to more intelligent responses infused with a variety of expressions. Importantly, MLLMs align closely with human perceptual experiences, leveraging the synergistic nature of our multisensory inputs to form a comprehensive understanding of the world [276, 26]. Coupled with a user-friendly interface, MLLMs can offer intuitive, flexible, and adaptable interactions, allowing users to engage with intelligent assistants through a spectrum of input methods. According to the ways of'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='and adaptable interactions, allowing users to engage with intelligent assistants through a spectrum of input methods. According to the ways of construct- ing models, current MLLMs can be generally divided into three streams: pre-training, fine-tuning, and prompting. In this sec- tion, we will discuss more details of these main streams, as well as the important application of MLLMs in visual reasoning. Pre-training: This stream of MLLMs intends to support differ- ent modalities using unified'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='as the important application of MLLMs in visual reasoning. Pre-training: This stream of MLLMs intends to support differ- ent modalities using unified end-to-end models. For instance, Flamingo [269] applies gated cross-attention to fuse vision and language modalities, which are collected from pre-trained and frozen visual encoder and LLM, respectively. Moreover, BLIP- 2 [270] proposes a two-stage strategy to pre-train a Querying Transformer (Q-Former) for the alignment between vision and'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Moreover, BLIP- 2 [270] proposes a two-stage strategy to pre-train a Querying Transformer (Q-Former) for the alignment between vision and language modalities: in the first stage, vision-language repre- sentation learning is bootstrapped from a frozen visual encoder; and in the second stage, a frozen LLM bootstraps vision-to- language generative learning for zero-shot image-to-text gen- eration. Similarly, MiniGPT-4 [277] deploys pre-trained and frozen ViT [278], Q-Former and Vicuna LLM [159],'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='for zero-shot image-to-text gen- eration. Similarly, MiniGPT-4 [277] deploys pre-trained and frozen ViT [278], Q-Former and Vicuna LLM [159], only train- ing the linear projection layer for vision and language modali- ties alignment. Fine-tuning: Derived from instruction tuning [16] for NLP tasks [20, 16, 97], researchers are fine-tune pre-trained LLMs using multimodal instructions. Following this method, LLMs can be easily and effectively extended as multimodal chat- bots [277, 271, 29] and'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='LLMs using multimodal instructions. Following this method, LLMs can be easily and effectively extended as multimodal chat- bots [277, 271, 29] and multimodal task solvers [279, 30, 280]. The key issue of this stream of MLLMs is to collect multi- modal instruction-following data for fine-tuning [58]. To ad- dress this issue, the solutions of benchmark adaptation [279, 281, 282], self-instruction [19, 31, 283], and hybrid composi- tion [284, 280] are employed, respectively. To mitigate the gap'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='adaptation [279, 281, 282], self-instruction [19, 31, 283], and hybrid composi- tion [284, 280] are employed, respectively. To mitigate the gap between the original language modality and additional modal- ities, the learnable interface is introduced to connect differ- ent modalities from frozen pre-trained models. Particularly, the learnable interface is expected to work in a parameter- efficient tuning manner: e.g., LLaMA-Adapter [285] applies an efficient transformer-based adapter module for'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='is expected to work in a parameter- efficient tuning manner: e.g., LLaMA-Adapter [285] applies an efficient transformer-based adapter module for training, and LaVIN [284] dynamically learns the multimodal feature weights using a mixture-of-modality adapter. Different from the learnable interface, the expert models can directly convert multimodalities into language: e.g., VideoChat-Text [272] in- corporates Whisper [286], a speech recognition expert model, to generate the captions of given'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='into language: e.g., VideoChat-Text [272] in- corporates Whisper [286], a speech recognition expert model, to generate the captions of given videos for the understanding of following LLMs. Prompting: Different from the fine-tuning technique that'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='directly updates the model parameters given task-specific datasets, the prompting technique provides certain context, ex- amples, or instructions to the model, fulfilling specialized tasks without changing the model parameters. Since prompting can significantly reduce the need for large-scale multimodal data, this technique is widely used to construct MLLMs. Particularly, to solve multimodal Chain of Thought (CoT) problems [103], LLMs are prompted to generate both the reasoning process and the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='MLLMs. Particularly, to solve multimodal Chain of Thought (CoT) problems [103], LLMs are prompted to generate both the reasoning process and the answer given multimodal inputs [287]. On this front, differ- ent learning paradigms are exploited in practice: for example, Multimodal-CoT [287] involves two stages of rationale genera- tion and answer inference, where the input of the second stage is a combination of the original input and the output of the first stage; and CoT-PT [288] applies both'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='where the input of the second stage is a combination of the original input and the output of the first stage; and CoT-PT [288] applies both prompt tuning and spe- cific visual bias to generate a chain of reasoning implicitly. In addition to CoT problems, LLMs can also be prompted with multimodal descriptions and tools, effectively dividing complex tasks into sub-tasks [289, 290]. Visual Reasoning Application: Recent visual reasoning sys- tems [291, 292, 216, 293] tend to apply LLMs for better'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='tasks into sub-tasks [289, 290]. Visual Reasoning Application: Recent visual reasoning sys- tems [291, 292, 216, 293] tend to apply LLMs for better visual information analysis and visual-language integration. Differ- ent from previous works [294, 295] that rely on limited VQA datasets and small-scale neural networks, current LLM-aided methods offer benefits of stronger generalization ability, emer- gent ability, and interactivity [58]. To realize visual reasoning with the help of LLMs,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='offer benefits of stronger generalization ability, emer- gent ability, and interactivity [58]. To realize visual reasoning with the help of LLMs, prompting and fine-tuning techniques can also be utilized: for example, PointClip V2 [292] applies LLMs to generate 3D-specific prompts, which are encoded as textual features and then combined with visual features for 3D recognition; and GPT4Tools [31] employs LoRA [250] to fine-tune LLMs following tool-related instructions. Serving as a controller'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='features for 3D recognition; and GPT4Tools [31] employs LoRA [250] to fine-tune LLMs following tool-related instructions. Serving as a controller [293], decision maker [296], or semantics re- finer [291, 297], LLMs significantly facilitates the progress of visual reasoning research.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='3.8. Summary and Discussion\\n\\n3.8.1. Architecture'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Due to the gigantic scale of LLMs, minor changes in archi- tecture and training strategies have a big impact on performance and stability. Here, we summarize key architectural modules used in various LLMs, leading to better performance, reduced training time and memory, and better training stability. Layer Normalization: The performance and training stability of LLMs are affected significantly by layer normalization. Pre- norm, that is normalizing inputs rather than outputs, is more common'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='stability of LLMs are affected significantly by layer normalization. Pre- norm, that is normalizing inputs rather than outputs, is more common among LLMs stabilizing the training [6, 127, 108]. BLOOM [13] and AlexaTM [122] utilize an additional layer normalization before embedding layer to stabilize the training of large-scale models, while the model’s zero-shot generaliza- tion ability can be negatively impacted [13]. However, another study [33] finds that pre-norm degrades fine-tuned model'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='model’s zero-shot generaliza- tion ability can be negatively impacted [13]. However, another study [33] finds that pre-norm degrades fine-tuned model per- formance as compared to post-norm, and there are no stability benefits of pre-norm beyond the 100B scale. Therefore, GLM- 130B [33] used deep-norm which is a variant of post-norm for better downstream task performance after fine-tuning. Positional Encoding: Like other building blocks of the model,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='23'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='positional encoding also affects the performance and training stability of LLMs. BLOOM [13] finds ALiBi outperforms learned and rotary positional encodings. Contrary to this, GLM-130B [33] identifies rotary positional encoding as being better than ALiBi. So, there is no conclusion in the literature about positional encodings yet. Parallel Attention: In this type of attention, feed-forward and attention layers are parallel to each other rather than sequen- tial in a transformer block. It has'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='In this type of attention, feed-forward and attention layers are parallel to each other rather than sequen- tial in a transformer block. It has been shown to reduce train- ing time by 15%. There is no evidence of performance drop due to this change in the literature and it is used by the models PaLM [15], GPT-NeoX [118], and CodeGen [140]. Multi-Query Attention It has shared key and value attention heads in a transformer block while query attention heads are projected as usual. This reduces'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Attention It has shared key and value attention heads in a transformer block while query attention heads are projected as usual. This reduces memory usage and speeds up sampling in autoregressive decoding. No performance degrada- tion has been observed with this change and it makes the train- ing efficient allowing larger batch sizes. Multi-query attention is used in [15, 142]. Mixture of Experts: This type of architecture enables eas- ily scaling models to trillions of parameters [92, 91].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='attention is used in [15, 142]. Mixture of Experts: This type of architecture enables eas- ily scaling models to trillions of parameters [92, 91]. Only a few experts are activated during the computation making them compute-efficient. The performance of MoE models is better than dense models for the same amount of data and requires less computation during fine-tuning to achieve performance similar to dense models as discussed in [91]. MoE architectures are less prone to catastrophic forgetting,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='during fine-tuning to achieve performance similar to dense models as discussed in [91]. MoE architectures are less prone to catastrophic forgetting, therefore are more suited for continual learning [92]. Extracting smaller sub-models for downstream tasks is possible without losing any performance, making MoE architecture hardware-friendly [92]. Sparse vs Dense Activated: GPT-3 [6] uses sparse transform- ers [67] whereas GLaM [91] and PanGu-(cid:80) [92] use MoE [121] architectures to lower'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Sparse vs Dense Activated: GPT-3 [6] uses sparse transform- ers [67] whereas GLaM [91] and PanGu-(cid:80) [92] use MoE [121] architectures to lower computational costs and increase the model size and capacity. According to the literature, sparse modules do not degrade the model’s performance [67]. How- ever, more experiments are required to verify this statement.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='3.8.2. Training Strategies'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Training models at a huge scale require tricks to reduce train- ing costs, avoid loss divergence, and achieve better perfor- mance. We summarize and discuss some of these key tricks used in different LLMs. Mixed Precision: It is a famous method for LLMs to reduce memory usage and improve training efficiency. In mixed pre- cision, forward and backward passes are performed in FP16 format whereas optimizer states and master weights are kept in FP32 format [120]. A drawback associated with this'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='passes are performed in FP16 format whereas optimizer states and master weights are kept in FP32 format [120]. A drawback associated with this for- mat change is training instability due to a smaller value range resulting in loss spikes [33]. An alternative to FP16 is BF16 which has a comparatively larger range and performs precision- sensitive operations like gradient accumulation and softmax in FP32 [13]. BF16 has better performance and training stability but uses more memory and is supported'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='like gradient accumulation and softmax in FP32 [13]. BF16 has better performance and training stability but uses more memory and is supported on specific hardware, for example, A100 GPUs. Therefore, its adoption in LLMs is limited. Training Instability: Loss divergence or spiking is a common issue in LLMs that occurs multiple times during training. This'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='happens in the presence of gradient clipping [15]. To mitigate this problem, many approaches suggest restarting training from an earlier checkpoint [15, 33, 91], skipping 200-500 earlier data batches at the point of divergence in [15] and re-shuffling batches in [91]. The embedding layer gradient shrink proves to further stabilize the training as its gradient norm is significantly larger than the other layers [33]. Another suggestion to improve training stability for larger models is not to use'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='gradient norm is significantly larger than the other layers [33]. Another suggestion to improve training stability for larger models is not to use biases in dense and norm layers as in [15]. Weight Initialization: It plays a significant role in model con- vergence and training stability. GPT-NeoX [118] initializes feed-forward layers before residuals with 2 as in [153] and √ d L other layers with the small initialization scheme [298]. This avoids activations growing exponentially with'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='residuals with 2 as in [153] and √ d L other layers with the small initialization scheme [298]. This avoids activations growing exponentially with increasing depth. MT-NLG [117] found higher variance for weight initialization leads to unstable training, hence validating small initialization scheme [298]. Various models perform random weight initial- ization which can cause bad initialization, Galactica [148] sug- gests a longer warmup to negate the effect. Learning Rate: A suitable learning'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='ization which can cause bad initialization, Galactica [148] sug- gests a longer warmup to negate the effect. Learning Rate: A suitable learning rate is important for sta- ble training. It is suggested to use a lower value [13, 15, 124] with warmup and decay (cosine or linear). Usually, the learn- ing rate is within the range 1e−4 to 8e−4. Moreover, MT-NLG (530B) [117] and GPT-NeoX (20B) [118] suggest interpolat- ing learning rates based on the model size using the GPT-3 [6] models ranging'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='MT-NLG (530B) [117] and GPT-NeoX (20B) [118] suggest interpolat- ing learning rates based on the model size using the GPT-3 [6] models ranging between 13B and 175B. This avoids tuning the learning rate hyperparameter. Training Parallelism: 3D parallelism, a combination of data, pipeline, and tensor parallelism, is the most utilized training parallelism approach in LLMs [33, 15, 14, 13, 117, 115, 112]. In addition to 3D parallelism, BLOOM [13] uses a zero op- timizer [37] to shard optimizer'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='approach in LLMs [33, 15, 14, 13, 117, 115, 112]. In addition to 3D parallelism, BLOOM [13] uses a zero op- timizer [37] to shard optimizer states. PanGu-α [108] and PanGu-Σ [92] go beyond 3D parallelism and apply 5D paral- lelism which additionally contains optimizer parallelism and rematerialization. Mode Switching: It adds task-related tokens at the beginning of the text during training. These tokens refer to the natural language understanding and natural language generation tasks which are'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='the beginning of the text during training. These tokens refer to the natural language understanding and natural language generation tasks which are shown to improve downstream task performance in [125, 124, 122]. During fine-tuning and inference, tokens are appended based on the downstream tasks. Controllable Text Generation: Generating credible and con- trolled text from a pre-trained model is challenging. GPT-3 [6] and other LLMs use in-context learning to control generated text. While'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='and con- trolled text from a pre-trained model is challenging. GPT-3 [6] and other LLMs use in-context learning to control generated text. While in-context learning helps in controlling the gener- ated text, ERNIE 3.0 Titan [35] suggests using adversarial loss to rank its generated text for credibility and soft prompts such as genre, topic, keywords, sentiment, and length for better control on generated text.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='3.8.3. Supervised Models vs Generalized Models\\n\\nAlthough generalized models are capable of performing di- verse tasks with good performance they have not yet outper- formed models trained in supervised settings. The supervised trained models are still state-of-the-art in various NLP tasks by a large margin as shown in [6, 15, 18].\\n\\n24\\n\\n3.8.4. Zero-Shot vs Few-Shot'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='LLMs perform well in zero-shot and few-shot settings. But the performance difference between zero-shot and few-shot is large for pre-trained models [6, 15], naming LLMs as meta- learners [6]. LLMs zero-shot evaluations underperform unsu- pervised methods in neural machine translation [6]. The liter- ature shows pre-training is not enough for good zero-shot per- formance [15, 16]. To improve the zero-shot performance the literature suggests using instruction fine-tuning that improves the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='good zero-shot per- formance [15, 16]. To improve the zero-shot performance the literature suggests using instruction fine-tuning that improves the zero-shot performance significantly and outperforms base- lines. Instruction fine-tuning has also been shown to improve zero-shot generalization to unseen tasks. Another model, Flan- PaLM [16], unlocks zero-shot reasoning with CoT training.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='3.8.5. Encoder vs Decoder vs Encoder-Decoder'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Traditionally, these architectures perform well for different tasks, for example, encoder-only for NLU tasks, decoder-only for NLG, and encoder-decoder for sequence2sequence model- ing. Encoder-only models are famous for smaller models such as Bert [7], RoBERTa [299], etc., whereas LLMs are either decoder-only [6, 118, 13] or encoder-decoder [10, 11, 122]. While decoder-only models are good at NLG tasks, various LLMs, PaLM [15], OPT [14], GPT-3 [6], BLOOM [13], LLaMA [156], are decoder-only'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='11, 122]. While decoder-only models are good at NLG tasks, various LLMs, PaLM [15], OPT [14], GPT-3 [6], BLOOM [13], LLaMA [156], are decoder-only models with significant per- In contradic- formance gains on both NLU and NLG tasks. tion to this, T5 [10] and UL2 [125] identify encoder-decoder models out-performing decoder-only models. In another study, PaLM [15] finds increasing the size of decoder-only models can reduce the performance gap between decoder-only and encoder-decoder architectures.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='PaLM [15] finds increasing the size of decoder-only models can reduce the performance gap between decoder-only and encoder-decoder architectures. Although decoder-only architectures have become a trend for LLMs, many recently proposed approaches [125, 122] use mode-switching tokens in text with encoder-decoder architec- tures to enable task-specific modes. Similarly, CodeT5+ [34] uses an encoder-decoder architecture with multiple training ob- jectives for different tasks, activating the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='modes. Similarly, CodeT5+ [34] uses an encoder-decoder architecture with multiple training ob- jectives for different tasks, activating the encoder, decoder, or both according to the tasks. These variations in architecture and training objectives allow a model to perform well in differ- ent settings. Because of this dynamic configuration, the future of LLMs can be attributed to encoder-decoder architectures.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='4. Model Configurations'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='We provide different statistics of pre-trained and instruction- tuned models in this section. This includes information such as publication venue, license type, model creators, steps trained, parallelism, etc in Table 3 and Table 4. Architecture details of pre-trained LLMs are available in Table 5. Providing these details for instruction-tuned models is unnecessary because it fine-tunes pre-trained models for instruction datasets. Hence, architectural details are the same as the baselines.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='models is unnecessary because it fine-tunes pre-trained models for instruction datasets. Hence, architectural details are the same as the baselines. Moreover, optimization settings for various LLMs are available in Table 6 and Table 7. We do not include details on precision, warmup, and weight decay in Table 7. These details are not as important as others to mention for instruction-tuned models, and are not provided by the papers.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Table 3: Summary of pre-trained LLMs (>10B). Only the LLMs discussed individually in the previous sections are summarized. “Data/Tokens” is the model’s pre-training data, which is either the number of tokens or data size. “Data Cleaning” indicates whether data cleaning is performed or not. This includes heuristics (Heur), deduplication (Dedup), quality filtering (QF), and privacy filtering (PF), “Cost” is the calculated training cost obtained by multiplying the GPUs/TPUs hourly rate with the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='quality filtering (QF), and privacy filtering (PF), “Cost” is the calculated training cost obtained by multiplying the GPUs/TPUs hourly rate with the number of GPUs and the training time. The actual cost may vary due to many reasons such as using in-house GPUs or getting a discounted rate, re-training, number of employees working on the problem, etc. “Training Parallelism” indicates distributed training using data parallelism (D), tensor parallelism (T), pipeline parallelism (P), context'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='etc. “Training Parallelism” indicates distributed training using data parallelism (D), tensor parallelism (T), pipeline parallelism (P), context parallelism (C), model parallelism (M), optimizer parallelism (OP), and rematerialization (R), where for “Library” column, “DS” is a short form for Deep Speed. In column “Commercial Use”, we assumed a model is for non-commercial purposes if its license is unavailable.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Models\\n\\nPublication Venue\\n\\nLicense Type\\n\\nModel\\n\\nCreators Purpose'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content=\"General Google T5 [10] General OpenAI GPT-3 [6] General Google mT5 [11] PanGu-α [108] Huawei General Tsinghua General CPM-2 [12] Coding OpenAI Codex [141] General Baidu ERNIE 3.0 [110] General AI21 Jurassic-1 [112] General Naver HyperCLOVA [114] General - Yuan 1.0 [115] General Google Gopher [116] ERNIE 3.0 Titan [35] General Baidu GPT-NeoX-20B [118] BigScience'22 Apache-2.0 EleutherAI General OPT [14] General RAIL-1.0 BigScience General BLOOM [13] Science Apache-2.0 Galactica [148] General -\"),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content=\"BigScience'22 Apache-2.0 EleutherAI General OPT [14] General RAIL-1.0 BigScience General BLOOM [13] Science Apache-2.0 Galactica [148] General - GLaM [91] LaMDA [150] Dialog - Apache-v2.0 MS.+Nvidia General MT-NLG [117] Coding Apache-v2.0 Google AlphaCode [142] General Google Chinchilla [96] PaLM [15] General Google Apache v2.0 Amazon General AlexaTM [122] General U-PaLM [124] - Apache-2.0 UL2 [125] General Apache-2.0 Multiple General GLM [33] Apache-2.0 Salesforce Coding CodeGen [140] General\"),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content=\"[122] General U-PaLM [124] - Apache-2.0 UL2 [125] General Apache-2.0 Multiple General GLM [33] Apache-2.0 Salesforce Coding CodeGen [140] General LLaMA [127] PanGuΣ [92] General 1.085T BloombergGPT [151] Xuan Yuan 2.0 [152] CodeT5+ [34] StarCoder [147] LLaMA-2 [21] PaLM-2 [123] LLaMA-3.1 [130] Mixtral 8x22B [131] web'24 Snowflake Arctic [132] web'24 Nemotron-4 340B [137]web'24 DeepSeek [138] DeepSeek-v2 [139]\"),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content=\"JMLR'20 NeurIPS'20 NAACL'21 arXiv'21 AI Open'21 arXiv'21 arXiv'21 White-Paper'21 Apache-2.0 EMNLP'21 arXiv'21 arXiv'21 arXiv'21\\n\\nApache-2.0 - Apache-2.0 Apache-2.0 MIT - -\\n\\nApache-2.0 - -\\n\\narXiv'22 arXiv'22 arXiv'22 ICML'22 arXiv'22 arXiv'22 Science'22 arXiv'22 arXiv'22 arXiv'22 arXiv'22 ICLR'23 ICLR'23 ICLR'23 arXiv'23 arXiv'23 arXiv23 arXiv23 arXiv'23 arXiv'23 arXiv'23 arXiv'23 arXiv'24\\n\\nMIT\\n\\nMeta\\n\\nMeta Google Google\\n\\n-\\n\\nGoogle Google\\n\\n- -\\n\\nMeta Huawei\"),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content=\"MIT\\n\\nMeta\\n\\nMeta Google Google\\n\\n-\\n\\nGoogle Google\\n\\n- -\\n\\nMeta Huawei\\n\\nBloomberg Finance RAIL-1.0 Du Xiaoman Finance Salesforce Coding Coding OpenRAIL-M BigCode General LLaMA-2.0 General - LLaMA-3.0 General Apache-2.0 Mistral AI General Apache-2.0 Snowflake General General DeepSeek General DeepSeek General\\n\\nBSD-3\\n\\nMeta Google Meta\\n\\nNvidia MIT MIT\\n\\nNvidia\\n\\narXiv'24 arXiv'24\\n\\nNo. of Params\"),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content=\"BSD-3\\n\\nMeta Google Meta\\n\\nNvidia MIT MIT\\n\\nNvidia\\n\\narXiv'24 arXiv'24\\n\\nNo. of Params\\n\\n11B 175B 13B 200B 198B 12B 10B 178B 82B 245B 280B 260B 20B 175B 176B 120B 1.2T 137B 530B 41B 70B 540B 20B 540B 20B 130B 16B 65B\\n\\n50B 176B 16B 15.5B 70B - 405B 141B 480B 340B 67B 67B\\n\\nCommercial Use ✓ × ✓ ✓ ✓ × × ✓ × ✓ × × ✓ ✓ ✓ × × × × ✓ × × × × ✓ × ✓ × × × ✓ ✓ ✓ ✓ × ✓ ✓ ✓ ✓ ✓ ✓\\n\\nData/ Tokens\\n\\nSteps Trained\\n\\nData Cleaning Heur+Dedup Dedup+QF -\"),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Data/ Tokens\\n\\nSteps Trained\\n\\nData Cleaning Heur+Dedup Dedup+QF -\\n\\n1M - 1M 260k 1M 2.6TB 100B 375B Heur+Dedup 300B 300B Clf+Dedup+PF 180B Heur+Clf+Dedup QF+Dedup 300B 300B Heur+Dedup\\n\\n1T 300B 1T\\n\\n1.1TB Heur+Dedup\\n\\nDedup Heur\\n\\n120k∗ - - 26k∗ - -\\n\\n\\n\\n150k 825GB 180B 150k 366B - 225k 106B 600k∗ 600B 3M 2.81T 270B 967B Heur+Dedup QF+Dedup 1.4T Heur 780B Filtered 1.1T - - - 1T 400B - 577B Heur+Dedup 1.4T Clf+Heur+Dedup 329B 569B 366B 51.5B\\n\\nNone Dedup Dedup+PR Dedup Clf Filtered -'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='None Dedup Dedup+PR Dedup Clf Filtered -\\n\\n205k - 255k 500k 20k 2M - 650k 350k - 139k - 110k 250k 500k -\\n\\nDedup Filtered Dedup\\n\\n1T Dedup+QF+PF 2T Minimal Filtering Ddedup+PF+QF - Dedup+QF - - - Dedup+QF QF\\n\\n1.2M 15T\\n\\n- - - -\\n\\n3.5T 9T 2T 8.1T\\n\\nNo. of Processing Units\\n\\n1024 - - 2048 - - 384 800 1024 2128 4096 - 96 992 384 128 1024 1024 4480 - - 6144 128 512 512 768 - 2048 512 512 - 16 512 - - 16k - - 6144 - -\\n\\nProcessing Unit Type\\n\\nTraining Time'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Processing Unit Type\\n\\nTraining Time\\n\\nTPU v3 - V100 - - - Ascend 910 - - - - - V100 - GPU 321h A100 - GPU 920h TPU v3 - Ascend 910 - 40G A100 - 80G A100 2520h 80G A100 - 80GB A100 - TPU v4 1384h TPU v3 - 80G A100 - TPU v4 - TPUv4 - TPU v4 2880h A100 120h TPU v4 - TPU v4 1440h 40G A100 - TPU v4 80G A100 504h Ascend 910 2400h 1272h 40G A100 - 80GB A100 - 40G A100 624h 80G A100 1.7Mh 80G A100 - - 80G H100 30.84Mh\\n\\n80G H100 - H800\\n\\n- - 300.6Kh 172.8Kh\\n\\nCalculated Train. Cost'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='80G H100 - H800\\n\\n- - 300.6Kh 172.8Kh\\n\\nCalculated Train. Cost\\n\\n- - - - - - - 1.32 Mil - 13.19 Mil - - - 3.87 Mil - - 4.96 Mil - - - - 1.47 Mil 0.25 Mil - 3.37 Mil - 4.12 Mil - 1.97 Mil - - 1.28 Mil - - - - - - - -\\n\\nTraining Parallelism D+M M - D+OP+P+O+R D+M - M∗ D+M+P M D+T+P D+M D+M+P+D* M D+T D+T+P - M D+M D+T+P M - D+M M - M M D+M D+M D+OP+P+O+R M P - D+T+P - - D+T+P+C - T+P D+T+P D+T+P D+P\\n\\nLibrary'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Library\\n\\nMesh TensorFlow - - MindSpore JAXFormer - PaddlePaddle Megatron+DS Megatron - JAX+Haiku PaddlePaddle Megatron+DS+PyTorch Megatron Megatron+DS Metaseq GSPMD Lingvo Megatron+DS JAX+Haiku JAX+Haiku JAX+T5X DS - JAX+T5X - JAXFormer xFormers MindSpore PyTorch DS DS Megatron-LM - - PyTorch - DS - DS HAI-LLM\\n\\nTable 4: Summary of instruction tuned LLMs (>10B). All abbreviations are the same as Table 3. Entries in “Data/Tokens” starting with “S-” represent the number of training samples.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content=\"Models\\n\\nPublication Venue\\n\\nLicense Type\\n\\nModel Creators\\n\\nPurpose\\n\\nNo. of Params\\n\\nCommercial Use\\n\\nPre-trained Models\\n\\nSteps Trained\\n\\nData/ Tokens\\n\\nNo. of Processing Units\\n\\nProcessing Unit Type\\n\\nTrain. Time\\n\\nCalculated Train. Cost\\n\\nTrain.\\n\\nParallelism Library\\n\\nWebGPT [166] T0 [17] Tk-Instruct [18] OPT-IML [97] Flan-U-PaLM [16] ICLR'22 Apache-2.0 ACL'23 mT0 [154] Sparrow [167] arXiv'22 WizardCoder [164] arXiv'23 Alpaca [158] Vicuna [159] LIMA [185] Koala [300]\"),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content=\"General 175B arXiv'21 11B ICLR'22 Apache-2.0 BigScience General General EMNLP'22 MIT 11B General 175B arXiv'22 General 540B 13B 70B 15B 13B 13B 65B 13B\\n\\n\\n\\nOpenAI\\n\\nAI2+ Meta Google Apache-2.0 HuggingFace+ General Dialog Google Coding Apache-2.0 HK Bapt. General Stanford Github'23 Apache-2.0 General LMSYS Github'23 Apache-2.0 Meta+ arXiv'23 General Github'23 Apache-2.0 UC-Berkley General\\n\\n\\n\\n\\n\\n\\n\\n× ✓ ✓ × ✓ ✓ × × ✓ ✓ - ×\"),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='× ✓ ✓ × ✓ ✓ × × ✓ ✓ - ×\\n\\nGPT-3 - T5 250B T5 - OPT 2B U-PaLM - mT5 - Chinchilla - S-78k StarCoder LLaMA 3-Epoch S-52k LLaMA 3-Epoch S-125k LLaMA 15-Epoch S-1000 LLaMA 2-Epoch S-472k\\n\\n- 1000 8k 30k - - 200\\n\\n512 256 128 512 - 64 - 8 - - 8\\n\\nTPU v3 TPU v3 40G A100 TPU v4 - TPU v3 - 80G A100 - - A100\\n\\n270h 4h - - - - - 3h - - 6h\\n\\n0.48 Mil 0.0036 Mil - - - - - 600 - - 100\\n\\n- - D+T - - M - FSDP FSDP - -\\n\\n- Google T5 Megatron JAX+T5X - - - PyTorch PyTorch - JAX/FLAX\\n\\n5. Datasets and Evaluation'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='- - D+T - - M - FSDP FSDP - -\\n\\n- Google T5 Megatron JAX+T5X - - - PyTorch PyTorch - JAX/FLAX\\n\\n5. Datasets and Evaluation\\n\\n5.1. Training Datasets\\n\\nGenerating training and evaluation datasets is expensive be- cause of the large-scale data demand of LLMs. Hence, datasets for training and benchmarking these models are topics of key importance. A summary of datasets commonly used by LLMs is provided next.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='The performance of LLMs largely depends on the training data’s quality, size, and diversity. Preparing training datasets of high quality at a large scale is laborious. Researchers have suggested various pre-training and fine-tuning datasets to en- hance LLMs capabilities. We summarize these efforts in Ta- ble 8. While numerous training datasets are available in the literature, we cover the most widely used ones in our summary.\\n\\n25'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='25\\n\\nTable 5: Architecture details of LLMs. Here, “PE” is the positional embedding, “nL” is the number of layers, “nH” is the number of attention heads, “HS” is the size of hidden states.\\n\\nModels\\n\\nType\\n\\nTraining Objective\\n\\nAttention\\n\\nVocab\\n\\nTokenizer\\n\\nNorm\\n\\nPE\\n\\nActivation Bias\\n\\nnL\\n\\nnH'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='T5 (11B) GPT3 (175B) mT5 (13B) PanGu-α (200B) CPM-2 (198B) Codex (12B) ERNIE 3.0 (10B) Jurassic-1 (178B) HyperCLOVA (82B) Yuan 1.0 (245B) Gopher (280B) ERNIE 3.0 Titan (260B) GPT-NeoX-20B OPT (175B) BLOOM (176B) Galactica (120B) GLaM (1.2T) LaMDA (137B) MT-NLG (530B) AlphaCode (41B) Chinchilla (70B) PaLM (540B) AlexaTM (20B) Sparrow (70B) U-PaLM (540B) UL2 (20B) GLM (130B) CodeGen (16B) LLaMA (65B) PanGu-Σ (1085B) BloombergGPT (50B) Xuan Yuan 2.0 (176B) CodeT5+ (16B) StarCoder (15.5B) LLaMA-2'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='(540B) UL2 (20B) GLM (130B) CodeGen (16B) LLaMA (65B) PanGu-Σ (1085B) BloombergGPT (50B) Xuan Yuan 2.0 (176B) CodeT5+ (16B) StarCoder (15.5B) LLaMA-2 (70B) PaLM-2 LLaMA-3.1 (405B) Nemotron-4 (340B) DeepSeek (67B) DeepSeek-v2 (67B)'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Enc-Dec Causal-Dec Enc-Dec Causal-Dec Enc-Dec Causal-Dec Causal-Dec Causal-Dec Causal-Dec Causal-Dec Causal-Dec Causal-Dec Causal-Dec Causal-Dec Causal-Dec Causal-Dec MoE-Dec Causal-Dec Causal-Dec Enc-Dec Causal-Dec Causal-Dec Enc-Dec Causal-Dec Non-Causal-Dec Enc-Dec Non-Causal-Dec Causal-Dec Causal-Dec Causal-Dec Causal-Dec Causal-Dec Enc-Dec Causal-Dec Causal-Dec - Causal-Dec Causal-Dec Causal-Dec MoE-Dec'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Span Corruption Next Token Span Corruption Next Token Span Corruption Next Token Next Token Next Token Next Token Next Token Next Token Next Token Next Token Next Token Next Token Next Token Next Token Next Token Next Token Next Token Next Token Next Token Denoising Pref.&Rule RM'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Standard Dense+Sparse Standard Standard Standard Standard Standard Standard Dense+Sparse Standard Standard Standard Parallel Standard Standard Standard Standard Standard Standard Multi-query Standard Parallel+Multi-query Standard -\\n\\nMoD Parallel+Multi-query MoD AR Blank Infilling Next Token Next Token Next Token Next Token Next Token SC+NT+Cont.+Match FIM Next Token MoD Next Token Next Token Next Token Next Token Multi-Head Latent'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Standard Standard Parallel Standard Standard Standard Self Standard Multi-query Grouped-query Parallel Grouped-query Standard Grouped-query\\n\\n32k - 250k 40k 250k - - 256k - - 32k - 50k - 250k 50k 256k 32k 50k 8k 32k 256k 150k 32k 256k 32k 130k - 32k - 131k 250k - 49k 32k - 128k 256k 100k 100k'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='SentencePiece - SentencePiece BPE SentencePiece BPE+ WordPiece SentencePiece∗ BPE* - SentencePiece WordPiece BPE BPE BPE BPE+custom SentencePiece BPE BPE SentencePiece SentencePiece-NFKC SentencePiece SentencePiece SentencePiece-NFKC SentencePiece SentencePiece SentencePiece BPE BPE BPE Unigram BPE Code-Specific BPE BPE - BPE SentencePiece BBPE BBPE'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Pre-RMS Layer Pre-RMS Layer Pre-RMS Pre-Layer Post-Layer Pre-Layer Pre-Layer - Pre-RMS Post-Layer Layer - Layer Layer Layer Layer Pre-Layer - Pre-RMS Layer Pre-Layer Pre-RMS Layer - Deep Layer Pre-RMS Fused Layer Layer Layer - - Pre-RMS - Pre-RMS - Pre-RMS Pre-RMS'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Relative Learned Relative - Relative Learned Relative Learned Learned - Relative Relative Rotary - ALiBi Learned Relative Relative Learned - Relative RoPE Learned Relative RoPE - RoPE RoPE RoPE - ALiBi ALiBi - Learned RoPE - RoPE RoPE RoPE RoPE\\n\\nReLU GeLU ReLU - ReLU GeLU GeLU GeLU GeLU - GeLU GeLU GeLU ReLU GeLU GeLU GeLU GeGLU GeLU - GeLU SwiGLU GeLU GeLU SwiGLU - GeGLU - SwiGLU FastGeLU GeLU GeLU - - SwiGLUE - SwiGLU ReLU SwiGLU SwiGLU'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='× 24 ✓ 96 - - - 64 - 24 - 96 - 48 ✓ 76 - 64 - 76 ✓ 80 - 48 ✓ 44 ✓ 96 ✓ 70 × 96 ✓ 64 - 64 ✓ 105 - 64 ✓ 80 × 118 ✓ 78 ✓ 16∗ × 118 64 - ✓ 70 34 - 80 - 40 - ✓ 70 ✓ 70 - - 40 - - - - - 126 - × 96 95 - 60 -\\n\\n128 96 - 128 64 96 64 96 80 - 128 192 64 96 112 80 128 128 128 128 64 48 32 64 48 16 96 24 64 40 40 112 - 48 - - 128 96 64 128\\n\\n5.2. Evaluation Datasets and Tasks'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='The evaluation of LLMs is important in gauging their profi- ciency and limitations. This process measures the model’s abil- ity to comprehend, generate, and interact with human language across a spectrum of tasks. Evaluating a language model (LM) is divided into two broader categories: 1) natural language un- derstanding (NLU) and 2) natural language generation (NLG). It is emphasized that tasks in NLU and NLG are softly catego- rized and are often used interchangeably in the literature.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='language generation (NLG). It is emphasized that tasks in NLU and NLG are softly catego- rized and are often used interchangeably in the literature. Natural Language Understanding: It measures the language understanding capacity of LMs. It encompasses multiple tasks, including sentiment analysis, text classification, natural lan- guage inference (NLI), question answering (QA), common- sense reasoning (CR), mathematical reasoning (MR), reading comprehension (RC), etc. Natural Language'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='(NLI), question answering (QA), common- sense reasoning (CR), mathematical reasoning (MR), reading comprehension (RC), etc. Natural Language Generation: It assesses the language gener- ation capabilities of LLMs by understanding the provided input context. It includes tasks such as summarization, sentence com- pletion, machine translation (MT), dialogue generation, etc. Numerous datasets are proposed for each task, evaluating LLMs against different characteristics. To provide an overview of'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='dialogue generation, etc. Numerous datasets are proposed for each task, evaluating LLMs against different characteristics. To provide an overview of evaluation datasets, we briefly discuss a few famous datasets within each category and offer a comprehensive list of datasets in Table 9. Moreover, we show a detailed overview of the train- ing datasets and evaluation tasks and benchmarks used by vari-'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='ous pre-trained LLMs in Table 10 and fine-tuned LLMs in Ta- ble 11. We also compare the top-performing LLMs in various NLP tasks in Table 12.\\n\\n5.2.1. Multi-task\\n\\nMMLU [307]: A benchmark that measures the knowledge acquired by models during pretraining and evaluates models in zero-shot and few-shot settings across 57 subjects, testing both world knowledge and problem-solving ability.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='SuperGLUE [2]: A more challenging and diverse successor to the GLUE [309] benchmark, SuperGLUE includes a variety of language understanding tasks, such as question answering, natural language inference, and co-reference resolution. It is designed to provide a rigorous test of language understanding and requires significant progress in areas like sample-efficient, transfer, multi-task, and unsupervised or self-supervised learn- ing.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='BIG-bench [308]: The BIG-bench (Behavior of Intelligent Generative Models Benchmark) is a large-scale benchmark de- signed to test the abilities of LLMs across a wide range of tasks, including reasoning, creativity, ethics, and understanding of specific domains.\\n\\nGLUE [309]: The General Language Understanding Evalua- tion (GLUE) benchmark is a collection of resources for train- ing, evaluating, and analyzing natural language understanding\\n\\n26\\n\\nHS'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='26\\n\\nHS\\n\\n1024 12288 - 16384 - 12288 4096 13824 10240 16384 16384 12288 - - 14336 10240 32768 8192 20480 6144 8192 18432 4096 8192 18432 4096 12288 - 8192 5120 7680 14336 - 6144 - - 16384 18432 8192 5120\\n\\nTable 6: Summary of optimization settings used for pre-trained LLMs. The values for weight decay, gradient clipping, and dropout are 0.1, 1.0, and 0.1, respectively, for most of the LLMs.\\n\\nModels\\n\\nBatch Size\\n\\nSequence Length\\n\\nLR\\n\\nWarmup\\n\\nLR Decay\\n\\nOptimizers\\n\\nPrecision\\n\\nWeight Grad'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Models\\n\\nBatch Size\\n\\nSequence Length\\n\\nLR\\n\\nWarmup\\n\\nLR Decay\\n\\nOptimizers\\n\\nPrecision\\n\\nWeight Grad\\n\\nAdaFactorAdam AdamWFP16 BF16 Mixed Decay\\n\\nClip Dropout'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='T5 (11B) GPT3 (175B) mT5 (13B) PanGu-α (200B) CPM-2 (198B) Codex (12B) ERNIE 3.0 (12B) Jurassic-1 (178B) HyperCLOVA (82B) Yuan 1.0 (245B) Gopher (280B) ERNIE 3.0 Titan (260B) GPT-NeoX-20B OPT (175B) BLOOM (176B) Galactica (120B) GLaM (1.2T) LaMDA (137B) MT-NLG (530B) AlphaCode (41B) Chinchilla (70B) PaLM (540B) AlexaTM (20B) U-PaLM (540B) UL2 (20B) GLM (130B) CodeGen (16B) LLaMA (65B) PanGu-Σ (1.085T) BloombergGPT (50B) Xuan Yuan 2.0 (176B) CodeT5+ (16B) StarCoder (15.5B) LLaMA-2 (70B)'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='UL2 (20B) GLM (130B) CodeGen (16B) LLaMA (65B) PanGu-Σ (1.085T) BloombergGPT (50B) Xuan Yuan 2.0 (176B) CodeT5+ (16B) StarCoder (15.5B) LLaMA-2 (70B) LLaMA-3.1 (405B) Nemotron-4 (340B) DeepSeek (67B) DeepSeek-v2 (67B)'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='211 32K 1024 - 1024 - 6144 3.2M 1024 <10M 3M - 1538 2M 2048 2M 1M 256K 1920 2048 1.5M 2048 2M 32 1024 4224 2M 4M Tokens 512 2048 2048 2048 512 4M Tokens 16M 2304 4608 9216\\n\\n512 - 1024 1024 1024 - 512 2048 - 2048 2048 512 2048 2048 2048 2048 1024 - 2048 1536+768 2048 2048 1024 2048 1024 2048 2048 2048 1024 2048 2048 1024 8k 4k 8192 4096 4096 4k'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='0.01 6e-5 0.01 2e-5 0.001 6e-5 1e-4 6e-5 6e-5 1.6e-4 4e-5 1e-4 0.97e-5 1.2e-4 6e-5 7e-6 0.01 - 5e-5 1e-4 1e-4 0.01 1e-4 1e-4 - 8e-5 5e-5 1.5e-4 2e-5 6e-5 6e-5 2e-4 3e-4 1.5e-4 8e-5 - 3.2e-4 2.4e-4\\n\\n× ✓ - - - ✓ ✓ ✓ - ✓ ✓ ✓ ✓ - ✓ ✓ - - ✓ ✓ ✓ - - - - ✓ ✓ ✓ ✓ ✓ ✓ - ✓ ✓ ✓ - ✓ ✓\\n\\ninverse square root ✓\\n\\ncosine\\n\\ninverse square root ✓ - ✓\\n\\n- cosine linear cosine cosine cosine decay to 10% cosine decay to 10% linear cosine linear cosine linear decay to 10% inverse square root ✓ -'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='- cosine linear cosine cosine cosine decay to 10% cosine decay to 10% linear cosine linear cosine linear decay to 10% inverse square root ✓ -\\n\\ncosine decay to 10% cosine decay to 10% cosine decay to 10% inverse square root ✓ linear decay to 5% cosine inverse square root cosine cosine cosine decay to 10% - cosine cosine linear cosine cosine linear+cosine linear cosine step-decay ✓ -\\n\\n\\n\\n✓\\n\\n\\n\\n✓ ✓ ✓\\n\\n✓ ✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n\\n\\n\\n\\n✓\\n\\n✓ ✓\\n\\n✓\\n\\n\\n\\n✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓ - ✓ ✓\\n\\n✓ - - - ✓ - ✓ - - ✓ ✓ ✓'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='✓\\n\\n\\n\\n✓ ✓ ✓\\n\\n✓ ✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n\\n\\n\\n\\n✓\\n\\n✓ ✓\\n\\n✓\\n\\n\\n\\n✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓ - ✓ ✓\\n\\n✓ - - - ✓ - ✓ - - ✓ ✓ ✓\\n\\n\\n\\n\\n\\n\\n\\n- ✓ - -\\n\\n✓\\n\\n\\n\\n\\n\\n\\n\\n✓ -\\n\\n- -\\n\\n\\n\\n\\n\\n- ✓\\n\\n-\\n\\n✓ -\\n\\nFP32 + ✓ -\\n\\n✓ ✓ ✓ - ✓ - -\\n\\n\\n\\n-\\n\\n-\\n\\n- ✓ ✓\\n\\n✓\\n\\n✓ ✓ ✓ ✓ ✓ -\\n\\n\\n\\n✓ - - - ✓ ✓ ✓ ✓ ✓ - ✓ ✓ ✓ ✓ ✓ - - ✓ ✓ - ✓ ✓ - × ✓ ✓ ✓ - ✓ ✓ ✓ ✓ ✓ - - ✓ ✓\\n\\n✓ - - - - - ✓ - - ✓ ✓ ✓ ✓ ✓ ✓ ✓ - ✓ ✓ - ✓ - - - ✓ ✓ ✓ - ✓ ✓ - - ✓ - - ✓ ✓\\n\\n✓ - ✓ - ✓ - - - - - - - × ✓ × ✓ × - - - - × ✓ - - ✓ - - - × - - - - - × - -'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='✓ - ✓ - ✓ - - - - - - - × ✓ × ✓ × - - - - × ✓ - - ✓ - - - × - - - - - × - -\\n\\nTable 7: Summary of optimization settings used for instruction-tuned LLMs. Values for gradient clipping and dropout are the same as the pre-trained models, while no model uses weight decay for instruction tuning.\\n\\nModels\\n\\nBatch Size\\n\\nSequence Length\\n\\nLR Warmup\\n\\nLR_Decay\\n\\nOptimizers\\n\\nGrad\\n\\nAdaFactor Adam AdamW Clip Dropout'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Models\\n\\nBatch Size\\n\\nSequence Length\\n\\nLR Warmup\\n\\nLR_Decay\\n\\nOptimizers\\n\\nGrad\\n\\nAdaFactor Adam AdamW Clip Dropout\\n\\nWebGPT (175B) T0 (11B) Tk-Instruct (11B) OPT-IML (175B) Flan-U-PaLM (540B) Sparrow (70B) WizardCoder (15B) Alpaca (13B) Vicuna (13B) LIMA (65B)\\n\\nBC:512, RM:32 1024 1024 128 32 RM: 8+16, RL:16 512 128 128 32\\n\\n1280 - 2048 - - 2048 512 -2048 2048\\n\\n6e-5 1e-3 1e-5 5e-5 1e-3 2e-6 2e-5 1e-5 2e-5 1e-5\\n\\n- - × - ✓ ✓ ✓ ✓ ×'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='1280 - 2048 - - 2048 512 -2048 2048\\n\\n6e-5 1e-3 1e-5 5e-5 1e-3 2e-6 2e-5 1e-5 2e-5 1e-5\\n\\n- - × - ✓ ✓ ✓ ✓ ×\\n\\n- constant linear constant cosine decay to 10% cosine cosine cosine linear language text.\\n\\n✓ -\\n\\n✓ ✓ - -\\n\\n✓\\n\\n✓\\n\\n-\\n\\n\\n\\n✓ ✓ ✓\\n\\n- - ✓ - ✓ - ✓ - -\\n\\n✓ - ✓ ✓ × - × × ✓\\n\\nsystems. It includes a variety of tasks that test a wide range of linguistic phenomena, making it a comprehensive tool for eval- uating language understanding in AI.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='5.2.2. Language Understanding WinoGrande [354]: A large-scale dataset inspired by the orig- inal Winograd [357] Schema Challenge tests models on their ability to resolve pronoun ambiguity and encourages the devel- opment of models that understand the broad context in natural'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='CoQA [316]: A conversational question-answering dataset, CoQA challenges models with questions that rely on conver- sation history and require free-form text answers. Its diverse content from seven domains makes it a rigorous test for mod- els’ ability to handle a wide range of topics and conversational contexts.\\n\\nWiC [317]: This dataset assesses a model’s ability to dis- cern word meanings based on context, aiding in tasks related\\n\\n27'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='WiC [317]: This dataset assesses a model’s ability to dis- cern word meanings based on context, aiding in tasks related\\n\\n27\\n\\nTable 8: Details of various well-known pre-training and fine-tuning datasets. Here, alignment means aligning with human preferences.\\n\\nDataset C4 [10]\\n\\nType Pretrain\\n\\nSize/Samples 806GB\\n\\nTasks -\\n\\nSource Common Crawl\\n\\nCreation Automated A clean, multilingual dataset with billions\\n\\nComments\\n\\nof tokens\\n\\nmC4 [11]\\n\\nPretrain\\n\\n38.49TB\\n\\n\\n\\nCommon Crawl'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Creation Automated A clean, multilingual dataset with billions\\n\\nComments\\n\\nof tokens\\n\\nmC4 [11]\\n\\nPretrain\\n\\n38.49TB\\n\\n\\n\\nCommon Crawl\\n\\nAutomated A multilingual extension of\\n\\nthe C4 dataset, mC4 identifies over 100 lan- guages using cld3 from 71 monthly web scrapes of Common Crawl.\\n\\nPILE [301]\\n\\nROOTs [302]\\n\\nPretrain\\n\\nPretrain\\n\\n825GB\\n\\n1.61TB\\n\\n\\n\\n\\n\\nCommon Crawl, PubMed Central, OpenWebText2, ArXiv, GitHub, Books3, and others 498 Hugging Face datasets\\n\\nAutomated A massive dataset comprised of 22 con-'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Automated A massive dataset comprised of 22 con-\\n\\nAutomated\\n\\nstituent sub-datasets 46 natural and 13 programming lan- guages\\n\\nMassiveText [116]\\n\\nWikipedia [303]\\n\\nRedPajama [304]\\n\\nPushShift.io Reddit\\n\\nBigPython [140] Pool of Prompt (P3) [17]\\n\\nxP3 [154] Super-NaturalInstructions (SNI) [18]\\n\\nFlan [16] OPT-IML [97] Self-Instruct [19]\\n\\nPretrain\\n\\nPretrain\\n\\nPretrain\\n\\nPretrain\\n\\nPretrain Instructions\\n\\nInstructions Instructions\\n\\nInstructions Instructions Instructions\\n\\n10.5TB\\n\\n\\n\\n5TB\\n\\n21.1GB\\n\\n5.5TB 12M'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Pretrain\\n\\nPretrain\\n\\nPretrain Instructions\\n\\nInstructions Instructions\\n\\nInstructions Instructions Instructions\\n\\n10.5TB\\n\\n\\n\\n5TB\\n\\n21.1GB\\n\\n5.5TB 12M\\n\\n81M 12.4M\\n\\n15M 18.1M 82k\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCoding 62\\n\\n71 1616\\n\\n1836 1667 175\\n\\nMassiveWeb, Books, News, Wikipedia, Github, C4 Wikipedia CommonCrawl, C4, Wikipedia, Github, Books, StackExchange Reddit\\n\\nGitHub PromptSource\\n\\nP3+Multilingual datasets Multiple datasets\\n\\nMuffin+T0-SF+NIV2 - -\\n\\nAutomated\\n\\n99% of the data is in English\\n\\nAutomated Dump of wikipedia'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='P3+Multilingual datasets Multiple datasets\\n\\nMuffin+T0-SF+NIV2 - -\\n\\nAutomated\\n\\n99% of the data is in English\\n\\nAutomated Dump of wikipedia\\n\\nAutomated Open-source replica of LLaMA dataset\\n\\nAutomated\\n\\nAutomated Manual\\n\\nManual Manual\\n\\nManual Manual'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Automated Dump of wikipedia\\n\\nAutomated Open-source replica of LLaMA dataset\\n\\nAutomated\\n\\nAutomated Manual\\n\\nManual Manual\\n\\nManual Manual\\n\\nSubmissions and comments on Reddit from 2005 to 2019 - A Subset of PromptSource, created from 177 datasets including summarization, QA, classification, etc. Extending P3 to total 46 languages Extending P3 with additional multi- lingual datasets, total 46 languages Total 60 languages -\\n\\nAutomated Generated 52k instructions with 82k sam-\\n\\nAlpaca [158]'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Automated Generated 52k instructions with 82k sam-\\n\\nAlpaca [158]\\n\\nInstructions\\n\\n52k\\n\\n\\n\\n\\n\\nAutomated\\n\\nples from 175 seed tasks using GPT-3 Employed self-instruct method to gener- ate data from text-davinci-003\\n\\nVicuna [159]\\n\\nLLaMA-GPT-4 [160]\\n\\nInstructions\\n\\nInstructions\\n\\n125k\\n\\n52k\\n\\n\\n\\n\\n\\nShareGPT\\n\\nAlpaca\\n\\nAutomated Conversations\\n\\nby ShareGPT using public APIs Automated Recreated Alpaca dataset with GPT-4 in\\n\\nshared\\n\\nusers\\n\\nUnnatural Instructions [305] LIMA [185]\\n\\nInstructions Instructions\\n\\n68k 1k'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='shared\\n\\nusers\\n\\nUnnatural Instructions [305] LIMA [185]\\n\\nInstructions Instructions\\n\\n68k 1k\\n\\n-\\n\\n15-Seeds (SNI) Multiple datasets\\n\\nAutomated Manual\\n\\nEnglish and Chinese - Carefully created samples to test perfor- mance with fine-tuning on less data\\n\\nAnthropic-HH-RLHF [306] Anthropic-HH-RLHF-2 [178]\\n\\nAlignment Alignment\\n\\n142k 39k\\n\\n-\\n\\n-\\n\\nManual Manual\\n\\nto Word Sense Disambiguation.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Anthropic-HH-RLHF [306] Anthropic-HH-RLHF-2 [178]\\n\\nAlignment Alignment\\n\\n142k 39k\\n\\n-\\n\\n-\\n\\nManual Manual\\n\\nto Word Sense Disambiguation.\\n\\nWikitext103 [318]: With over 100 million tokens from Wikipedia’s top articles, this dataset is a rich resource for tasks that require understanding long-term dependencies, such as lan- guage modeling and translation.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='PG19 [319]: This is a digital library of diverse books from Project Gutenberg. It is specifically designed to facilitate re- search in unsupervised learning and language modeling, with a special focus on long-form content. C4 [10]: A clean, multilingual dataset, C4 offers billions of to- kens from web-crawled data. It is a comprehensive resource for training advanced Transformer models on various languages.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='ability to understand and generate coherent and sensible stories. LAMBADA [335]: This dataset evaluates contextual text un- derstanding through a word prediction task. Models must pre- dict the last word of a passage, which is easy for humans when given the whole passage, but not when given only the last sen- tence.\\n\\n5.2.4. Physical Knowledge and World Understanding'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='5.2.4. Physical Knowledge and World Understanding\\n\\nPIQA [340]: A dataset that probes the physical knowledge of models, aiming to understand how well they are learning about the real world.\\n\\nLCQMC [320]: The Large-scale Chinese Question Matching Corpus (LCQMC) is a dataset for evaluating the performance of models in semantic matching tasks. It contains pairs of ques- tions in Chinese and their matching status, making it a valuable resource for research in Chinese language understanding.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='5.2.3. Story Cloze and Sentence Completion\\n\\nStoryCloze [334]: It introduces a new “StoryCloze Test”, a commonsense reasoning framework for evaluating story under- standing, generation, and script learning. It considers a model’s\\n\\nTriviaQA [341]: A dataset that tests models on reading com- prehension and open domain question answering (QA) tasks, with a focus on Information Retrieval (IR)-style QA.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='ARC [342]: A larger version of the ARC-Challenge, this dataset contains both easy and challenging grade-school level, multiple-choice science questions. It is a comprehensive test of a model’s ability to understand and answer complex questions. ARC-Easy [342]: A subset of the ARC dataset, ARC- Easy, contains questions that are answered correctly by either a retrieval-based algorithm or a word co-occurrence algorithm.\\n\\n28\\n\\non\\n\\nTable 9: Categorized evaluation datasets used in evaluating LLMs.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='28\\n\\non\\n\\nTable 9: Categorized evaluation datasets used in evaluating LLMs.\\n\\nType Multi-Task\\n\\nLanguage Understanding'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Language Understanding\\n\\nDatasets/Benchmarks MMLU [307], SuperGLUE [2], BIG-bench [308], GLUE [309], BBH [308], CUGE [310], Zero- CLUE [311], FewCLUE [312], Blended Skill Talk [313], HELM [314], KLUE-STS [315] CoQA [316], WiC [317], Wikitext103 [318], PG19 [319], LCQMC [320], QQP [321], WinoGender [322], CB [323], FinRE [324], SanWen [325], AFQMC [311], BQ Corpus [326], CNSS [327], CKBQA 13 [328], CLUENER [311], Weibo [329], AQuA [330], OntoNotes [331], HeadQA [332], Twitter Dataset [333]'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Story Cloze and Sentence Completion\\n\\nStoryCloze [334], LAMBADA [335], LCSTS [336], AdGen [337], E2E [338], CHID [339], CHID- FC [312]\\n\\nPhysical Knowledge and World Understanding\\n\\nContextual Language Understanding Commonsense Reasoning\\n\\nReading Comprehension\\n\\nMathematical Reasoning'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='PIQA [340], TriviaQA [341], ARC [342], ARC-Easy [342], ARC-Challenge [342], PROST [343], Open- BookQA [344], WebNLG [345], DogWhistle Insider & Outsider [346] RACE [347], RACE-Middle [347], RACE-High [347], QuAC [348], StrategyQA [349], Quiz Bowl [350], cMedQA [351],cMedQA2 [352], MATINF-QA [353] WinoGrande [354], HellaSwag [355], COPA [356], WSC [357], CSQA [358], SIQA [359], C3 [360], CLUEWSC2020 [311], CLUEWSC [311], CLUEWSC-FC [312], ReCoRD [361] SQuAD [362], BoolQ [363], SQUADv2 [364],'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[357], CSQA [358], SIQA [359], C3 [360], CLUEWSC2020 [311], CLUEWSC [311], CLUEWSC-FC [312], ReCoRD [361] SQuAD [362], BoolQ [363], SQUADv2 [364], DROP [365], RTE [366], WebQA [367], CMRC2017 [368], CMRC2018 [369], CMRC2019 [370], COTE-BD [371], COTE-DP [371], COTE-MFW [371], Mul- tiRC [372], Natural Questions [373], CNSE [327], DRCD [374], DuReader [375], Dureaderrobust [376], DuReader-QG [375], SciQ [377], Sogou-log [378], Dureaderrobust-QG [376], QA4MRE [379], KorQuAD 1.0 [380],'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='DuReader [375], Dureaderrobust [376], DuReader-QG [375], SciQ [377], Sogou-log [378], Dureaderrobust-QG [376], QA4MRE [379], KorQuAD 1.0 [380], CAIL2018-Task1 & Task2 [381] MATH [382], Math23k [383], GSM8K [384], MathQA [385], MGSM [386], MultiArith [387], AS- Div [388], MAWPS [389], SVAMP [390] HumanEval [141], DS-1000 [391], MBPP [392], APPS [382], CodeContests [142]'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Problem Solving Natural Language Inference & Logical Reasoning\\n\\nCross-Lingual Understanding\\n\\nTruthfulness and Fact Checking Biases and Ethics in AI Toxicity Language Translation Scientific Knowledge\\n\\nDialogue'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='ANLI [393], MNLI-m [394], MNLI-mm [394],QNLI [362], WNLI [357], OCNLI [311], CMNLI [311], ANLI R1 [393], ANLI R2 [393], ANLI R3 [393], HANS [395], OCNLI-FC [312], LogiQA [396], Strate- gyQA [349] MLQA [397], XNLI [398], PAWS-X [399], XSum [400], XCOPA [401], XWinograd [402], TyDiQA- GoldP [403], MLSum [404] TruthfulQA [405], MultiFC [406], Fact Checking on Fever [407] ETHOS [408], StereoSet [409], BBQ [410], Winobias [411], CrowS-Pairs [412] RealToxicityPrompts [413], CivilComments toxicity'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Checking on Fever [407] ETHOS [408], StereoSet [409], BBQ [410], Winobias [411], CrowS-Pairs [412] RealToxicityPrompts [413], CivilComments toxicity classification [414] WMT [415], WMT20 [416], WMT20-enzh [416], EPRSTMT [312], CCPM [417] AminoProbe [148], BioLAMA [148], Chemical Reactions [148], Galaxy Clusters [148], Mineral Groups [148] Wizard of Wikipedia [418], Empathetic Dialogues [419], DPC-generated [96] dialogues, ConvAI2 [420], KdConv [421] TNEWS-FC [312], YNAT [315], KLUE-TC [315],'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='of Wikipedia [418], Empathetic Dialogues [419], DPC-generated [96] dialogues, ConvAI2 [420], KdConv [421] TNEWS-FC [312], YNAT [315], KLUE-TC [315], CSL [311], CSL-FC [312], IFLYTEK [422]'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Topic Classification\\n\\nIt is a great starting point for models beginning to explore ad- vanced question-answering.\\n\\ntions. It is designed to evaluate the comprehension ability of models in a more academic and challenging context.\\n\\nARC-Challenge [342]: A rigorous question-answering dataset, ARC-Challenge includes complex, grade-school level questions that demand reasoning beyond simple retrieval, test- ing the true comprehension capabilities of models.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='QuAC [348]: This dataset simulates an information-seeking dialog between students and teachers using hidden Wikipedia text. It introduces unique challenges not found in machine com- prehension datasets, making it a valuable resource for advanc- ing dialog systems.\\n\\n5.2.5. Contextual Language Understanding'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='5.2.5. Contextual Language Understanding\\n\\nRACE [347]: The RACE dataset is a reading comprehension dataset collected from English examinations in China, which benchmarks AI models for understanding and answering ques- tions on long and complex passages, simulating the challenge of a real-world examination.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='5.2.6. Commonsense Reasoning HellaSwag [355]: A dataset that challenges models to pick the best ending to a context uses Adversarial Filtering to create a ‘Goldilocks’ zone of complexity, where generated text is absurd to humans but often misclassified by models.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='RACE-Middle [347]: Another subset of the RACE [347] dataset, RACE-Middle, contains middle school-level English exam questions. It offers a slightly less challenging but academ- ically oriented evaluation of a model’s comprehension skills.\\n\\nRACE-High [347]: A subset of the RACE [347] dataset, RACE-High consists of high school-level English exam ques-'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='RACE-High [347]: A subset of the RACE [347] dataset, RACE-High consists of high school-level English exam ques-\\n\\nCOPA [401]: This dataset evaluates a model’s progress in open-domain commonsense causal reasoning. Each question comprises a premise and two alternatives, and the model must select the more plausible alternative, testing a model’s ability to understand and reason about cause and effect.\\n\\nWSC [357]: The Winograd Schema Challenge (WSC) is a\\n\\n29'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='WSC [357]: The Winograd Schema Challenge (WSC) is a\\n\\n29\\n\\nTable 10: An illustration of training datasets and evaluation tasks employed by pre-trained LLMs. Here, “QA” is question-answering, “Clf” is classification, “NLI” is natural language inference, “MT” is machine translation, “RC” is reading comprehension, “CR” is commonsense reasoning, “MR” is mathematical reasoning, “Mem.” is memorization.\\n\\nBenchmark\\n\\nModels\\n\\nTraining Dataset\\n\\nBIG- bench\\n\\nMMLU\\n\\nSuper GLUE\\n\\nQA Clf NLI MT\\n\\nCloze/ Completion'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Benchmark\\n\\nModels\\n\\nTraining Dataset\\n\\nBIG- bench\\n\\nMMLU\\n\\nSuper GLUE\\n\\nQA Clf NLI MT\\n\\nCloze/ Completion\\n\\nRC CR MR Coding\\n\\nT5 GPT-3\\n\\nmT5 PanGu-α CPM-2 Codex ERNIE-3.0\\n\\nJurassic-1\\n\\nHyperCLOVA\\n\\nYuan 1.0\\n\\nGopher'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='C4 [10] Common Crawl, WebText, Books Cor- pora, Wikipedia mC4 [11] 1.1TB Chinese Text Corpus WuDaoCorpus [109] 54 million public repositories from Github Chinese text corpora, Baidu Search, Web text, QA-long, QA-short, Poetry and Cou- plet Domain-specific data from medical, law, and financial area Baidu knowledge graph with more than 50 million facts Wikipedia, OWT, Books, C4, Pile [301], arXiv, GitHub Korean blogs, Community sites, News, KiN Korean Wikipedia, Wikipedia (En- glish and'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='facts Wikipedia, OWT, Books, C4, Pile [301], arXiv, GitHub Korean blogs, Community sites, News, KiN Korean Wikipedia, Wikipedia (En- glish and Japanese), Modu-Corpus: Mes- senger, News, Spoken and written lan- guage corpus, Web corpus Common Crawl, SogouT, Sogou News, Baidu Baike, Wikipedia, Books subsets of MassiveWeb Books, C4, News, GitHub and Wikipedia samples from Mas- siveText'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='✓\\n\\n✓\\n\\n✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓\\n\\n✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓\\n\\n✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='ERNIE-3.0 TITAN Same as ERNIE 3.0 and ERNIE 3.0 ad- versarial dataset, ERNIE 3.0 controllable dataset Pile [301] RoBERTa [299], Pile [301], PushShift.io Reddit [423] ROOTs [13] arXiv, PMC, Semantic Scholar, Wikipedia, StackExchange, LibreText, Open Text- books, RefSeq Genome, OEIS, LIPID MAPS, NASAExoplanet, Common Crawl, ScientificCC, AcademicCC, GitHub repos- itories Khan Problems, GSM8K, OneS- mallStep Filtered Webpages, Social media conversa- tions Wikipedia, Forums, Books, News Infiniset :'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='GitHub repos- itories Khan Problems, GSM8K, OneS- mallStep Filtered Webpages, Social media conversa- tions Wikipedia, Forums, Books, News Infiniset : Public documents, Dialogs, Ut- terances Two snapshots of Common Crawl and Books3, OpenWebText2, Stack Exchange, PubMed Abstracts, Wikipedia, PG-19 [242], BookCorpus2, NIH ExPorter, Pile, CC-Stories, RealNews Selected GitHub repositories, CodeCon- tests: Codeforces, Description2Code, Co- deNet MassiveWeb, MassiveText Books, C4, News, GitHub,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='RealNews Selected GitHub repositories, CodeCon- tests: Codeforces, Description2Code, Co- deNet MassiveWeb, MassiveText Books, C4, News, GitHub, Wikipedia webpages, books, Wikipedia, news, arti- cles, source code, social media conversa- tions Wikipedia, mC4 Same as PaLM - - Pile, BigQuery, BigPython CommonCrawl, C4, Github, Wikipedia, Books, arXiv, StackExchange WuDaoCorpora, CLUE, Pile, C4, Python code inPile, Pile, C4, Wikipedia CodeSearchNet, Github Code The Stack v1.2 ✓ Web documents, Code,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='StackExchange WuDaoCorpora, CLUE, Pile, C4, Python code inPile, Pile, C4, Wikipedia CodeSearchNet, Github Code The Stack v1.2 ✓ Web documents, Code, Books, Maths, Conversation'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='GPT-NeoX-20B OPT\\n\\nBLOOM Galactica\\n\\nGLaM\\n\\nLaMDA\\n\\nMT-NLG\\n\\nAlphaCode\\n\\nChinchilla\\n\\nPaLM\\n\\nAlexaTM U-PaLM UL2 GLM-130B CodeGen LLaMA\\n\\nPanGu-Σ\\n\\nBloombergGPT CodeT5+ StarCoder LLaMA-2 PaLM-2\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓ ✓\\n\\n✓ ✓\\n\\n✓\\n\\n✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓\\n\\n✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓ ✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓\\n\\n✓\\n\\n✓ ✓ ✓ ✓\\n\\n30'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓\\n\\n✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓ ✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓\\n\\n✓\\n\\n✓ ✓ ✓ ✓\\n\\n30\\n\\nTruthful/ Bias/ Toxicity/ Mem.\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓ ✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\nTable 11: An illustration of training datasets and evaluation benchmarks used in fine-tuned LLMs. “SNI” is a short of Super-NaturalInsturctions.\\n\\nModels\\n\\nTraining Dataset\\n\\nBIG- bench\\n\\nMMLU BBH RAFT FLAN SNI\\n\\nPromptSource TyDiQA HumanEval MBPP\\n\\nTruthful/ Bias/ Toxicity'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Models\\n\\nTraining Dataset\\n\\nBIG- bench\\n\\nMMLU BBH RAFT FLAN SNI\\n\\nPromptSource TyDiQA HumanEval MBPP\\n\\nTruthful/ Bias/ Toxicity\\n\\nT0 WebGPT\\n\\nTk-INSTRUCT mT0 OPT-IML\\n\\nFlan WizardCoder'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='MMLU BBH RAFT FLAN SNI\\n\\nPromptSource TyDiQA HumanEval MBPP\\n\\nTruthful/ Bias/ Toxicity\\n\\nT0 WebGPT\\n\\nTk-INSTRUCT mT0 OPT-IML\\n\\nFlan WizardCoder\\n\\nPool of Prompts fact- ELI5 [424], [166], TriviaQA [341], check ARC-Challenge ARC- [342], Easy [342], Hand-written data, Demonstrations of humans, Com- parisons between model-generated answers SNI [18] xP3 [154] PromptSource [17], FLAN [16], [425], UnifiedSKG [426], SNI [428], CrossFit T5 [10], Reasoning Muffin, T0-SF, NIv2, CoT Code Alpaca\\n\\nELI5'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='ELI5\\n\\n[427], ExMix\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\nreading comprehension task in which a system must resolve references in a text, often requiring world knowledge and rea- soning about the text.\\n\\nof machine performance.\\n\\n5.2.8. Mathematical Reasoning\\n\\nCSQA [358]: The CommonsenseQA is a question-answering dataset that requires commonsense knowledge to evaluate the ability of AI models to understand and answer questions.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='MATH [382]: This dataset is a platform for evaluating the mathematical problem-solving abilities of AI models. It con- tains a diverse set of math problems, ranging from arithmetic to calculus, and is designed to test the model’s ability to under- stand and solve complex mathematical problems.\\n\\n5.2.7. Reading Comprehension'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='5.2.7. Reading Comprehension\\n\\nBoolQ [363]: A dataset derived from Google search queries, BoolQ challenges models to answer binary (yes/no) questions. The questions are naturally occurring and are paired with a paragraph from a Wikipedia article containing the answer. It is a test of reading comprehension and reasoning.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Math23k [383]: This one challenges a model’s ability to un- derstand and solve mathematical word problems. It contains 23,000 Chinese arithmetic word problems that require models to perform reasoning and computation based on the problem description.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='SQUADv2 [364]: The Stanford Question Answering Dataset (SQuAD) [362] is a collection of questions posed by crowd workers on a set of Wikipedia articles, where the answer to ev- ery question is a segment of text from the corresponding reading passage. SQuADv2 combines the original SQuAD1.1 dataset with over 50,000 unanswerable questions. The aim is to evalu- ate a model’s ability to understand and answer questions based on a given context and to determine when a question is unan- swerable.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='GSM8K [384]: A dataset of diverse grade school math word problems, testing a model’s ability to perform multi-step math- ematical reasoning.\\n\\n5.2.9. Problem Solving and Logical Reasoning ANLI [393]: A large-scale dataset designed to test the robust- ness of machine learning models in Natural Language Inference (NLI) is created through an iterative, adversarial process where humans try to generate examples that models cannot correctly classify.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='DROP [365]: DROP, or Discrete Reasoning Over the con- tent of Paragraphs, is designed to test a model’s ability to un- derstand a wide variety of reading phenomena. It encourages comprehensive and reliable evaluation of reading comprehen- sion capabilities.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='RTE [366]: The Recognizing Textual Entailment (RTE) datasets come from a series of annual competitions on textual entailment, predicting whether a given sentence logically fol- lows from another and evaluating a model’s understanding of logical relationships in a text. WebQA [367]: A dataset for open-domain question answering, WebQA offers a large collection of web-based question-answer pairs. It is designed to assess the ability of AI models to under- stand and answer questions based on web'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='collection of web-based question-answer pairs. It is designed to assess the ability of AI models to under- stand and answer questions based on web content.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='CMRC2018 [369]: This dataset is a test of Chinese language models’ ability to reason comprehensively and is designed with a challenging span-extraction format that pushes the boundaries\\n\\nHumanEval [141]: A dataset for evaluating the problem- solving ability of AI models, which includes a diverse set of tasks that require various cognitive abilities, making it a com- prehensive tool for assessing general intelligence in AI.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='StrategyQA [349]: A question-answering dataset that re- quires reasoning over multiple pieces of evidence to evaluate the strategic reasoning ability of AI models, pushing the bound- aries of what machines can understand and answer.\\n\\n5.2.10. Cross-Lingual Understanding'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='5.2.10. Cross-Lingual Understanding\\n\\nXNLI [398]: A cross-lingual benchmark, XNLI extends the MultiNLI [429] corpus to 15 languages, including low-resource ones like Urdu. It tests models on cross-lingual sentence under- standing, with 112,500 annotated pairs across three categories: entailment, contradiction, and neutral. PAWS-X [399]: PAWS-X, or Cross-lingual Paraphrase Adver- saries from Word Scrambling, is a multilingual version of the\\n\\n31'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='31\\n\\nPAWS [430] dataset for paraphrase identification. It includes examples in seven languages and is designed to evaluate the performance of cross-lingual paraphrase identification models.\\n\\n5.2.11. Truthfulness\\n\\nTruthful-QA [405]: A unique benchmark that measures a language model’s truthfulness when generating answers. The dataset includes questions across various categories like health, law, and politics, some designed to test the model against com- mon human misconceptions.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='5.2.12. Biases and Ethics in AI\\n\\nETHOS [408]: ETHOS is a hate speech detection dataset built from YouTube and Reddit comments. It is a tool in the fight against online hate speech, offering binary and multi-label variants for robust content moderation.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='StereoSet [409]: StereoSet is a comprehensive dataset de- signed to measure and evaluate the presence of stereotypical biases in language models. It focuses on four key domains: gender, profession, race, and religion. Contrasting stereotypi- cal bias against language modeling ability provides a valuable tool for understanding and mitigating biases in large language models.\\n\\n6. Applications'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Applying Large Language Models (LLMs) to a variety of downstream tasks has become a popular trend in both AI- related research communities and industries, with many emerg- ing uses being discovered and explored daily. LLMs, which are capable of understanding and generating human-like text, have found meaningful applications across a variety of fields. This section provides an overview of LLM applications in medicine, education, science, mathematics, law, finance, robotics, and coding. While'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='fields. This section provides an overview of LLM applications in medicine, education, science, mathematics, law, finance, robotics, and coding. While each of these domains pose different challenges, LLMs open up opportunities to make significant contributions to these domains through their generalizability. General Purpose: LLMs are being widely considered as general-purpose tools for a wide variety of tasks [431]. This is due to their inherent ability to understand, generate, and manipulate'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='considered as general-purpose tools for a wide variety of tasks [431]. This is due to their inherent ability to understand, generate, and manipulate human-like text in a contextually relevant man- ner. This allows them to perform tasks ranging from simple language translation and question-answering to more complex tasks like summarization, text generation, and even program- ming help [432]. The utility of LLMs is further enhanced by their ability to adapt to the specific style and tone of the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='and even program- ming help [432]. The utility of LLMs is further enhanced by their ability to adapt to the specific style and tone of the text they are processing, making the outputs more user-friendly and context-aware. In everyday applications, LLMs can be used as personal assistants, helping users draft emails or schedule appointments [433]; they can also be deployed in customer ser- vice to handle common questions or applied to generate content for digital platforms like websites by'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='they can also be deployed in customer ser- vice to handle common questions or applied to generate content for digital platforms like websites by creating human-like text based on given prompts [434]. Moreover, LLMs play a cru- cial role in data analysis, where they can filter large volumes of text data, summarize key points, and find patterns that would take humans much longer to identify [435]. Despite their wide- ranging applications, it is essential to remember that LLMs,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='32'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='similar to any AI system, are only as good as the data they have been trained on. Medicine: The application of LLMs in the field of medicine is reshaping healthcare delivery and research. For example, LLMs are increasingly used in clinical decision support systems to provide physicians with evidence-based treatment recommen- dations [436, 437, 438]. By analyzing patient data and medical literature, they can help identify potential diagnoses, suggest appropriate tests, and recommend optimal'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='438]. By analyzing patient data and medical literature, they can help identify potential diagnoses, suggest appropriate tests, and recommend optimal treatment strategies. Moreover, LLMs can also enhance patient interactions with healthcare systems; e.g., they can be used in chatbot applica- tions [439, 440, 441] to answer patient queries about symptoms or medications, schedule appointments, and even provide es- sential health advice. For medical research, LLMs are used to extract and filter'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='symptoms or medications, schedule appointments, and even provide es- sential health advice. For medical research, LLMs are used to extract and filter information from a considerable amount of medical literature, identify relevant studies, summarize find- ings, and even predict future research trends [442, 443, 444]. For medical education, LLMs can help create training mate- rials, generate exam questions, provide detailed explanations of complex medical topics, and offer personalized feedback'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='help create training mate- rials, generate exam questions, provide detailed explanations of complex medical topics, and offer personalized feedback to students [445, 446, 447, 448]. They can also simulate patient interactions, enabling students to practice and improve their clinical skills. At a broader level, LLMs can assist in public health initiatives by analyzing media data to detect disease out- breaks, monitor public sentiment towards health policies, and disseminate health information in'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='by analyzing media data to detect disease out- breaks, monitor public sentiment towards health policies, and disseminate health information in a clear and understandable manner [449]. LLMs can be employed to support public health initiatives, addressing related issues such as data privacy, the necessity for explainability, and the potential risk of propagat- ing biases [450, 451]. Education: The integration of LLMs into the educational sec- tor offers opportunities to enhance learning'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='risk of propagat- ing biases [450, 451]. Education: The integration of LLMs into the educational sec- tor offers opportunities to enhance learning experiences, teacher support, and educational content development. For students, by analyzing their learning styles, performance, and preferences, LLMs can provide customized study materials and practice questions to develop personalized learning experiences [452]. For teachers, LLMs can help to create lesson plans and grade assignments and generate'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='questions to develop personalized learning experiences [452]. For teachers, LLMs can help to create lesson plans and grade assignments and generate diverse and inclusive educational content, significantly saving more time for teaching and student interaction [453, 454]. In language learning, LLMs serve as advanced conversational partners capable of simulating conver- sations in multiple languages, correcting grammar, enhancing vocabulary, and aiding pronunciation for the needs of fluency in'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='of simulating conver- sations in multiple languages, correcting grammar, enhancing vocabulary, and aiding pronunciation for the needs of fluency in practice [455]. Furthermore, LLMs improve accessibility in education by providing support for students with disabili- ties. They can generate real-time transcriptions for the hear- ing impaired, offer reading assistance for the visually impaired, and simplify complex texts for those with learning disabili- ties [451]. As LLMs continue to evolve,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='reading assistance for the visually impaired, and simplify complex texts for those with learning disabili- ties [451]. As LLMs continue to evolve, their applications in education can benefit more students and teachers from different perspectives in practice. Science: Similar to medical applications, LLMs can expedite the research process by quickly analyzing and summarizing sci- entific literature. By briefing comprehensible and accessible re- search summaries, LLMs can assist researchers in'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='analyzing and summarizing sci- entific literature. By briefing comprehensible and accessible re- search summaries, LLMs can assist researchers in staying up- to-date with the latest findings, even in fields outside their area of expertise [456, 457]. In addition, LLMs can aid scientists'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Table 12: Performance comparison of top performing LLMs across various NLU and NLG tasks. Here, “N-Shots” indicate the number of example prompts provided to the model during the evaluation, representing its capability in few-shot or zero-shot learning settings, “f” represents the fine-tuned version, and “B” represents the benchmark.\\n\\nTask\\n\\nMulti-Task\\n\\nLanguage Understanding Story Comprehension and Generation Physical Knowledge and World Understanding Contextual Language Understanding'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Multi-Task\\n\\nLanguage Understanding Story Comprehension and Generation Physical Knowledge and World Understanding Contextual Language Understanding\\n\\nDataset/Benchmark\\n\\nBIG-bench (B) MMLU (B) SuperGLUE (B) HellaSwag StoryCloze PIQA TriviaQA\\n\\nLAMBADA\\n\\nTop-1\\n\\nTop-2\\n\\nModel (Size) Chinchilla (70B) GPT-4 (-) ERNIE 3.0 (12B) GPT-4 (-) GPT3 (175B) PaLM-2 (Large) PaLM-2 (Large)\\n\\nScore (N-shots) 65.1 (5-shot) 86.4 (5-shot) 90.6 (-) 95.3 (10-shot) 87.7 (few shot) 85.0 (one shot) 86.1 (one shot)'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Score (N-shots) 65.1 (5-shot) 86.4 (5-shot) 90.6 (-) 95.3 (10-shot) 87.7 (few shot) 85.0 (one shot) 86.1 (one shot)\\n\\nModel (Size) Gopher (280B) Gemini (Ultra) PaLM(f) (540B) Gemini (Ultra) PaLM-2 (Large) LLaMa (65B) LLaMA-2 (70B)\\n\\nScore (N-shots) 53.97 (5-shot) 83.7 (5-shot) 90.4 (-) 87.8 (10-shot) 87.4 (one shot) 82.8 (zero shot) 85.0 (one shot)\\n\\nPaLM (540B)\\n\\n89.7 (few shot) MT-NLG (530B)\\n\\n87.15 (few shot)\\n\\nTop-3'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='PaLM (540B)\\n\\n89.7 (few shot) MT-NLG (530B)\\n\\n87.15 (few shot)\\n\\nTop-3\\n\\nModel (Size) PaLM (540B) Flan-PaLM-2(f) (Large) T5 (11B) PaLM-2 (Large) OPT (175B) MT-NLG (530B) PaLM (540B)\\n\\nPaLM-2 (Large)\\n\\nScore (N-shots) 53.7 (5-shot) 81.2 (5-shot) 88.9 (-) 86.8 (one shot) 79.82 (-) 81.99 (zero shot) 81.4 (one shot)\\n\\n86.9 (one shot)\\n\\nCommonsense Reasoning\\n\\nReading Comprehension Truthfulness\\n\\nMathematical Reasoning\\n\\nWinoGrande SIQA BoolQ Truthful-QA MATH GSM8K'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='86.9 (one shot)\\n\\nCommonsense Reasoning\\n\\nReading Comprehension Truthfulness\\n\\nMathematical Reasoning\\n\\nWinoGrande SIQA BoolQ Truthful-QA MATH GSM8K\\n\\nGPT-4 (-) LLaMA (65B) PaLM(f) (540B) LLaMA (65B) Gemini (Ultra) GPT-4 (-)\\n\\n87.5 (5-shot) 52.3 (zero shot) 92.2 (-) 57 (-) 53.2 (4-shot) 92.0 (5-shot)\\n\\nPaLM-2 (Large) Chinchilla (70B) T5 (11B)\\n\\nPaLM-2 (Large) PaLM-2 (Large)\\n\\n83.0 (one shot) 51.3 (zero shot) 91.2 (-)\\n\\n34.3 (4-shot) 80.7 (8-shot)\\n\\nPaLM (540B) Gopher (280B) PaLM-2 (Large)'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='PaLM-2 (Large) PaLM-2 (Large)\\n\\n83.0 (one shot) 51.3 (zero shot) 91.2 (-)\\n\\n34.3 (4-shot) 80.7 (8-shot)\\n\\nPaLM (540B) Gopher (280B) PaLM-2 (Large)\\n\\nLLaMa-2 (65B) U-PaLM (540B)\\n\\n81.1 (zero shot) 50.6 (zero shot) 90.9 (one shot)\\n\\n13.5 (4-shot) 58.5 (-)\\n\\nProblem Solving and Logical Reasoning\\n\\nHumanEval\\n\\nGemini(f) (Ultra)\\n\\n74.4 (zero shot)\\n\\nGPT-4 (-)\\n\\n67.0 (zero shot)\\n\\nCode Llama (34B)\\n\\n48.8 (zero shot)'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='in formulating new hypotheses and research questions since their ability to process large-scale datasets allows them to un- veil insights that might not be immediately apparent to human researchers [458]. Moreover, for scientific writing, LLMs can help researchers draft documents, suggest improvements, and ensure adherence to specific formatting guidelines [459, 460]. This not only saves time but also improves the clarity of scien- tific communication, enabling interdisciplinary teams to work'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='guidelines [459, 460]. This not only saves time but also improves the clarity of scien- tific communication, enabling interdisciplinary teams to work together more effectively. Maths: In addition to providing mathematical research and education support, LLMs can assist in solving mathematical problems by giving step-by-step explanations and guiding users through complex proofs and calculations. They can help iden- tify errors in reasoning or computation and suggest corrections, serving as an'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='users through complex proofs and calculations. They can help iden- tify errors in reasoning or computation and suggest corrections, serving as an invaluable tool for both learning and verification purposes [461, 462]. LLMs can be employed to check the valid- ity of mathematical proofs, offering a preliminary filter before human review. While they are not a substitute for the meticu- lous work of mathematicians, they can help simplify the process of proof verification [463, 464]. Moreover, LLMs'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='are not a substitute for the meticu- lous work of mathematicians, they can help simplify the process of proof verification [463, 464]. Moreover, LLMs enhance ac- cessibility to mathematics by translating complex concepts and findings into understandable language for non-specialists [465], where the gap between theoretical mathematics and applied contexts such as physics, engineering, and economics can be bridged. Law: LLMs can assist with the thematic analysis of legal doc- uments, including'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='contexts such as physics, engineering, and economics can be bridged. Law: LLMs can assist with the thematic analysis of legal doc- uments, including generating initial coding for datasets, iden- tifying themes, and classifying data according to these themes. This collaborative effort between legal experts and LLMs has proved to be effective in analyzing legal texts such as court opinions on theft, improving both the efficiency and quality of the research [466]. Additionally, LLMs have been'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='analyzing legal texts such as court opinions on theft, improving both the efficiency and quality of the research [466]. Additionally, LLMs have been evaluated for their ability to generate explanations of legal terms, focusing on improving factual accuracy and relevance by incorporating sentences from case law. By feeding relevant case law into the LLM, the augmented models can generate higher-quality expla- nations with less factually incorrect information [467]. More- over, LLMs can be'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='the LLM, the augmented models can generate higher-quality expla- nations with less factually incorrect information [467]. More- over, LLMs can be trained with specialized domain knowledge'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='to perform legal reasoning tasks [468] and answer legal ques- tions [469]. Finance: LLMs like BloombergGPT [151], trained on exten- sive proprietary financial datasets, exhibit superior performance on financial tasks. This indicates the value of domain-specific training in creating LLMs that can more accurately understand and process industry-specific language and concepts. The intro- duction of FinGPT [470] as an open-source model offers trans- parent and accessible resources to develop novel'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='language and concepts. The intro- duction of FinGPT [470] as an open-source model offers trans- parent and accessible resources to develop novel applications such as robo-advising, algorithmic trading, and low-code so- lutions, ultimately expanding the capabilities of financial ser- vices. Both BloombergGPT and FinGPT show the adaptabil- ity of LLMs to the financial domain, with the former showing the power of custom datasets and the latter emphasizing a data- centric approach and low-rank'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='LLMs to the financial domain, with the former showing the power of custom datasets and the latter emphasizing a data- centric approach and low-rank adaptation techniques for cus- tomization. Moreover, LLMs demonstrate an ability to break down complex financial tasks into actionable plans, enabling end-to-end solutions that were previously unfeasible with a sin- gle model [471]. Robotics: In robotics research, LLMs have promising appli- cations, such as enhancing human-robot interaction [28,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='with a sin- gle model [471]. Robotics: In robotics research, LLMs have promising appli- cations, such as enhancing human-robot interaction [28, 472, 473, 474], task planning [237], motion planning [246], nav- igation [246, 475], object manipulation [236], personalized robots [476], etc. LLMs enable robots to understand the en- vironment effectively and generate plans to complete tasks col- laboratively [240, 26]. They can facilitate continuous learning by allowing robots to access and integrate'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='and generate plans to complete tasks col- laboratively [240, 26]. They can facilitate continuous learning by allowing robots to access and integrate information from a wide range of sources, helping robots acquire new skills, adapt to changes, and refine their paths [224, 233, 234].'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='7. Challenges and Future Directions\\n\\nLLMs such as GPT-4 and its predecessors have significantly advanced natural language processing. Nevertheless, they also bring along a set of challenges. The computational cost, ad- versarial robustness, and interpretability are among the tech- nical challenges that are intrinsic to these models. Further- more, as these models are scaled up to handle more complex\\n\\n33'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='tasks or to operate in more complex or dynamic environments, new challenges in scalability, privacy, and real-time processing emerge. On the frontier of foundational research, integrating multi-modality and the effectiveness of transfer learning are be- ing keenly explored. Additionally, the continuous learning as- pect of these models, which aims to have models that can adapt to new information over time, presents a fresh set of challenges. These challenges not only underscore the technical'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='to have models that can adapt to new information over time, presents a fresh set of challenges. These challenges not only underscore the technical intricacies involved but also highlight the broader impact and the future trajectory of LLMs in real-world applications. The following sections delve into these challenges, shedding light on the on- going and potential efforts to address them. Computational Cost: Training LLMs require extensive compu- tational resources, which increases production'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='and potential efforts to address them. Computational Cost: Training LLMs require extensive compu- tational resources, which increases production costs and raises environmental concerns due to substantial energy consump- tion during large-scale training. Improved performance occurs as computational resources increase, but the rate of improve- ment gradually decreases when both the model and dataset size remain fixed, following the power law of diminishing re- turns [477]. Bias and Fairness: LLMs'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='decreases when both the model and dataset size remain fixed, following the power law of diminishing re- turns [477]. Bias and Fairness: LLMs can inherit and amplify societal bi- ases in their training data. These biases can manifest in the model’s outputs, leading to potential ethical and fairness is- sues [478]. Overfitting: Although LLMs possess substantial learning ca- pabilities, they are susceptible to overfitting noisy and peculiar patterns within their extensive training data.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='possess substantial learning ca- pabilities, they are susceptible to overfitting noisy and peculiar patterns within their extensive training data. Consequently, this may cause them to generate illogical responses [479]. The de- bate about Memorization vs. Generalization in LLMs is about finding the right balance. Memorization allows the model to remember specific details from its training data, ensuring it can provide accurate answers to precise questions. However, gen- eralization enables the'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='specific details from its training data, ensuring it can provide accurate answers to precise questions. However, gen- eralization enables the model to make inferences and produce responses for inputs it has not seen before, which is essential for handling various real-world tasks. Striking the right bal- ance is the challenge: too much memorization can lead to over- fitting, making the model inflexible and struggling with new inputs [480]. Economic and Research Inequality: The high cost of'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='can lead to over- fitting, making the model inflexible and struggling with new inputs [480]. Economic and Research Inequality: The high cost of train- ing and deploying LLMs may make their development concen- trated within well-funded organizations, potentially worsening economic and research inequalities in AI [481]. Reasoning and Planning: Some reasoning and planning tasks, even as seemingly simple as common-sense planning, which humans find easy, remain well beyond the current capabilities'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='reasoning and planning tasks, even as seemingly simple as common-sense planning, which humans find easy, remain well beyond the current capabilities of LLMs evaluated using an assessment framework. This is not entirely unexpected, considering that LLMs primarily generate text completions based on likelihood and offer no solid guaran- tees in terms of reasoning abilities [482]. Hallucinations: LLMs exhibit “hallucinations\", where they generate responses that, while sounding plausible, are'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='terms of reasoning abilities [482]. Hallucinations: LLMs exhibit “hallucinations\", where they generate responses that, while sounding plausible, are incorrect or do not align with the provided information [483]. Hallucina- tions can be categorized into three categories.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Input-conflicting hallucination, wherein LLMs produce content that diverges from the input given by users.\\n\\nContext-conflicting hallucination, where LLMs generate\\n\\n34\\n\\ncontent that contradicts information they have generated earlier.\\n\\nFact-conflicting hallucination involves LLM’s generation of content that does not align with established world knowledge.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Prompt Engineering: Prompts serve as inputs to LLMs, and their syntax and semantics play a crucial role in determining the model’s output. The prompt variations, sometimes counter- intuitive to humans, can result in significant changes in model output and are addressed through prompt engineering, which involves designing natural language queries to guide LLMs responses effectively [484, 32]. Limited Knowledge: Information acquired during pretraining is limited and may become obsolete after some'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='guide LLMs responses effectively [484, 32]. Limited Knowledge: Information acquired during pretraining is limited and may become obsolete after some time. Re- training the model using updated data is costly. To generate factually accurate responses, people use a retrieval augmen- tation pipeline [198]. However, pre-trained models are not trained with retrieval augmentation generation (RAG) [6, 21]; hence, adapting the training pipeline is necessary [193, 25]. Safety and Controllability: Using'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='with retrieval augmentation generation (RAG) [6, 21]; hence, adapting the training pipeline is necessary [193, 25]. Safety and Controllability: Using LLMs comes with the risk of generating harmful, misleading, or inappropriate content, whether by accident or when given specific prompts. Ensuring these models are safely utilized is a significant concern [485]. Security and Privacy: LLMs are prone to leaking personal information and generating false, unethical, misaligned re- sponses. Researchers'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[485]. Security and Privacy: LLMs are prone to leaking personal information and generating false, unethical, misaligned re- sponses. Researchers have explored various security attacks, i.e., backdoor attacks, jailbreaking, prompt injection, and data poisoning, that lead to breaking LLMs security. Therefore, developing better defense mechanisms is essential to ensure LLMs are safe, reliable, and trustworthy for complex AI applications [486]. Multi-Modality: Multi-modal learning, where LLMs are'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='essential to ensure LLMs are safe, reliable, and trustworthy for complex AI applications [486]. Multi-Modality: Multi-modal learning, where LLMs are trained on diverse data like text, images, and videos, aims to create models with richer understanding but faces challenges in data alignment, fusion strategies, and higher computational demands. Catastrophic Forgetting: LLMs are often pre-trained on large datasets and then fine-tuned on domain-specific data, reducing training resources. However,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Forgetting: LLMs are often pre-trained on large datasets and then fine-tuned on domain-specific data, reducing training resources. However, they face issues like domain adaptation and catastrophic forgetting, which hinder the retention of original knowledge when learning new tasks. Adversarial Robustness: Large Language Models (LLMs) have shown great capabilities in various tasks but are vul- nerable to adversarial attacks, where slight, deliberate input alterations can mislead them. Especially'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='capabilities in various tasks but are vul- nerable to adversarial attacks, where slight, deliberate input alterations can mislead them. Especially with models like BERT, adversarial fine-tuning can enhance robustness, al- though it sometimes compromises generalization [487]. As LLMs integrate more into complex systems, examining their security properties becomes crucial, given the emerging field of adversarial attacks on LLMs within trustworthy ML [488]. This vulnerability is notable in'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='properties becomes crucial, given the emerging field of adversarial attacks on LLMs within trustworthy ML [488]. This vulnerability is notable in safety-critical domains, ne- cessitating robust adversarial evaluation tools to ensure LLM reliability [489]. Interpretability and Explainability: The “black-box” nature of LLMs poses challenges in understanding their decision- making, which is crucial for broader acceptance and trust,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='especially in sensitive domains. Despite their advanced capabilities, the lack of insight into their operation limits their effectiveness and trustworthiness [490, 491]. Efforts are being made to make LLMs more explainable to promote user trust and to ensure responsible AI usage. Understanding the logic behind LLMs’ responses is essential for fostering trust and ensuring they align with human values and legal standards. Privacy Concerns: Privacy concerns in Large Language Models (LLMs) have'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='trust and ensuring they align with human values and legal standards. Privacy Concerns: Privacy concerns in Large Language Models (LLMs) have escalated with their growth in complexity and size, particularly around data sharing and potential misuse. There is a risk of malicious content creation, filter bypass, and data privacy issues, especially in e-commerce, where If models are trained protecting customer privacy is crucial. on private data, additional concerns arise if such models are made'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='in e-commerce, where If models are trained protecting customer privacy is crucial. on private data, additional concerns arise if such models are made publicly available. LLMs tend to memorize phrases from their training sets, which an adversary could exploit to extract sensitive data, posing a threat to personal privacy [492, 493]. Real-Time Processing: Real-time processing in Large Lan- guage Models (LLMs) is pivotal for various applications, especially with the rising popularity of mobile AI'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Real-time processing in Large Lan- guage Models (LLMs) is pivotal for various applications, especially with the rising popularity of mobile AI applications and concerns regarding information security and privacy. However, LLMs often have hundreds of layers and millions of parameters, which impede real-time processing due to the high computational demands and limited weight storage on hardware platforms, particularly in edge computing environ- ments [494]. While certain efforts like MobileBERT'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='demands and limited weight storage on hardware platforms, particularly in edge computing environ- ments [494]. While certain efforts like MobileBERT aim to reduce memory requirements, they still face substantial execution overhead due to the large number of model layers, leading to high inference latency. Long-Term Dependencies: Large Language Models have shown considerable progress in understanding and generating text, yet they often struggle with preserving context and handling long-term'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Models have shown considerable progress in understanding and generating text, yet they often struggle with preserving context and handling long-term dependencies, particularly in complex, multi-turn conversations or long documents. This limitation can lead to incoherent or irrelevant responses. Hardware Acceleration: The growth of LLMs presents signif- icant hardware challenges due to the increasing computational and memory demands associated with training and deploying these models. GPUs have'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='icant hardware challenges due to the increasing computational and memory demands associated with training and deploying these models. GPUs have played a crucial role in meeting the hardware requirements for training LLMs, with the networking industry also evolving to optimize hardware for training workloads. However, the growing size of LLMs, which has been outpacing hardware progress, makes model inference in- creasingly costly. Model quantization is a promising approach to bridge the widening'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='has been outpacing hardware progress, makes model inference in- creasingly costly. Model quantization is a promising approach to bridge the widening gap between LLM size and hardware capacity [495]. Although specialized hardware acceleration like GPUs or TPUs can significantly reduce the computational cost, making real-time applications more feasible, they may not fully resolve all limitations, necessitating further advancements in hardware technology. Regulatory and Ethical Frameworks: The'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='they may not fully resolve all limitations, necessitating further advancements in hardware technology. Regulatory and Ethical Frameworks: The rapid advancements in artificial intelligence have given rise to sophisticated Large Language Models (LLMs) like OpenAI’s GPT-4 [157] and Google’s Bard. These developments underscore the imperative to manage the ethical and social for regulatory oversight challenges accompanying LLMs’ widespread use [496]. For instance, LLMs can generate content that can'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='the ethical and social for regulatory oversight challenges accompanying LLMs’ widespread use [496]. For instance, LLMs can generate content that can be used posi-'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='35\\n\\ntively or negatively, emphasizing the need for proactive ethical frameworks and policy measures to guide their responsible use and assign accountability for their outputs [497]. Auditing is identified as a promising governance mechanism to ensure that AI systems, including LLMs, are designed and deployed ethically, legally, and technically robust [498].\\n\\n8. Conclusion'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='This article has comprehensively reviewed the develop- ments in LLMs. It contributes to summarizing significant findings of LLMs in the existing literature and provides a detailed analysis of the design aspects, including architec- tures, datasets, and training pipelines. We identified crucial architectural components and training strategies employed by different LLMs. These aspects are presented as summaries and discussions throughout the article. Moreover, we have discussed the performance'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='by different LLMs. These aspects are presented as summaries and discussions throughout the article. Moreover, we have discussed the performance differences of LLMs in zero-shot and few-shot settings, explored the impact of fine-tuning, and compared supervised and generalized models and encoder vs. decoder vs. encoder-decoder architectures. A comprehensive review of multi-modal LLMs, retrieval augmented LLMs, LLMs-powered agents, efficient LLMs, datasets, evaluation, applications, and challenges'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='review of multi-modal LLMs, retrieval augmented LLMs, LLMs-powered agents, efficient LLMs, datasets, evaluation, applications, and challenges is also provided. This article is anticipated to serve as a valuable resource for researchers, offering insights into the recent advancements in LLMs and providing fundamental concepts and details to develop better LLMs.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='Acknowledgement: The author/s would like to acknowl- edge the support received from Saudi Data and AI Authority (SDAIA) and King Fahd University of Petroleum and Miner- als (KFUPM) under SDAIA-KFUPM Joint Research Center for Artificial Intelligence Grant No. JRC-AI-RFP-11.\\n\\nReferences'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='References\\n\\n[1] A. Chernyavskiy, D. Ilvovsky, P. Nakov, Transformers:“the end of his- tory” for natural language processing?, in: Machine Learning and Knowledge Discovery in Databases. Research Track: European Con- ference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021, Proceedings, Part III 21, Springer, 2021, pp. 677–693. 1'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[2] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, S. Bowman, Superglue: A stickier benchmark for general- purpose language understanding systems, Advances in neural informa- tion processing systems 32 (2019). 1, 26, 29\\n\\n[3] D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thoppilan, Z. Yang, A. Kulshreshtha, G. Nemade, Y. Lu, et al., Towards a human- like open-domain chatbot, arXiv preprint arXiv:2001.09977 (2020). 1'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[4] B. A. y Arcas, Do large language models understand us?, Daedalus\\n\\n151 (2) (2022) 183–197. 2\\n\\n[5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al., Language models are unsupervised multitask learners, OpenAI blog 1 (8) (2019) 9. 2, 7'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models are few-shot learners, Advances in neural information processing sys- tems 33 (2020) 1877–1901. 2, 6, 7, 8, 9, 16, 18, 23, 24, 25, 34\\n\\n[7] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805 (2018). 2, 18, 24'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[8] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, L. Zettlemoyer, Deep contextualized word representations, in: NAACL- HLT, Association for Computational Linguistics, 2018, pp. 2227–2237. 2\\n\\n[9] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, L. Zettlemoyer, Bart: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehen- sion, arXiv preprint arXiv:1910.13461 (2019). 2'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[10] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, P. J. Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, The Journal of Machine Learning Re- search 21 (1) (2020) 5485–5551. 2, 7, 8, 18, 19, 24, 25, 28, 30, 31 [11] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, C. Raffel, mt5: A massively multilingual pre-trained text-to- text transformer, arXiv preprint arXiv:2010.11934 (2020). 2, 7, 8, 24,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='A. Barua, C. Raffel, mt5: A massively multilingual pre-trained text-to- text transformer, arXiv preprint arXiv:2010.11934 (2020). 2, 7, 8, 24, 25, 28, 30'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[12] Z. Zhang, Y. Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y. Yao, F. Qi, J. Guan, P. Ke, et al., Cpm-2: Large-scale cost-effective pre-trained lan- guage models, AI Open 2 (2021) 216–224. 2, 8, 25\\n\\n[13] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili´c, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al., Bloom: A 176b- parameter open-access multilingual language model, arXiv preprint arXiv:2211.05100 (2022). 2, 4, 9, 11, 23, 24, 25, 30'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[14] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al., Opt: Open pre-trained transformer language models, arXiv preprint arXiv:2205.01068 (2022). 2, 9, 11, 24, 25\\n\\n[15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al., Palm: Scal- ing language modeling with pathways, arXiv preprint arXiv:2204.02311 (2022). 2, 6, 9, 11, 23, 24, 25'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[16] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, et al., Scaling instruction-finetuned language models, arXiv preprint arXiv:2210.11416 (2022). 2, 7, 11, 16, 17, 22, 24, 25, 28, 31\\n\\n[17] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja, et al., Multitask prompted training enables zero-shot task generalization, arXiv preprint arXiv:2110.08207 (2021). 2, 11, 16, 25, 28, 31'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[18] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, et al., Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks, in: Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 2022, pp. 5085–5109. 2, 7, 11, 16, 17, 24, 25, 28, 31'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[19] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, H. Ha- jishirzi, Self-instruct: Aligning language model with self generated in- structions, arXiv preprint arXiv:2212.10560 (2022). 2, 16, 19, 22, 28'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[20] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al., Training language mod- els to follow instructions with human feedback, Advances in Neural In- formation Processing Systems 35 (2022) 27730–27744. 2, 7, 11, 16, 22'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[21] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288 (2023). 2, 7, 10, 16, 25, 34'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[22] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yo- gatama, M. Bosma, D. Zhou, D. Metzler, et al., Emergent abilities of large language models, arXiv preprint arXiv:2206.07682 (2022). 2 [23] T. Webb, K. J. Holyoak, H. Lu, Emergent analogical reasoning in large language models, Nature Human Behaviour 7 (9) (2023) 1526–1541. 2 [24] D. A. Boiko, R. MacKnight, G. Gomes, Emergent autonomous sci- entific research capabilities of large language models, arXiv preprint'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='2 [24] D. A. Boiko, R. MacKnight, G. Gomes, Emergent autonomous sci- entific research capabilities of large language models, arXiv preprint arXiv:2304.05332 (2023). 2'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[25] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, E. Grave, Few-shot learning with retrieval augmented language models, arXiv preprint arXiv:2208.03299 (2022). 2, 18, 19, 34\\n\\n[26] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al., Palm-e: An embodied\\n\\n36\\n\\nmultimodal language model, arXiv preprint arXiv:2303.03378 (2023). 2, 20, 22, 33'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='36\\n\\nmultimodal language model, arXiv preprint arXiv:2303.03378 (2023). 2, 20, 22, 33\\n\\n[27] A. Parisi, Y. Zhao, N. Fiedel, Talm: Tool augmented language models,\\n\\narXiv preprint arXiv:2205.12255 (2022). 2, 19, 20\\n\\n[28] B. Zhang, H. Soh, Large language models as zero-shot human models for human-robot interaction, arXiv preprint arXiv:2303.03548 (2023). 2, 33'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[28] B. Zhang, H. Soh, Large language models as zero-shot human models for human-robot interaction, arXiv preprint arXiv:2303.03548 (2023). 2, 33\\n\\n[29] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, et al., mplug-owl: Modularization empowers large language models with multimodality, arXiv preprint arXiv:2304.14178 (2023). 2, 22'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[30] W. Wang, Z. Chen, X. Chen, J. Wu, X. Zhu, G. Zeng, P. Luo, T. Lu, J. Zhou, Y. Qiao, et al., Visionllm: Large language model is also an open-ended decoder for vision-centric tasks, arXiv preprint arXiv:2305.11175 (2023). 2, 22\\n\\n[31] R. Yang, L. Song, Y. Li, S. Zhao, Y. Ge, X. Li, Y. Shan, Gpt4tools: Teaching large language model to use tools via self-instruction, arXiv preprint arXiv:2305.18752 (2023). 2, 19, 22, 23\\n\\n[32] E. Saravia, Prompt Engineering Guide,\\n\\nhttps://github.com/dair-'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[32] E. Saravia, Prompt Engineering Guide,\\n\\nhttps://github.com/dair-\\n\\nai/Prompt-Engineering-Guide (12 2022). 2, 7, 18, 34'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='https://github.com/dair-\\n\\nai/Prompt-Engineering-Guide (12 2022). 2, 7, 18, 34\\n\\n[33] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, et al., Glm-130b: An open bilingual pre-trained model, arXiv preprint arXiv:2210.02414 (2022). 2, 10, 23, 24, 25 [34] Y. Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, S. C. Hoi, Codet5+: Open code large language models for code understanding and genera- tion, arXiv preprint arXiv:2305.07922 (2023). 2, 11, 24, 25'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[35] S. Wang, Y. Sun, Y. Xiang, Z. Wu, S. Ding, W. Gong, S. Feng, J. Shang, Y. Zhao, C. Pang, et al., Ernie 3.0 titan: Exploring larger-scale knowl- edge enhanced pre-training for language understanding and generation, arXiv preprint arXiv:2112.12731 (2021). 2, 8, 24, 25'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[36] J. Rasley, S. Rajbhandari, O. Ruwase, Y. He, Deepspeed: System op- timizations enable training deep learning models with over 100 billion parameters, in: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 3505– 3506. 2, 5'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[37] S. Rajbhandari, J. Rasley, O. Ruwase, Y. He, Zero: Memory optimiza- tions toward training trillion parameter models, in: SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, IEEE, 2020, pp. 1–16. 2, 4, 24\\n\\n[38] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, G. Neubig, Towards a unified view of parameter-efficient transfer learning, arXiv preprint arXiv:2110.04366 (2021). 2, 20, 21'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[39] Z. Hu, Y. Lan, L. Wang, W. Xu, E.-P. Lim, R. K.-W. Lee, L. Bing, S. Po- ria, Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models, arXiv preprint arXiv:2304.01933 (2023). 2, 20\\n\\n[40] B. Lester, R. Al-Rfou, N. Constant, The power of scale for parameter- efficient prompt tuning, arXiv preprint arXiv:2104.08691 (2021). 2, 8, 20, 21\\n\\n[41] X. L. Li, P. Liang, Prefix-tuning: Optimizing continuous prompts for'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[41] X. L. Li, P. Liang, Prefix-tuning: Optimizing continuous prompts for\\n\\ngeneration, arXiv preprint arXiv:2101.00190 (2021). 2, 20, 21\\n\\n[42] X. Ma, G. Fang, X. Wang, Llm-pruner: On the structural pruning of\\n\\nlarge language models, arXiv preprint arXiv:2305.11627 (2023). 2, 22'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[42] X. Ma, G. Fang, X. Wang, Llm-pruner: On the structural pruning of\\n\\nlarge language models, arXiv preprint arXiv:2305.11627 (2023). 2, 22\\n\\n[43] R. Xu, F. Luo, C. Wang, B. Chang, J. Huang, S. Huang, F. Huang, From dense to sparse: Contrastive pruning for better pre-trained lan- guage model compression, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36, 2022, pp. 11547–11555. 2, 22'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[44] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, S. Han, Smoothquant: Accurate and efficient post-training quantization for large language models, in: ICML, Vol. 202 of Proceedings of Machine Learning Re- search, PMLR, 2023, pp. 38087–38099. 2, 21\\n\\n[45] C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo, N. Wong, Compression of generative pre-trained language models via quantiza- tion, arXiv preprint arXiv:2203.10705 (2022). 2, 21'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[46] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, S. Naidu, Giraffe: Adventures in expanding context lengths in llms, arXiv preprint arXiv:2308.10882 (2023). 2, 17\\n\\n[47] B. Peng, J. Quesnelle, H. Fan, E. Shippole, Yarn: Efficient con- text window extension of large language models, arXiv preprint arXiv:2309.00071 (2023). 2, 17\\n\\n[48] M. Guo, J. Ainslie, D. Uthus, S. Ontanon, J. Ni, Y.-H. Sung, Y. Yang,'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[48] M. Guo, J. Ainslie, D. Uthus, S. Ontanon, J. Ni, Y.-H. Sung, Y. Yang,\\n\\nLongt5: Efficient text-to-text transformer for long sequences, arXiv preprint arXiv:2112.07916 (2021). 2, 18\\n\\n[49] S. Chen, S. Wong, L. Chen, Y. Tian, Extending context window of large language models via positional interpolation, arXiv preprint arXiv:2306.15595 (2023). 2, 17'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[50] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, et al., A survey of large language models, arXiv preprint arXiv:2303.18223 (2023). 2, 3, 7\\n\\n[51] U. Naseem, I. Razzak, S. K. Khan, M. Prasad, A comprehensive sur- vey on word representation models: From classical to state-of-the-art word representation language models, Transactions on Asian and Low- Resource Language Information Processing 20 (5) (2021) 1–35. 2, 3'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[52] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz, E. Agirre, I. Heinz, D. Roth, Recent advances in natural language pro- cessing via large pre-trained language models: A survey, arXiv preprint arXiv:2111.01243 (2021). 2, 3\\n\\n[53] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan, L. He, et al., A comprehensive survey on pretrained foundation models: A history from bert to chatgpt, arXiv preprint arXiv:2302.09419 (2023). 2, 3'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[54] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, learning, arXiv preprint\\n\\nJ. Xu, Z. Sui, A survey for in-context arXiv:2301.00234 (2022). 2, 7, 18\\n\\n[55] J. Huang, K. C.-C. Chang, Towards reasoning in large language models:\\n\\nA survey, arXiv preprint arXiv:2212.10403 (2022). 2, 7, 18\\n\\n[56] Y. Wang, W. Zhong, L. Li, F. Mi, X. Zeng, W. Huang, L. Shang, X. Jiang, Q. Liu, Aligning large language models with human: A survey, arXiv preprint arXiv:2307.12966 (2023). 2'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[57] X. Zhu, J. Li, Y. Liu, C. Ma, W. Wang, A survey on model compression\\n\\nfor large language models, arXiv preprint arXiv:2308.07633 (2023). 2\\n\\n[58] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, E. Chen, A survey on multi- modal large language models, arXiv preprint arXiv:2306.13549 (2023). 2, 22, 23\\n\\n[59] J. J. Webster, C. Kit, Tokenization as the initial phase in nlp, in: COL- ING 1992 volume 4: The 14th international conference on computa- tional linguistics, 1992. 4'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[60] T. Kudo, Subword regularization: Improving neural network translation models with multiple subword candidates, in: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), 2018, pp. 66–75. 4\\n\\n[61] R. Sennrich, B. Haddow, A. Birch, Neural machine translation of rare words with subword units, in: Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), 2016, pp. 1715–1725. 4'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[62] M. Schuster, K. Nakajima, Japanese and korean voice search, in: 2012 IEEE international conference on acoustics, speech and signal process- ing (ICASSP), IEEE, 2012, pp. 5149–5152. 4\\n\\n[63] S. J. Mielke, Z. Alyafeai, E. Salesky, C. Raffel, M. Dey, M. Gallé, A. Raja, C. Si, W. Y. Lee, B. Sagot, et al., Between words and char- acters: A brief history of open-vocabulary modeling and tokenization in nlp, arXiv preprint arXiv:2112.10508 (2021). 4'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[64] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural information processing systems 30 (2017). 4, 7'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[65] O. Press, N. Smith, M. Lewis, Train short, test long: Attention with linear biases enables input length extrapolation, in: International Con- ference on Learning Representations, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0 4, 17 [66] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, Y. Liu, Roformer: En- hanced transformer with rotary position embedding, arXiv preprint arXiv:2104.09864 (2021). 4, 9, 17'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[67] R. Child, S. Gray, A. Radford, I. Sutskever, Generating long sequences with sparse transformers, arXiv preprint arXiv:1904.10509 (2019). 4, 7, 23\\n\\n[68] T. Dao, D. Fu, S. Ermon, A. Rudra, C. Ré, Flashattention: Fast and memory-efficient exact attention with io-awareness, Advances in Neural Information Processing Systems 35 (2022) 16344–16359. 4\\n\\n[69] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks\\n\\nare universal approximators, Neural networks 2 (5) (1989) 359–366. 4'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[69] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks\\n\\nare universal approximators, Neural networks 2 (5) (1989) 359–366. 4\\n\\n[70] V. Nair, G. E. Hinton, Rectified linear units improve restricted boltz- mann machines, in: Proceedings of the 27th international conference on\\n\\n37\\n\\nmachine learning (ICML-10), 2010, pp. 807–814. 4\\n\\n[71] D. Hendrycks, K. Gimpel, Gaussian error linear units (gelus), arXiv\\n\\npreprint arXiv:1606.08415 (2016). 4'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[72] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov, Dropout: a simple way to prevent neural networks from overfitting, The journal of machine learning research 15 (1) (2014) 1929–1958. 4 [73] D. Krueger, T. Maharaj, J. Kramár, M. Pezeshki, N. Ballas, N. R. Ke, A. Goyal, Y. Bengio, A. Courville, C. Pal, Zoneout: Regular- izing rnns by randomly preserving hidden activations, arXiv preprint arXiv:1606.01305 (2016). 4 [74] N. Shazeer, Glu variants arXiv:2002.05202 (2020).'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='rnns by randomly preserving hidden activations, arXiv preprint arXiv:1606.01305 (2016). 4 [74] N. Shazeer, Glu variants arXiv:2002.05202 (2020). 4'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='improve transformer,\\n\\narXiv preprint\\n\\n[75] Y. N. Dauphin, A. Fan, M. Auli, D. Grangier, Language modeling with gated convolutional networks, in: International conference on machine learning, PMLR, 2017, pp. 933–941. 4\\n\\n[76] J. L. Ba, J. R. Kiros, G. E. Hinton, Layer normalization, arXiv preprint\\n\\narXiv:1607.06450 (2016). 4\\n\\n[77] B. Zhang, R. Sennrich, Root mean square layer normalization, Advances\\n\\nin Neural Information Processing Systems 32 (2019). 4'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[77] B. Zhang, R. Sennrich, Root mean square layer normalization, Advances\\n\\nin Neural Information Processing Systems 32 (2019). 4\\n\\n[78] A. Baevski, M. Auli, Adaptive input representations for neural language\\n\\nmodeling, arXiv preprint arXiv:1809.10853 (2018). 4'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[78] A. Baevski, M. Auli, Adaptive input representations for neural language\\n\\nmodeling, arXiv preprint arXiv:1809.10853 (2018). 4\\n\\n[79] H. Wang, S. Ma, L. Dong, S. Huang, D. Zhang, F. Wei, Deepnet: Scaling transformers to 1,000 layers, arXiv preprint arXiv:2203.00555 (2022). 4 [80] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro, Megatron-lm: Training multi-billion parameter language models using model parallelism, arXiv preprint arXiv:1909.08053 (2019). 4, 5'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[81] \"bmtrain: Efficient training for big models.\".\\n\\nURL https://github.com/OpenBMB/BMTrain 4, 5\\n\\n[82] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cis- tac, T. Rault, R. Louf, M. Funtowicz, et al., Transformers: State-of-the- art natural language processing, in: Proceedings of the 2020 conference on empirical methods in natural language processing: system demon- strations, 2020, pp. 38–45. 5'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[83] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclau- rin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, et al., Jax: composable transformations of python+ numpy programs (2018). 5\\n\\n[84] S. Li, J. Fang, Z. Bian, H. Liu, Y. Liu, H. Huang, B. Wang, Y. You, Colossal-ai: A unified deep learning system for large-scale parallel train- ing, arXiv preprint arXiv:2110.14883 (2021). 5'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[85] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, J. Tang, Fastmoe: A fast mixture-of-expert training system, arXiv preprint arXiv:2103.13262 (2021). 5\\n\\n[86] L. Huawei Technologies Co., Huawei mindspore ai development frame- work, in: Artificial Intelligence Technology, Springer, 2022, pp. 137– 162. 5'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[87] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., Pytorch: An imper- ative style, high-performance deep learning library, Advances in neural information processing systems 32 (2019). 5\\n\\n[88] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, et al., Tensorflow: a system for large- scale machine learning., in: Osdi, Vol. 16, Savannah, GA, USA, 2016, pp. 265–283. 5'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[89] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, Z. Zhang, Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems, arXiv preprint arXiv:1512.01274 (2015). 5\\n\\n[90] W. Fedus, B. Zoph, N. Shazeer, Switch transformers: Scaling to tril- lion parameter models with simple and efficient sparsity, The Journal of Machine Learning Research 23 (1) (2022) 5232–5270. 5, 9'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[91] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, et al., Glam: Efficient scaling of language models with mixture-of-experts, in: International Conference on Ma- chine Learning, PMLR, 2022, pp. 5547–5569. 5, 9, 23, 24, 25'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[92] X. Ren, P. Zhou, X. Meng, X. Huang, Y. Wang, W. Wang, P. Li, X. Zhang, A. Podolskiy, G. Arshinov, et al., Pangu-(cid:80): Towards trillion parameter language model with sparse heterogeneous computing, arXiv preprint arXiv:2303.10845 (2023). 5, 10, 16, 23, 24, 25\\n\\n[93] T. Wang, A. Roberts, D. Hesslow, T. Le Scao, H. W. Chung, I. Beltagy, J. Launay, C. Raffel, What language model architecture and pretrain-'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[93] T. Wang, A. Roberts, D. Hesslow, T. Le Scao, H. W. Chung, I. Beltagy, J. Launay, C. Raffel, What language model architecture and pretrain-\\n\\ning objective works best for zero-shot generalization?, in: International Conference on Machine Learning, PMLR, 2022, pp. 22964–22984. 5'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='ing objective works best for zero-shot generalization?, in: International Conference on Machine Learning, PMLR, 2022, pp. 22964–22984. 5\\n\\n[94] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, H.-W. Hon, Unified language model pre-training for natural language understanding and generation, Advances in neural information process- ing systems 32 (2019). 6'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[95] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, D. Amodei, Scaling laws for neural language models, arXiv preprint arXiv:2001.08361 (2020). 6\\n\\n[96] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al., Training compute-optimal large language models, arXiv preprint arXiv:2203.15556 (2022). 6, 9, 25, 29'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[97] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura, et al., Opt-iml: Scaling language model in- struction meta learning through the lens of generalization, arXiv preprint arXiv:2212.12017 (2022). 7, 11, 16, 17, 22, 25, 28\\n\\n[98] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, C. Gan, Principle-driven self-alignment of language models from scratch with minimal human supervision, arXiv preprint arXiv:2305.03047 (2023). 7, 17'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[99] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, et al., A general language assistant as a laboratory for alignment, arXiv preprint arXiv:2112.00861 (2021). 7\\n\\n[100] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, G. Irving, Fine-tuning language models from human pref- erences, arXiv preprint arXiv:1909.08593 (2019). 7'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[101] S. Kim, S. J. Joo, D. Kim, J. Jang, S. Ye, J. Shin, M. Seo, The cot collec- tion: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning, arXiv preprint arXiv:2305.14045 (2023). 7, 16\\n\\n[102] Q. Liu, F. Zhou, Z. Jiang, L. Dou, M. Lin, From zero to hero: Exam- ining the power of symbolic tasks in instruction tuning, arXiv preprint arXiv:2304.07995 (2023). 7, 16'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[103] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al., Chain-of-thought prompting elicits reasoning in large language models, Advances in Neural Information Processing Systems 35 (2022) 24824–24837. 7, 20, 23'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[104] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowd- hery, D. Zhou, Self-consistency improves chain of thought reasoning in language models, arXiv preprint arXiv:2203.11171 (2022). 7, 20 [105] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, K. Narasimhan, Tree of thoughts: Deliberate problem solving with large language mod- els, arXiv preprint arXiv:2305.10601 (2023). 7, 20'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[106] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, S. Gelly, Parameter-efficient transfer learn- ing for nlp, in: International Conference on Machine Learning, PMLR, 2019, pp. 2790–2799. 7, 20'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[107] S. McCandlish, J. Kaplan, D. Amodei, O. D. Team, An empirical model of large-batch training, arXiv preprint arXiv:1812.06162 (2018). 7 [108] W. Zeng, X. Ren, T. Su, H. Wang, Y. Liao, Z. Wang, X. Jiang, Z. Yang, K. Wang, X. Zhang, et al., Pangu-α : Large-scale autoregressive pre- trained chinese language models with auto-parallel computation, arXiv preprint arXiv:2104.12369 (2021). 8, 23, 24, 25'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[109] S. Yuan, H. Zhao, Z. Du, M. Ding, X. Liu, Y. Cen, X. Zou, Z. Yang, J. Tang, Wudaocorpora: A super large-scale chinese corpora for pre- training language models, AI Open 2 (2021) 65–68. 8, 30\\n\\n[110] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen, Y. Zhao, Y. Lu, et al., Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation, arXiv preprint arXiv:2107.02137 (2021). 8, 25'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[111] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, R. Salakhutdinov, Transformer-xl: Attentive language models beyond a fixed-length con- text, arXiv preprint arXiv:1901.02860 (2019). 8\\n\\n[112] O. Lieber, O. Sharir, B. Lenz, Y. Shoham, Jurassic-1: Technical details\\n\\nand evaluation, White Paper. AI21 Labs 1 (2021). 8, 24, 25'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[112] O. Lieber, O. Sharir, B. Lenz, Y. Shoham, Jurassic-1: Technical details\\n\\nand evaluation, White Paper. AI21 Labs 1 (2021). 8, 24, 25\\n\\n[113] Y. Levine, N. Wies, O. Sharir, H. Bata, A. Shashua, Limits to depth ef- ficiencies of self-attention, Advances in Neural Information Processing Systems 33 (2020) 22640–22651. 8, 11\\n\\n[114] B. Kim, H. Kim, S.-W. Lee, G. Lee, D. Kwak, D. H. Jeon, S. Park,\\n\\n38'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[114] B. Kim, H. Kim, S.-W. Lee, G. Lee, D. Kwak, D. H. Jeon, S. Park,\\n\\n38\\n\\nS. Kim, S. Kim, D. Seo, et al., What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers, arXiv preprint arXiv:2109.04650 (2021). 8, 25'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[115] S. Wu, X. Zhao, T. Yu, R. Zhang, C. Shen, H. Liu, F. Li, H. Zhu, J. Luo, L. Xu, et al., Yuan 1.0: Large-scale pre-trained language model in zero- shot and few-shot learning, arXiv preprint arXiv:2110.04725 (2021). 8, 24, 25\\n\\n[116] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, et al., Scaling lan- guage models: Methods, analysis & insights from training gopher, arXiv preprint arXiv:2112.11446 (2021). 8, 9, 25, 28'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[117] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti, et al., Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model, arXiv preprint arXiv:2201.11990 (2022). 8, 9, 24, 25'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[118] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K. McDonell, J. Phang, et al., Gpt-neox-20b: An open- source autoregressive language model, arXiv preprint arXiv:2204.06745 (2022). 9, 23, 24, 25\\n\\n[119] W. Ben, K. Aran, Gpt-j-6b: A 6 billion parameter autoregressive lan-\\n\\nguage model (2021). 9'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[120] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, et al., Mixed pre- cision training, arXiv preprint arXiv:1710.03740 (2017). 9, 23 [121] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hin- ton, J. Dean, Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, arXiv preprint arXiv:1701.06538 (2017). 9, 23 [122] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza, H.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='mixture-of-experts layer, arXiv preprint arXiv:1701.06538 (2017). 9, 23 [122] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza, H. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky, et al., Alex- atm 20b: Few-shot learning using a large-scale multilingual seq2seq model, arXiv preprint arXiv:2208.01448 (2022). 9, 23, 24, 25'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[123] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al., Palm 2 technical report, arXiv preprint arXiv:2305.10403 (2023). 9, 25\\n\\n[124] Y. Tay, J. Wei, H. W. Chung, V. Q. Tran, D. R. So, S. Shakeri, X. Garcia, H. S. Zheng, J. Rao, A. Chowdhery, et al., Transcending scaling laws with 0.1% extra compute, arXiv preprint arXiv:2210.11399 (2022). 9, 24, 25'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[125] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W. Chung, D. Bahri, T. Schuster, S. Zheng, et al., Ul2: Unifying lan- guage learning paradigms, in: The Eleventh International Conference on Learning Representations, 2022. 9, 10, 24, 25'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[126] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, J. Tang, Glm: Gen- eral language model pretraining with autoregressive blank infilling, in: Proceedings of the 60th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), 2022, pp. 320–335. 10'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[127] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al., Llama: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971 (2023). 10, 23, 25\\n\\n[128] M. N. Rabe, C. Staats, Self-attention does not need o(n2) memory, arXiv\\n\\npreprint arXiv:2112.05682 (2021). 10'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[128] M. N. Rabe, C. Staats, Self-attention does not need o(n2) memory, arXiv\\n\\npreprint arXiv:2112.05682 (2021). 10\\n\\n[129] V. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch, M. Shoeybi, B. Catanzaro, Reducing activation recomputation in large transformer models, Proceedings of Machine Learning and Systems 5 (2023). 10'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[130] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al., The llama 3 herd of models, arXiv preprint arXiv:2407.21783 (2024). 10, 25\\n\\n[131] https://mistral.ai/news/mixtral-8x22b/. 10, 25 [132] https://github.com/Snowflake-Labs/snowflake-arctic. 10,\\n\\n25'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[131] https://mistral.ai/news/mixtral-8x22b/. 10, 25 [132] https://github.com/Snowflake-Labs/snowflake-arctic. 10,\\n\\n25\\n\\n[133] https://github.com/xai-org/grok-1. 10 [134] https://x.ai/blog/grok-1.5. 10 [135] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al., Gemini: a family of highly capable multimodal models, arXiv preprint arXiv:2312.11805 (2023). 10\\n\\n[136] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[136] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b.\\n\\nAlayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al., Gem- ini 1.5: Unlocking multimodal understanding across millions of tokens of context, arXiv preprint arXiv:2403.05530 (2024). 10'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[137] B. Adler, N. Agarwal, A. Aithal, D. H. Anh, P. Bhattacharya, A. Brun- dyn, J. Casper, B. Catanzaro, S. Clay, J. Cohen, et al., Nemotron-4 340b technical report, arXiv preprint arXiv:2406.11704 (2024). 10, 25 [138] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, et al., Deepseek llm: Scaling open-source language models with longtermism, arXiv preprint arXiv:2401.02954 (2024). 10, 25 [139] DeepSeek-AI, A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='models with longtermism, arXiv preprint arXiv:2401.02954 (2024). 10, 25 [139] DeepSeek-AI, A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Deng, C. Ruan, D. Dai, D. Guo, D. Yang, D. Chen, D. Ji, E. Li, F. Lin, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Xu, H. Yang, H. Zhang, H. Ding, H. Xin, H. Gao, H. Li, H. Qu, J. L. Cai, J. Liang, J. Guo, J. Ni, J. Li, J. Chen, J. Yuan, J. Qiu, J. Song, K. Dong, K. Gao, K. Guan, L. Wang, L. Zhang, L. Xu, L. Xia, L. Zhao, L. Zhang, M. Li, M. Wang, M.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='J. Ni, J. Li, J. Chen, J. Yuan, J. Qiu, J. Song, K. Dong, K. Gao, K. Guan, L. Wang, L. Zhang, L. Xu, L. Xia, L. Zhao, L. Zhang, M. Li, M. Wang, M. Zhang, M. Zhang, M. Tang, M. Li, N. Tian, P. Huang, P. Wang, P. Zhang, Q. Zhu, Q. Chen, Q. Du, R. J. Chen, R. L. Jin, R. Ge, R. Pan, R. Xu, R. Chen, S. S. Li, S. Lu, S. Zhou, S. Chen, S. Wu, S. Ye, S. Ma, S. Wang, S. Zhou, S. Yu, S. Zhou, S. Zheng, T. Wang, T. Pei, T. Yuan, T. Sun, W. L. Xiao, W. Zeng, W. An, W. Liu, W. Liang, W. Gao, W. Zhang, X. Q.'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='S. Wang, S. Zhou, S. Yu, S. Zhou, S. Zheng, T. Wang, T. Pei, T. Yuan, T. Sun, W. L. Xiao, W. Zeng, W. An, W. Liu, W. Liang, W. Gao, W. Zhang, X. Q. Li, X. Jin, X. Wang, X. Bi, X. Liu, X. Wang, X. Shen, X. Chen, X. Chen, X. Nie, X. Sun, Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model, CoRR abs/2405.04434 (2024). 10, 25'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[140] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, C. Xiong, Codegen: An open large language model for code with multi- turn program synthesis, arXiv preprint arXiv:2203.13474 (2022). 11, 23, 25, 28\\n\\n[141] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Ed- wards, Y. Burda, N. Joseph, G. Brockman, et al., Evaluating large lan- guage models trained on code, arXiv preprint arXiv:2107.03374 (2021). 11, 25, 29, 31'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[142] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al., Competition-level code generation with alphacode, Science 378 (6624) (2022) 1092–1097. 11, 23, 25, 29\\n\\n[143] N. Shazeer, Fast transformer decoding: One write-head is all you need,\\n\\narXiv preprint arXiv:1911.02150 (2019). 11\\n\\n[144] R. Y. Pang, H. He, Text generation by learning from demonstrations,\\n\\narXiv preprint arXiv:2009.07839 (2020). 11'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[144] R. Y. Pang, H. He, Text generation by learning from demonstrations,\\n\\narXiv preprint arXiv:2009.07839 (2020). 11\\n\\n[145] R. Dabre, A. Fujita, Softmax tempering for training neural machine translation models, arXiv preprint arXiv:2009.09372 (2020). 11 [146] Y. Wang, W. Wang, S. Joty, S. C. Hoi, Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and genera- tion, arXiv preprint arXiv:2109.00859 (2021). 11'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[147] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, et al., Starcoder: may the source be with you!, arXiv preprint arXiv:2305.06161 (2023). 11, 25'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[148] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia, A. Poulton, V. Kerkez, R. Stojnic, Galactica: A large language model for science, arXiv preprint arXiv:2211.09085 (2022). 11, 24, 25, 29 [149] FairScale authors, Fairscale: A general purpose modular pytorch library for high performance and large scale training, https://github.com/ facebookresearch/fairscale (2021). 11'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[150] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al., Lamda: Language models for dialog applications, arXiv preprint arXiv:2201.08239 (2022). 11, 25 [151] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann, P. Kambadur, D. Rosenberg, G. Mann, Bloomberggpt: A large language model for finance, arXiv preprint arXiv:2303.17564 (2023). 11, 25, 33 [152] X. Zhang, Q. Yang, D. Xu, Xuanyuan 2.0: A large chinese finan-'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='language model for finance, arXiv preprint arXiv:2303.17564 (2023). 11, 25, 33 [152] X. Zhang, Q. Yang, D. Xu, Xuanyuan 2.0: A large chinese finan- cial chat model with hundreds of billions parameters, arXiv preprint arXiv:2305.12002 (2023). 11, 17, 25'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[153] W. Ben, Mesh-transformer-jax: Model-parallel implementation of trans-\\n\\nformer language model with jax (2021). 12, 24\\n\\n[154] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman, T. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf, et al., Crosslingual generalization through multitask finetuning, arXiv preprint arXiv:2211.01786 (2022). 16, 25, 28, 31'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[155] D. Yin, X. Liu, F. Yin, M. Zhong, H. Bansal, J. Han, K.-W. Chang, Dynosaur: A dynamic growth paradigm for instruction-tuning data cu-\\n\\n39\\n\\nration, arXiv preprint arXiv:2305.14327 (2023). 16\\n\\n[156] P. Gao, J. Han, R. Zhang, Z. Lin, S. Geng, A. Zhou, W. Zhang, P. Lu, C. He, X. Yue, et al., Llama-adapter v2: Parameter-efficient visual in- struction model, arXiv preprint arXiv:2304.15010 (2023). 16, 24'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[157] Openai. gpt-4 technical report (2023). 16, 35 [158] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, T. B. Hashimoto, Stanford alpaca: An instruction-following llama model, https://github.com/tatsu-lab/stanford_alpaca (2023). 16, 25, 28'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[159] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, E. P. Xing, Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality (March 2023). URL https://lmsys.org/blog/2023-03-30-vicuna/ 16, 22, 25, 28\\n\\n[160] B. Peng, C. Li, P. He, M. Galley, J. Gao, Instruction tuning with gpt-4,\\n\\narXiv preprint arXiv:2304.03277 (2023). 16, 28\\n\\n[161] T. Liu, B. K. H. Low, Goat: Fine-tuned llama outperforms gpt-4 on'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='arXiv preprint arXiv:2304.03277 (2023). 16, 28\\n\\n[161] T. Liu, B. K. H. Low, Goat: Fine-tuned llama outperforms gpt-4 on\\n\\narithmetic tasks, arXiv preprint arXiv:2305.14201 (2023). 16\\n\\n[162] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, T. Liu, Huatuo: Tuning llama model with chinese medical knowledge, arXiv preprint arXiv:2304.06975 (2023). 16'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[163] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, D. Jiang, Wizardlm: Empowering large language models to follow complex in- structions, arXiv preprint arXiv:2304.12244 (2023). 16\\n\\n[164] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin, D. Jiang, Wizardcoder: Empowering code large language models with evol-instruct, arXiv preprint arXiv:2306.08568 (2023). 16, 25'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[165] J. Menick, M. Trebacz, V. Mikulik, J. Aslanides, F. Song, M. Chadwick, M. Glaese, S. Young, L. Campbell-Gillingham, G. Irving, et al., Teach- ing language models to support answers with verified quotes, arXiv preprint arXiv:2203.11147 (2022). 17 J. Hilton, S. Balaji,\\n\\n[166] R. Nakano,\\n\\nJ. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al., Webgpt: Browser- assisted question-answering with human feedback, arXiv preprint arXiv:2112.09332 (2021). 17, 19, 20, 25, 31'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[167] A. Glaese, N. McAleese, M. Tr˛ebacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al., Improving alignment of dialogue agents via targeted human judgements, arXiv preprint arXiv:2209.14375 (2022). 17, 20, 25\\n\\n[168] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, C. Finn, Direct preference optimization: Your language model is secretly a re- ward model, arXiv preprint arXiv:2305.18290 (2023). 17'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[169] H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum, T. Zhang, Raft: Reward ranked finetuning for generative foundation model alignment, arXiv preprint arXiv:2304.06767 (2023). 17 [170] Z. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, F. Huang, Rrhf: Rank responses to align language models with human feedback without tears, arXiv preprint arXiv:2304.05302 (2023). 17'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[171] F. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, H. Wang, Preference rank- ing optimization for human alignment, arXiv preprint arXiv:2306.17492 (2023). 17\\n\\n[172] H. Liu, C. Sferrazza, P. Abbeel, Languages are rewards: Hindsight fine- tuning using human feedback, arXiv preprint arXiv:2302.02676 (2023). 17'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[173] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al., Constitutional ai: Harm- lessness from ai feedback, arXiv preprint arXiv:2212.08073 (2022). 17 [174] Y. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin, P. Liang, T. B. Hashimoto, Alpacafarm: A simulation frame- work for methods that learn from human feedback, arXiv preprint arXiv:2305.14387 (2023). 17'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[175] C. Si, Z. Gan, Z. Yang, S. Wang, J. Wang, J. Boyd-Graber, L. Wang, Prompting gpt-3 to be reliable, arXiv preprint arXiv:2210.09150 (2022). 17\\n\\n[176] D. Ganguli, A. Askell, N. Schiefer, T. Liao, K. Lukoši¯ut˙e, A. Chen, A. Goldie, A. Mirhoseini, C. Olsson, D. Hernandez, et al., The capac- ity for moral self-correction in large language models, arXiv preprint arXiv:2302.07459 (2023). 17\\n\\n[177] A. Wei, N. Haghtalab, J. Steinhardt, Jailbroken: How does llm safety'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[177] A. Wei, N. Haghtalab, J. Steinhardt, Jailbroken: How does llm safety\\n\\ntraining fail?, arXiv preprint arXiv:2307.02483 (2023). 17\\n\\n[178] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, B. Mann, E. Perez, N. Schiefer, K. Ndousse, et al., Red teaming lan- guage models to reduce harms: Methods, scaling behaviors, and lessons learned, arXiv preprint arXiv:2209.07858 (2022). 17, 28'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[179] S. Casper, J. Lin, J. Kwon, G. Culp, D. Hadfield-Menell, Explore, estab- lish, exploit: Red teaming language models from scratch, arXiv preprint arXiv:2306.09442 (2023). 17\\n\\n[180] E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, G. Irving, Red teaming language models with language models, arXiv preprint arXiv:2202.03286 (2022). 17'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[181] T. Scialom, T. Chakrabarty, S. Muresan, Fine-tuned language models are continual learners, in: Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 2022, pp. 6107–6122. 17\\n\\n[182] Z. Shi, A. Lipani, Don’t stop pretraining? make prompt-based fine-\\n\\ntuning powerful learner, arXiv preprint arXiv:2305.01711 (2023). 17'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[182] Z. Shi, A. Lipani, Don’t stop pretraining? make prompt-based fine-\\n\\ntuning powerful learner, arXiv preprint arXiv:2305.01711 (2023). 17\\n\\n[183] H. Gupta, S. A. Sawant, S. Mishra, M. Nakamura, A. Mitra, S. Mashetty, C. Baral, Instruction tuned models are quick learners, arXiv preprint arXiv:2306.05539 (2023). 17'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[184] H. Chen, Y. Zhang, Q. Zhang, H. Yang, X. Hu, X. Ma, Y. Yanggong, J. Zhao, Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning, arXiv preprint arXiv:2305.09246 (2023). 17\\n\\n[185] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu, et al., Lima: Less is more for alignment, arXiv preprint arXiv:2305.11206 (2023). 17, 25, 28'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[186] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, S. Wang, Lm-infinite: Sim- ple on-the-fly length generalization for large language models, arXiv preprint arXiv:2308.16137 (2023). 17, 18\\n\\n[187] J. Ainslie, T. Lei, M. de Jong, S. Ontañón, S. Brahma, Y. Zemlyan- skiy, D. Uthus, M. Guo, J. Lee-Thorp, Y. Tay, et al., Colt5: Faster long-range transformers with conditional computation, arXiv preprint arXiv:2303.09752 (2023). 18'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[188] J. Ding, S. Ma, L. Dong, X. Zhang, S. Huang, W. Wang, F. Wei, Longnet: Scaling transformers to 1,000,000,000 tokens, arXiv preprint arXiv:2307.02486 (2023). 18\\n\\n[189] Y. Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, J. Jia, Longlora: Effi- cient fine-tuning of long-context large language models, arXiv preprint arXiv:2309.12307 (2023). 18'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[190] N. Ratner, Y. Levine, Y. Belinkov, O. Ram, I. Magar, O. Abend, E. Karpas, A. Shashua, K. Leyton-Brown, Y. Shoham, Parallel context windows for large language models, in: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 6383–6402. 18\\n\\n[191] W. Wang, L. Dong, H. Cheng, X. Liu, X. Yan, J. Gao, F. Wei, Augmenting language models with long-term memory, arXiv preprint arXiv:2306.07174 (2023). 18'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[192] X. Xu, Z. Gou, W. Wu, Z.-Y. Niu, H. Wu, H. Wang, S. Wang, Long time no see! open-domain conversation with long-term persona memory, arXiv preprint arXiv:2203.05797 (2022). 18\\n\\n[193] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli- can, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al., Improving language models by retrieving from trillions of tokens, in: International conference on machine learning, PMLR, 2022, pp. 2206– 2240. 18, 19, 34'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[194] W. Zhong, L. Guo, Q. Gao, Y. Wang, Memorybank: Enhanc- ing large language models with long-term memory, arXiv preprint arXiv:2305.10250 (2023). 18\\n\\n[195] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, S. Yao, Reflexion: Language agents with verbal reinforcement learning, arXiv preprint arXiv:2303.11366 14 (2023). 18, 20'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[196] C. Hu, J. Fu, C. Du, S. Luo, J. Zhao, H. Zhao, Chatdb: Augment- ing llms with databases as their symbolic memory, arXiv preprint arXiv:2306.03901 (2023). 18\\n\\n[197] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang, J. Callan, G. Neubig, Active retrieval augmented generation, arXiv preprint arXiv:2305.06983 (2023). 18'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[198] O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton- Brown, Y. Shoham, In-context retrieval-augmented language models, arXiv preprint arXiv:2302.00083 (2023). 18, 34\\n\\n[199] X. Li, X. Qiu, Mot: Pre-thinking and recalling enable chatgpt to self- improve with memory-of-thoughts, arXiv preprint arXiv:2305.05181\\n\\n40\\n\\n(2023). 18'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='40\\n\\n(2023). 18\\n\\n[200] D. Schuurmans, Memory augmented large language models are compu- tationally universal, arXiv preprint arXiv:2301.04589 (2023). 18 [201] A. Modarressi, A. Imani, M. Fayyaz, H. Schütze, Ret-llm: Towards a general read-write memory for large language models, arXiv preprint arXiv:2305.14322 (2023). 18\\n\\n[202] S. Robertson, H. Zaragoza, et al., The probabilistic relevance frame- work: Bm25 and beyond, Foundations and Trends® in Information Re- trieval 3 (4) (2009) 333–389. 18'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[203] X. Wang,\\n\\nJ. Wei, D. Schuurmans, Q. Le, E. Chi, D. Zhou, Rationale-augmented ensembles in language models, arXiv preprint arXiv:2207.00747 (2022). 18\\n\\n[204] F. Zhang, B. Chen, Y. Zhang, J. Liu, D. Zan, Y. Mao, J.-G. Lou, W. Chen, Repocoder: Repository-level code completion through iterative retrieval and generation, arXiv preprint arXiv:2303.12570 (2023). 18'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[205] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y. Dong, O. Kuchaiev, B. Li, C. Xiao, et al., Shall we pretrain autoregressive language models with retrieval? a comprehensive study, arXiv preprint arXiv:2304.06762 (2023). 19\\n\\n[206] L. Wang, N. Yang, F. Wei, Learning to retrieve in-context examples for\\n\\nlarge language models, arXiv preprint arXiv:2307.07164 (2023). 19'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[206] L. Wang, N. Yang, F. Wei, Learning to retrieve in-context examples for\\n\\nlarge language models, arXiv preprint arXiv:2307.07164 (2023). 19\\n\\n[207] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, W. Chen, What makes good in-context examples for gpt-3?, arXiv preprint arXiv:2101.06804 (2021). 19\\n\\n[208] O. Rubin, J. Herzig, J. Berant, Learning to retrieve prompts for in-\\n\\ncontext learning, arXiv preprint arXiv:2112.08633 (2021). 19'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[208] O. Rubin, J. Herzig, J. Berant, Learning to retrieve prompts for in-\\n\\ncontext learning, arXiv preprint arXiv:2112.08633 (2021). 19\\n\\n[209] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle- moyer, W.-t. Yih, Replug: Retrieval-augmented black-box language models, arXiv preprint arXiv:2301.12652 (2023). 19\\n\\n[210] O. Rubin, J. Berant, Long-range language modeling with self-retrieval,\\n\\narXiv preprint arXiv:2306.13421 (2023). 19'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[210] O. Rubin, J. Berant, Long-range language modeling with self-retrieval,\\n\\narXiv preprint arXiv:2306.13421 (2023). 19\\n\\n[211] K. Guu, K. Lee, Z. Tung, P. Pasupat, M. Chang, Retrieval augmented language model pre-training, in: International conference on machine learning, PMLR, 2020, pp. 3929–3938. 19'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[212] S. Hofstätter, J. Chen, K. Raman, H. Zamani, Fid-light: Efficient and ef- fective retrieval-augmented text generation, in: Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2023, pp. 1437–1447. 19\\n\\n[213] M. Komeili, K. Shuster, J. Weston, Internet-augmented dialogue gener-\\n\\nation, arXiv preprint arXiv:2107.07566 (2021). 19'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[213] M. Komeili, K. Shuster, J. Weston, Internet-augmented dialogue gener-\\n\\nation, arXiv preprint arXiv:2107.07566 (2021). 19\\n\\n[214] A. Lazaridou, E. Gribovskaya, W. Stokowiec, N. Grigorev, Internet- augmented language models through few-shot prompting for open- domain question answering, arXiv preprint arXiv:2203.05115 (2022). 19'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[215] D. Gao, L. Ji, L. Zhou, K. Q. Lin, J. Chen, Z. Fan, M. Z. Shou, Assist- gpt: A general multi-modal assistant that can plan, execute, inspect, and learn, arXiv preprint arXiv:2306.08640 (2023). 19\\n\\n[216] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu, J. Gao, Chameleon: Plug-and-play compositional reasoning with large language models, arXiv preprint arXiv:2304.09842 (2023). 19, 20, 23'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[217] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, M. T. Ribeiro, Art: Automatic multi-step reasoning and tool-use for large lan- guage models, arXiv preprint arXiv:2303.09014 (2023). 19\\n\\n[218] C.-Y. Hsieh, S.-A. Chen, C.-L. Li, Y. Fujii, A. Ratner, C.-Y. Lee, R. Kr- ishna, T. Pfister, Tool documentation enables zero-shot tool-usage with large language models, arXiv preprint arXiv:2308.00675 (2023). 19'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[219] Y. Song, W. Xiong, D. Zhu, C. Li, K. Wang, Y. Tian, S. Li, Restgpt: Connecting large language models with real-world applications via rest- ful apis, arXiv preprint arXiv:2306.06624 (2023). 19\\n\\n[220] S. Hao, T. Liu, Z. Wang, Z. Hu, Toolkengpt: Augmenting frozen lan- guage models with massive tools via tool embeddings, arXiv preprint arXiv:2305.11554 (2023). 19'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[221] S. G. Patil, T. Zhang, X. Wang, J. E. Gonzalez, Gorilla: Large language model connected with massive apis, arXiv preprint arXiv:2305.15334 (2023). 19\\n\\n[222] Q. Xu, F. Hong, B. Li, C. Hu, Z. Chen, J. Zhang, On the tool manipu- lation capability of open-source large language models, arXiv preprint arXiv:2305.16504 (2023). 19'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[223] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang, B. Qian, et al., Toolllm: Facilitating large language models to master 16000+ real-world apis, arXiv preprint arXiv:2307.16789 (2023). 19,\\n\\n20\\n\\n[224] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, Y. Zhuang, Hugginggpt: Solv- ing ai tasks with chatgpt and its friends in huggingface, arXiv preprint arXiv:2303.17580 (2023). 19, 20, 33'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[225] Y. Liang, C. Wu, T. Song, W. Wu, Y. Xia, Y. Liu, Y. Ou, S. Lu, L. Ji, S. Mao, et al., Taskmatrix. ai: Completing tasks by connecting foun- dation models with millions of apis, arXiv preprint arXiv:2303.16434 (2023). 19\\n\\n[226] D. Surís, S. Menon, C. Vondrick, Vipergpt: Visual inference via python\\n\\nexecution for reasoning, arXiv preprint arXiv:2303.08128 (2023). 20'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[226] D. Surís, S. Menon, C. Vondrick, Vipergpt: Visual inference via python\\n\\nexecution for reasoning, arXiv preprint arXiv:2303.08128 (2023). 20\\n\\n[227] A. Maedche, S. Morana, S. Schacht, D. Werth, J. Krumeich, Advanced user assistance systems, Business & Information Systems Engineering 58 (2016) 367–370. 20\\n\\n[228] M. Campbell, A. J. Hoane Jr, F.-h. Hsu, Deep blue, Artificial intelligence\\n\\n134 (1-2) (2002) 57–83. 20'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[228] M. Campbell, A. J. Hoane Jr, F.-h. Hsu, Deep blue, Artificial intelligence\\n\\n134 (1-2) (2002) 57–83. 20\\n\\n[229] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, et al., Metagpt: Meta programming for multi-agent collaborative framework, arXiv preprint arXiv:2308.00352 (2023). 20'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[230] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou, et al., The rise and potential of large language model based agents: A survey, arXiv preprint arXiv:2309.07864 (2023). 20'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[231] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, et al., A survey on large language model based au- tonomous agents, arXiv preprint arXiv:2308.11432 (2023). 20 [232] W. Huang, P. Abbeel, D. Pathak, I. Mordatch, Language models as zero- shot planners: Extracting actionable knowledge for embodied agents, in: International Conference on Machine Learning, PMLR, 2022, pp. 9118–9147. 20'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[233] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, Z. Hu, Reason- ing with language model is planning with world model, arXiv preprint arXiv:2305.14992 (2023). 20, 33\\n\\n[234] W. Yao, S. Heinecke, J. C. Niebles, Z. Liu, Y. Feng, L. Xue, R. Murthy, Z. Chen, J. Zhang, D. Arpit, et al., Retroformer: Retrospective large language agents with policy gradient optimization, arXiv preprint arXiv:2308.02151 (2023). 20, 33'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[235] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, T. Jackson, N. Brown, L. Luu, S. Levine, K. Hausman, brian ichter, Inner mono- logue: Embodied reasoning through planning with language models, in: 6th Annual Conference on Robot Learning, 2022. URL https://openreview.net/forum?id=3R3Pz5i0tye 20 [236] C. Jin, W. Tan, J. Yang, B. Liu, R. Song, L. Wang, J. Fu, Alphablock: Embodied finetuning for vision-language reasoning in'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='20 [236] C. Jin, W. Tan, J. Yang, B. Liu, R. Song, L. Wang, J. Fu, Alphablock: Embodied finetuning for vision-language reasoning in robot manipula- tion, arXiv preprint arXiv:2305.18898 (2023). 20, 33'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[237] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, A. Garg, Progprompt: Generating situated robot task plans using large language models, in: 2023 IEEE International Conference on Robotics and Automation (ICRA), IEEE, 2023, pp. 11523–11530. 20, 33'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[238] W. Yu, N. Gileadi, C. Fu, S. Kirmani, K.-H. Lee, M. G. Arenas, H.-T. L. Chiang, T. Erez, L. Hasenclever, J. Humplik, et al., Language to rewards for robotic skill synthesis, arXiv preprint arXiv:2306.08647 (2023). 20 [239] X. Tang, A. Zou, Z. Zhang, Y. Zhao, X. Zhang, A. Cohan, M. Gerstein, Medagents: Large language models as collaborators for zero-shot med- ical reasoning, arXiv preprint arXiv:2311.10537 (2023). 20'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[240] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz, A. Irpan, E. Jang, R. Julian, et al., Do as i can, not as i say: Grounding language in robotic affordances, in: Conference on Robot Learning, PMLR, 2023, pp. 287–318. 20, 33\\n\\n[241] H. Ha, P. Florence, S. Song, Scaling up and distilling down: Language- guided robot skill acquisition, arXiv preprint arXiv:2307.14535 (2023). 20'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[242] A. Rajvanshi, K. Sikka, X. Lin, B. Lee, H.-P. Chiu, A. Velasquez, Say- nav: Grounding large language models for dynamic planning to navi- gation in new environments, arXiv preprint arXiv:2309.04077 (2023). 20\\n\\n[243] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, Y. Su, Llm-planner: Few-shot grounded planning for embodied agents with large language models, arXiv preprint arXiv:2212.04088 (2022). 20\\n\\n[244] V. S. Dorbala, J. F. Mullen Jr, D. Manocha, Can an embodied agent find'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[244] V. S. Dorbala, J. F. Mullen Jr, D. Manocha, Can an embodied agent find\\n\\n41\\n\\nyour\" cat-shaped mug\"? llm-based zero-shot object navigation, arXiv preprint arXiv:2303.03480 (2023). 20\\n\\n[245] C. Huang, O. Mees, A. Zeng, W. Burgard, Visual language maps for robot navigation, in: 2023 IEEE International Conference on Robotics and Automation (ICRA), IEEE, 2023, pp. 10608–10615. 20'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[246] Y. Ding, X. Zhang, C. Paxton, S. Zhang, Task and motion planning with large language models for object rearrangement, arXiv preprint arXiv:2303.06247 (2023). 20, 33\\n\\n[247] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, J. Tang, Gpt under-\\n\\nstands, too, arXiv preprint arXiv:2103.10385 (2021). 20, 21\\n\\n[248] G. Chen, F. Liu, Z. Meng, S. Liang, Revisiting parameter-efficient tun- ing: Are we really there yet?, arXiv preprint arXiv:2202.07962 (2022). 20'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[249] Y. Wang, S. Mukherjee, X. Liu, J. Gao, A. H. Awadallah, J. Gao, Adamix: Mixture-of-adapter for parameter-efficient tuning of large lan- guage models, arXiv preprint arXiv:2205.12410 1 (2) (2022) 4. 20 [250] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, Lora: Low-rank adaptation of large language models, arXiv preprint arXiv:2106.09685 (2021). 21, 22, 23'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[251] X. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, J. Tang, P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks, in: Pro- ceedings of the 60th Annual Meeting of the Association for Computa- tional Linguistics (Volume 2: Short Papers), 2022, pp. 61–68. 21 [252] A. Razdaibiedina, Y. Mao, R. Hou, M. Khabsa, M. Lewis, A. Almahairi, Progressive prompts: Continual learning for language models, arXiv preprint arXiv:2301.12314 (2023). 21'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[253] Z.-R. Zhang, C. Tan, H. Xu, C. Wang, J. Huang, S. Huang, To- wards adaptive prefix tuning for parameter-efficient language model fine-tuning, arXiv preprint arXiv:2305.15212 (2023). 21\\n\\n[254] E. B. Zaken, S. Ravfogel, Y. Goldberg, Bitfit: Simple parameter- efficient fine-tuning for transformer-based masked language-models, arXiv preprint arXiv:2106.10199 (2021). 21'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[255] T. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, Llm. int8 (): 8-bit matrix multiplication for transformers at scale, arXiv preprint arXiv:2208.07339 (2022). 21, 22\\n\\n[256] E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, Gptq: Accurate post-training quantization for generative pre-trained transformers, arXiv preprint arXiv:2210.17323 (2022). 21'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[257] X. Wei, Y. Zhang, Y. Li, X. Zhang, R. Gong, J. Guo, X. Liu, Outlier sup- pression+: Accurate quantization of large language models by equiva- lent and optimal shifting and scaling, arXiv preprint arXiv:2304.09145 (2023). 21\\n\\n[258] E. Frantar, D. Alistarh, Optimal brain compression: A framework for accurate post-training quantization and pruning, Advances in Neural In- formation Processing Systems 35 (2022) 4475–4488. 21'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[259] C. Lee, J. Jin, T. Kim, H. Kim, E. Park, Owq: Lessons learned from ac- tivation outliers for weight quantization in large language models, arXiv preprint arXiv:2306.02272 (2023). 21\\n\\n[260] S. J. Kwon, J. Kim, J. Bae, K. M. Yoo, J.-H. Kim, B. Park, B. Kim, J.- W. Ha, N. Sung, D. Lee, Alphatuning: Quantization-aware parameter- efficient adaptation of large-scale pre-trained language models, arXiv preprint arXiv:2210.03858 (2022). 21'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[261] T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer, Qlora: Efficient finetuning of quantized llms, arXiv preprint arXiv:2305.14314 (2023). 21, 22\\n\\n[262] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Kr- ishnamoorthi, V. Chandra, Llm-qat: Data-free quantization aware train- ing for large language models, arXiv preprint arXiv:2305.17888 (2023). 21, 22'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[263] Y. Guo, A. Yao, H. Zhao, Y. Chen, Network sketching: Exploiting bi- nary structure in deep cnns, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 5955–5963. 21\\n\\n[264] J. Kim, J. H. Lee, S. Kim, J. Park, K. M. Yoo, S. J. Kwon, D. Lee, Memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization, arXiv preprint arXiv:2305.14152 (2023). 22'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[265] M. Sun, Z. Liu, A. Bair, J. Z. Kolter, A simple and effective pruning approach for large language models, arXiv preprint arXiv:2306.11695 (2023). 22\\n\\n[266] Z. Wang, J. Wohlwend, T. Lei, Structured pruning of large language\\n\\nmodels, arXiv preprint arXiv:1910.04732 (2019). 22'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[266] Z. Wang, J. Wohlwend, T. Lei, Structured pruning of large language\\n\\nmodels, arXiv preprint arXiv:1910.04732 (2019). 22\\n\\n[267] L. Yin, Y. Wu, Z. Zhang, C.-Y. Hsieh, Y. Wang, Y. Jia, M. Pechenizkiy, Y. Liang, Z. Wang, S. Liu, Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity, arXiv preprint arXiv:2310.05175 (2023). 22'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[268] C. Tao, L. Hou, H. Bai, J. Wei, X. Jiang, Q. Liu, P. Luo, N. Wong, Structured pruning for efficient generative pre-trained language models, in: Findings of the Association for Computational Linguistics: ACL 2023, 2023, pp. 10880–10895. 22'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[269] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al., Flamingo: a visual lan- guage model for few-shot learning, Advances in Neural Information Pro- cessing Systems 35 (2022) 23716–23736. 22\\n\\n[270] J. Li, D. Li, S. Savarese, S. Hoi, Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, arXiv preprint arXiv:2301.12597 (2023). 22'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[271] H. Liu, C. Li, Q. Wu, Y. J. Lee, Visual instruction tuning, arXiv preprint\\n\\narXiv:2304.08485 (2023). 22\\n\\n[272] K. Li, Y. He, Y. Wang, Y. Li, W. Wang, P. Luo, Y. Wang, L. Wang, Y. Qiao, Videochat: Chat-centric video understanding, arXiv preprint arXiv:2305.06355 (2023). 22\\n\\n[273] M. Maaz, H. Rasheed, S. Khan, F. S. Khan, Video-chatgpt: Towards de- tailed video understanding via large vision and language models, arXiv preprint arXiv:2306.05424 (2023). 22'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[274] H. Zhang, X. Li, L. Bing, Video-llama: An instruction-tuned audio-visual language model for video understanding, arXiv preprint arXiv:2306.02858 (2023). 22\\n\\n[275] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D. Plumbley, Y. Zou, W. Wang, Wavcaps: A chatgpt-assisted weakly-labelled au- dio captioning dataset for audio-language multimodal research, arXiv preprint arXiv:2303.17395 (2023). 22'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[276] C. Lyu, M. Wu, L. Wang, X. Huang, B. Liu, Z. Du, S. Shi, Z. Tu, Macaw- llm: Multi-modal language modeling with image, audio, video, and text integration, arXiv preprint arXiv:2306.09093 (2023). 22\\n\\n[277] D. Zhu, J. Chen, X. Shen, X. Li, M. Elhoseiny, Minigpt-4: Enhancing vision-language understanding with advanced large language models, arXiv preprint arXiv:2304.10592 (2023). 22'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[278] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al., An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929 (2020). 22\\n\\n[279] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, S. Hoi, Instructblip: Towards general-purpose vision-language models with instruction tuning, arXiv preprint arXiv:2305.06500 (2023). 22'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[280] Z. Xu, Y. Shen, L. Huang, Multiinstruct: Improving multi-modal zero- shot learning via instruction tuning, arXiv preprint arXiv:2212.10773 (2022). 22\\n\\n[281] Z. Zhao, L. Guo, T. Yue, S. Chen, S. Shao, X. Zhu, Z. Yuan, J. Liu, Chatbridge: Bridging modalities with large language model as a lan- guage catalyst, arXiv preprint arXiv:2305.16103 (2023). 22'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[282] L. Li, Y. Yin, S. Li, L. Chen, P. Wang, S. Ren, M. Li, Y. Yang, J. Xu, X. Sun, et al., M3 it: A large-scale dataset towards multi-modal multi- lingual instruction tuning, arXiv preprint arXiv:2306.04387 (2023). 22 [283] R. Pi, J. Gao, S. Diao, R. Pan, H. Dong, J. Zhang, L. Yao, J. Han, H. Xu, L. K. T. Zhang, Detgpt: Detect what you need via reasoning, arXiv preprint arXiv:2305.14167 (2023). 22'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[284] G. Luo, Y. Zhou, T. Ren, S. Chen, X. Sun, R. Ji, Cheap and quick: Efficient vision-language instruction tuning for large language models, arXiv preprint arXiv:2305.15023 (2023). 22\\n\\n[285] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, Y. Qiao, Llama-adapter: Efficient fine-tuning of language models with zero-init attention, arXiv preprint arXiv:2303.16199 (2023). 22'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[286] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, I. Sutskever, Robust speech recognition via large-scale weak supervision, in: Inter- national Conference on Machine Learning, PMLR, 2023, pp. 28492– 28518. 22\\n\\n[287] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, A. Smola, Multi- modal chain-of-thought reasoning in language models, arXiv preprint arXiv:2302.00923 (2023). 23'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[288] J. Ge, H. Luo, S. Qian, Y. Gan, J. Fu, S. Zhan, Chain of thought prompt tuning in vision language models, arXiv preprint arXiv:2304.07919 (2023). 23\\n\\n42\\n\\n[289] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, N. Duan, Visual chatgpt: Talk- ing, drawing and editing with visual foundation models, arXiv preprint arXiv:2303.04671 (2023). 23'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[290] Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu, M. Zeng, L. Wang, Mm-react: Prompting chatgpt for multimodal rea- soning and action, arXiv preprint arXiv:2303.11381 (2023). 23 [291] T. Wang, J. Zhang, J. Fei, Y. Ge, H. Zheng, Y. Tang, Z. Li, M. Gao, S. Zhao, Y. Shan, et al., Caption anything: Interactive image descrip- tion with diverse multimodal controls, arXiv preprint arXiv:2305.02677 (2023). 23'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[292] X. Zhu, R. Zhang, B. He, Z. Zeng, S. Zhang, P. Gao, Pointclip v2: Adapting clip for powerful 3d open-world learning, arXiv preprint arXiv:2211.11682 (2022). 23\\n\\n[293] T. Gupta, A. Kembhavi, Visual programming: Compositional visual rea- soning without training, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 14953–14962. 23'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[294] P. Gao, Z. Jiang, H. You, P. Lu, S. C. Hoi, X. Wang, H. Li, Dynamic fusion with intra-and inter-modality attention flow for visual question answering, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 6639–6648. 23\\n\\n[295] Z. Yu, J. Yu, Y. Cui, D. Tao, Q. Tian, Deep modular co-attention net- works for visual question answering, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 6281– 6290. 23'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[296] H. You, R. Sun, Z. Wang, L. Chen, G. Wang, H. A. Ayyubi, K.- Iteratively decomposing vision W. Chang, S.-F. Chang, Idealgpt: and language reasoning via large language models, arXiv preprint arXiv:2305.14985 (2023). 23'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[297] R. Zhang, X. Hu, B. Li, S. Huang, H. Deng, Y. Qiao, P. Gao, H. Li, Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 15211–15222. 23'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[298] T. Q. Nguyen, J. Salazar, Transformers without tears: Improving the normalization of self-attention, CoRR abs/1910.05895 (2019). 24 [299] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, V. Stoyanov, Roberta: A robustly optimized bert pre- training approach, arXiv preprint arXiv:1907.11692 (2019). 24, 30 [300] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, D. Song, Koala: A dialogue model for academic research, Blog post (April'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='30 [300] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, D. Song, Koala: A dialogue model for academic research, Blog post (April 2023). URL https://bair.berkeley.edu/blog/2023/04/03/koala/ 25'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[301] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al., The pile: An 800gb dataset of diverse text for language modeling, arXiv preprint arXiv:2101.00027 (2020). 28, 30'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[302] H. Laurençon, L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral, T. Le Scao, L. Von Werra, C. Mou, E. González Ponferrada, H. Nguyen, et al., The bigscience roots corpus: A 1.6 tb composite multilingual dataset, Advances in Neural Information Processing Systems 35 (2022) 31809–31826. 28\\n\\n[303] Wikipedia.\\n\\nURL https://en.wikipedia.org/wiki/Main_Page 28\\n\\n[304] Together Computer, Redpajama: An open source recipe to reproduce\\n\\nllama training dataset (Apr. 2023). URL RedPajama-Data 28'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[304] Together Computer, Redpajama: An open source recipe to reproduce\\n\\nllama training dataset (Apr. 2023). URL RedPajama-Data 28\\n\\nhttps://github.com/togethercomputer/\\n\\n[305] O. Honovich, T. Scialom, O. Levy, T. Schick, Unnatural instructions: Tuning language models with (almost) no human labor, arXiv preprint arXiv:2212.09689 (2022). 28'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[306] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al., Training a helpful and harmless assistant with reinforcement learning from human feedback, arXiv preprint arXiv:2204.05862 (2022). 28\\n\\n[307] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, J. Steinhardt, Measuring massive multitask language understanding, arXiv preprint arXiv:2009.03300 (2020). 26, 29'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[308] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, et al., Beyond\\n\\nthe imitation game: Quantifying and extrapolating the capabilities of language models, arXiv preprint arXiv:2206.04615 (2022). 26, 29 [309] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, S. R. Bowman, Glue: A multi-task benchmark and analysis platform for natural language un- derstanding, arXiv preprint arXiv:1804.07461 (2018). 26, 29'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[310] Y. Yao, Q. Dong, J. Guan, B. Cao, Z. Zhang, C. Xiao, X. Wang, F. Qi, J. Bao, J. Nie, et al., Cuge: A chinese language understanding and gen- eration evaluation benchmark, arXiv preprint arXiv:2112.13610 (2021). 29\\n\\n[311] L. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y. Li, Y. Xu, K. Sun, D. Yu, C. Yu, et al., Clue: A chinese language understanding evaluation bench- mark, arXiv preprint arXiv:2004.05986 (2020). 29'),\n",
       " Document(metadata={'source': 'f:\\\\Tapas\\\\Learning\\\\LLMOps\\\\LLMOps_Learning\\\\Document_portal\\\\notebook\\\\data\\\\LLM.pdf'}, page_content='[312] L. Xu, X. Lu, C. Yuan, X. Zhang, H. Xu, H. Yuan, G. Wei, X. Pan, X. Tian, L. Qin, et al., Fewclue: A chinese few-shot learning evaluation benchmark, arXiv preprint arXiv:2107.07498 (2021). 29\\n\\n[313] E. M. Smith, M. Williamson, K. Shuster, J. Weston, Y.-L. Boureau, Can you put it all together: Evaluating conversational agents’ ability to blend skills, arXiv preprint arXiv:2004.08449 (2020). 29'),\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d15bf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\miniconda3\\envs\\llmops_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings.embed_documents(docs[0].page_content)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a011cd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store=FAISS.from_documents(docs,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c4a2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "retreival_data=vector_store.similarity_search(\"Who is Tapas Kumar Paul?\",k=10) # default k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51f86d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retreival_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec05fd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tapas Kumar Paul\\n\\nData Scientist in AIOPs\\n\\nKolkata, West Bengal\\n\\nMobile No- 8016254143 / 8158805321\\n\\nPan No- CWKPP3505R\\n\\nEmail- tapaskumarpaul84@gmail.com , Linkedin , GitHub\\n\\nSummary'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retreival_data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee082c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vector_store.as_retriever(search_type='similarity',k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "531d9268",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"\"\"\n",
    "You are a helpful assistant. Please answer the question based on provided context below.\n",
    "if the context does not have sufficient information, responsd with \n",
    "\"I don't have enough information for your question.\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8cc7f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt=PromptTemplate(template=system_prompt,\n",
    "                      input_variables=['context','question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6872e19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant. Please answer the question based on provided context below.\\nif the context does not have sufficient information, responsd with \\n\"I don\\'t have enough information for your question.\"\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\\n')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c3fec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2762d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d7b39b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "910be6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain=(\n",
    "    {\"context\" : retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2bcba1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\miniconda3\\envs\\llmops_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The user is asking about the Transformer algorithm. From the context provided, I need to piece together the key components.\n",
      "\n",
      "First, the Transformer uses stacked self-attention and fully connected layers. The encoder and decoder each have multiple layers. The original model had 6 encoder layers and 6 decoder layers. Each encoder layer has a multi-head self-attention mechanism followed by a position-wise fully connected layer. The decoder has similar layers but also an attention mechanism for the encoder. \n",
      "\n",
      "The context mentions that the Transformer doesn't use RNNs or convolution, which is a key point. Also, there's some info about adapter layers, which are used for fine-tuning. But the question is about the Transformer algorithm itself, not just the adapter part.\n",
      "\n",
      "I should explain the overall architecture, the role of self-attention, and the encoder-decoder structure. Maybe mention that it's used for sequence-to-sequence tasks. But wait, the context doesn't specify the exact number of layers in all cases, but the original is 6 each. Need to make sure I don't include info about adapters unless necessary. The user's question is about the Transformer algorithm, so focus on the core structure. Also, note that it's the first model to rely entirely on self-attention. Alright, that should cover the main points.\n",
      "</think>\n",
      "\n",
      "The Transformer is a neural network architecture introduced for sequence-to-sequence tasks, relying entirely on **self-attention mechanisms** and **position-wise fully connected layers** instead of recurrent or convolutional operations. Its key components include:\n",
      "\n",
      "1. **Encoder-Decoder Structure**:\n",
      "   - **Encoder**: A stack of identical layers, each containing:\n",
      "     - A **multi-head self-attention** mechanism to compute contextualized representations.\n",
      "     - A **position-wise fully connected feed-forward network**.\n",
      "   - **Decoder**: Also stacked layers with:\n",
      "     - A **masked self-attention** layer (to prevent attending to future positions).\n",
      "     - An **encoder-decoder attention** layer (to focus on relevant encoder outputs).\n",
      "     - A position-wise feed-forward network.\n",
      "\n",
      "2. **Self-Attention**: Enables the model to weigh relationships between input elements dynamically, capturing dependencies regardless of distance.\n",
      "\n",
      "3. **Positional Encoding**: Injects information about word positions into the model since self-attention lacks inherent sequence order awareness.\n",
      "\n",
      "4. **Scalability**: The architecture avoids recurrence, allowing parallel processing for efficiency. Variants later explored asymmetric designs (e.g., shallower encoders, deeper decoders) for specific tasks.\n",
      "\n",
      "The Transformer revolutionized NLP by enabling longer-range context modeling and efficient training, forming the basis for models like BERT and GPT.\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"what is transformer algorithm?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bac283",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
